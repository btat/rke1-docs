"use strict";(self.webpackChunkrke_docs=self.webpackChunkrke_docs||[]).push([[3127],{8927:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var r=t(5893),s=t(1151);const a={title:"Example Cluster.ymls"},o=void 0,i={id:"example-yamls/example-yamls",title:"Example Cluster.ymls",description:"There are lots of different configuration options that can be set in the cluster configuration file for RKE. Here are some examples of files:",source:"@site/docs/example-yamls/example-yamls.md",sourceDirName:"example-yamls",slug:"/example-yamls/",permalink:"/example-yamls/",draft:!1,unlisted:!1,editUrl:"https://github.com/rancher/rke1-docs/edit/main/docs/example-yamls/example-yamls.md",tags:[],version:"current",lastUpdatedAt:1704844723,formattedLastUpdatedAt:"Jan 9, 2024",frontMatter:{title:"Example Cluster.ymls"},sidebar:"mySidebar",previous:{title:"User-Defined Add-Ons",permalink:"/config-options/add-ons/user-defined-add-ons/"},next:{title:"Troubleshooting",permalink:"/troubleshooting/"}},l={},c=[{value:"Minimal <code>cluster.yml</code> example",id:"minimal-clusteryml-example",level:2},{value:"Full <code>cluster.yml</code> example",id:"full-clusteryml-example",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["There are lots of different ",(0,r.jsx)(n.a,{href:"/config-options/",children:"configuration options"})," that can be set in the cluster configuration file for RKE. Here are some examples of files:"]}),"\n",(0,r.jsx)(n.admonition,{title:"Note for Rancher 2 users",type:"note",children:(0,r.jsxs)(n.p,{children:["If you are configuring Cluster Options using a ",(0,r.jsx)(n.a,{href:"https://ranchermanager.docs.rancher.com/reference-guides/cluster-configuration/rancher-server-configuration/rke1-cluster-configuration#rke-cluster-config-file-reference",children:"Config File"})," when creating ",(0,r.jsx)(n.a,{href:"https://ranchermanager.docs.rancher.com/pages-for-subheaders/launch-kubernetes-with-rancher",children:"Rancher Launched Kubernetes"}),", the names of services should contain underscores only: ",(0,r.jsx)(n.code,{children:"kube_api"})," and ",(0,r.jsx)(n.code,{children:"kube_controller"}),". This only applies to Rancher v2.0.5 and v2.0.6."]})}),"\n",(0,r.jsxs)(n.h2,{id:"minimal-clusteryml-example",children:["Minimal ",(0,r.jsx)(n.code,{children:"cluster.yml"})," example"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"nodes:\n    - address: 1.2.3.4\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n        - worker\n"})}),"\n",(0,r.jsxs)(n.h2,{id:"full-clusteryml-example",children:["Full ",(0,r.jsx)(n.code,{children:"cluster.yml"})," example"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'nodes:\n    - address: 1.1.1.1\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n      port: 2222\n      docker_socket: /var/run/docker.sock\n    - address: 2.2.2.2\n      user: ubuntu\n      role:\n        - worker\n      ssh_key_path: /home/user/.ssh/id_rsa\n      ssh_key: |-\n        -----BEGIN RSA PRIVATE KEY-----\n\n        -----END RSA PRIVATE KEY-----\n      ssh_cert_path: /home/user/.ssh/test-key-cert.pub\n      ssh_cert: |-\n        ssh-rsa-cert-v01@openssh.com AAAAHHNzaC1yc2EtY2VydC12MDFAb3Bl....\n    - address: example.com\n      user: ubuntu\n      role:\n        - worker\n      hostname_override: node3\n      internal_address: 192.168.1.6\n      labels:\n        app: ingress\n      taints:\n        - key: test-key\n          value: test-value\n          effect: NoSchedule\n\n# If set to true, RKE will not fail when unsupported Docker version\n# are found\nignore_docker_version: false\n\n# Enable running cri-dockerd\n# Up to Kubernetes 1.23, kubelet contained code called dockershim\n# to support Docker runtime. The replacement is called cri-dockerd\n# and should be enabled if you want to keep using Docker as your\n# container runtime\n# Only available to enable in Kubernetes 1.21 and higher\nenable_cri_dockerd: true\n\n# Cluster level SSH private key\n# Used if no ssh information is set for the node\nssh_key_path: ~/.ssh/test\n\n# Enable use of SSH agent to use SSH private keys with passphrase\n# This requires the environment `SSH_AUTH_SOCK` configured pointing\n#to your SSH agent which has the private key added\nssh_agent_auth: true\n\n# List of registry credentials\n# If you are using a Docker Hub registry, you can omit the `url`\n# or set it to `docker.io`\n# is_default set to `true` will override the system default\n# registry set in the global settings\nprivate_registries:\n     - url: registry.com\n       user: Username\n       password: password\n       is_default: true\n\n# Bastion/Jump host configuration\nbastion_host:\n    address: x.x.x.x\n    user: ubuntu\n    port: 22\n    ssh_key_path: /home/user/.ssh/bastion_rsa\n# or\n#   ssh_key: |-\n#     -----BEGIN RSA PRIVATE KEY-----\n#\n#     -----END RSA PRIVATE KEY-----\n\n# Set the name of the Kubernetes cluster\ncluster_name: mycluster\n\n\n# The Kubernetes version used. The default versions of Kubernetes\n# are tied to specific versions of the system images.\n#\n# For RKE v0.2.x and below, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go\n#\n# For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go\n#\n# In case the kubernetes_version and kubernetes image in\n# system_images are defined, the system_images configuration\n# will take precedence over kubernetes_version.\nkubernetes_version: v1.10.3-rancher2\n\n# System Images are defaulted to a tag that is mapped to a specific\n# Kubernetes Version and not required in a cluster.yml.\n# Each individual system image can be specified if you want to use a different tag.\n#\n# For RKE v0.2.x and below, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go\n#\n# For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go\n#\nsystem_images:\n    kubernetes: rancher/hyperkube:v1.10.3-rancher2\n    etcd: rancher/coreos-etcd:v3.1.12\n    alpine: rancher/rke-tools:v0.1.9\n    nginx_proxy: rancher/rke-tools:v0.1.9\n    cert_downloader: rancher/rke-tools:v0.1.9\n    kubernetes_services_sidecar: rancher/rke-tools:v0.1.9\n    kubedns: rancher/k8s-dns-kube-dns-amd64:1.14.8\n    dnsmasq: rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8\n    kubedns_sidecar: rancher/k8s-dns-sidecar-amd64:1.14.8\n    kubedns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0\n    pod_infra_container: rancher/pause-amd64:3.1\n\nservices:\n    etcd:\n      # Custom uid/guid for etcd directory and files\n      uid: 52034\n      gid: 52034\n      # if external etcd is used\n      # path: /etcdcluster\n      # external_urls:\n      #   - https://etcd-example.com:2379\n      # ca_cert: |-\n      #   -----BEGIN CERTIFICATE-----\n      #   xxxxxxxxxx\n      #   -----END CERTIFICATE-----\n      # cert: |-\n      #   -----BEGIN CERTIFICATE-----\n      #   xxxxxxxxxx\n      #   -----END CERTIFICATE-----\n      # key: |-\n      #   -----BEGIN PRIVATE KEY-----\n      #   xxxxxxxxxx\n      #   -----END PRIVATE KEY-----\n    # Note for Rancher v2.0.5 and v2.0.6 users: If you are configuring\n    # Cluster Options using a Config File when creating Rancher Launched\n    # Kubernetes, the names of services should contain underscores\n    # only: `kube_api`.\n    kube-api:\n      # IP range for any services created on Kubernetes\n      # This must match the service_cluster_ip_range in kube-controller\n      service_cluster_ip_range: 10.43.0.0/16\n      # Expose a different port range for NodePort services\n      service_node_port_range: 30000-32767\n      pod_security_policy: false\n      # Encrypt secret data at Rest\n      # Available as of v0.3.1\n      secrets_encryption_config:\n        enabled: true\n        custom_config:\n          apiVersion: apiserver.config.k8s.io/v1\n          kind: EncryptionConfiguration\n          resources:\n          - resources:\n            - secrets\n            providers:\n            - aescbc:\n                keys:\n                - name: k-fw5hn\n                  secret: RTczRjFDODMwQzAyMDVBREU4NDJBMUZFNDhCNzM5N0I=\n            - identity: {}\n      # Enable audit logging\n      # Available as of v1.0.0\n      audit_log:\n        enabled: true\n        configuration:\n          max_age: 6\n          max_backup: 6\n          max_size: 110\n          path: /var/log/kube-audit/audit-log.json\n          format: json\n          policy:\n            apiVersion: audit.k8s.io/v1 # This is required.\n            kind: Policy\n            omitStages:\n              - "RequestReceived"\n            rules:\n              # Log pod changes at RequestResponse level\n              - level: RequestResponse\n                resources:\n                - group: ""\n                  # Resource "pods" doesn\'t match requests to any subresource of pods,\n                  # which is consistent with the RBAC policy.\n                  resources: ["pods"]\n      # Using the EventRateLimit admission control enforces a limit on the number of events\n      # that the API Server will accept in a given time period\n      # Available as of v1.0.0\n      event_rate_limit:\n        enabled: true\n        configuration:\n          apiVersion: eventratelimit.admission.k8s.io/v1alpha1\n          kind: Configuration\n          limits:\n          - type: Server\n            qps: 6000\n            burst: 30000\n      # Enable AlwaysPullImages Admission controller plugin\n      # Available as of v0.2.0\n      always_pull_images: false\n      # Add additional arguments to the kubernetes API server\n      # This WILL OVERRIDE any existing defaults\n      extra_args:\n        # Enable audit log to stdout\n        audit-log-path: "-"\n        # Increase number of delete workers\n        delete-collection-workers: 3\n        # Set the level of log output to debug-level\n        v: 4\n    # Note for Rancher 2 users: If you are configuring Cluster Options\n    # using a Config File when creating Rancher Launched Kubernetes,\n    # the names of services should contain underscores only:\n    # `kube_controller`. This only applies to Rancher v2.0.5 and v2.0.6.\n    kube-controller:\n      # CIDR pool used to assign IP addresses to pods in the cluster\n      cluster_cidr: 10.42.0.0/16\n      # IP range for any services created on Kubernetes\n      # This must match the service_cluster_ip_range in kube-api\n      service_cluster_ip_range: 10.43.0.0/16\n      # Add additional arguments to the kubernetes API server\n      # This WILL OVERRIDE any existing defaults\n      extra_args:\n        # Set the level of log output to debug-level\n        v: 4\n        # Enable RotateKubeletServerCertificate feature gate\n        feature-gates: RotateKubeletServerCertificate=true\n        # Enable TLS Certificates management\n        # https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/\n        cluster-signing-cert-file: "/etc/kubernetes/ssl/kube-ca.pem"\n        cluster-signing-key-file: "/etc/kubernetes/ssl/kube-ca-key.pem"\n    kubelet:\n      # Base domain for the cluster\n      cluster_domain: cluster.local\n      # IP address for the DNS service endpoint\n      cluster_dns_server: 10.43.0.10\n      # Fail if swap is on\n      fail_swap_on: false\n      # Configure pod-infra-container-image argument\n      pod-infra-container-image: "k8s.gcr.io/pause:3.2"\n      # Generate a certificate signed by the kube-ca Certificate Authority\n      # for the kubelet to use as a server certificate\n      # Available as of v1.0.0\n      generate_serving_certificate: true\n      extra_args:\n        # Set max pods to 250 instead of default 110\n        max-pods: 250\n        # Enable RotateKubeletServerCertificate feature gate\n        feature-gates: RotateKubeletServerCertificate=true\n      # Optionally define additional volume binds to a service\n      extra_binds:\n        - "/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins"\n    scheduler:\n      extra_args:\n        # Set the level of log output to debug-level\n        v: 4\n    kubeproxy:\n      extra_args:\n        # Set the level of log output to debug-level\n        v: 4\n\n# Currently, only authentication strategy supported is x509.\n# You can optionally create additional SANs (hostnames or IPs) to\n# add to the API server PKI certificate.\n# This is useful if you want to use a load balancer for the\n# control plane servers.\nauthentication:\n    strategy: x509\n    sans:\n      - "10.18.160.10"\n      - "my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com"\n\n# Kubernetes Authorization mode\n# Use `mode: rbac` to enable RBAC\n# Use `mode: none` to disable authorization\nauthorization:\n    mode: rbac\n\n# If you want to set a Kubernetes cloud provider, you specify\n# the name and configuration\ncloud_provider:\n    name: aws\n\n# Add-ons are deployed using kubernetes jobs. RKE will give\n# up on trying to get the job status after this timeout in seconds..\naddon_job_timeout: 30\n\n# Specify network plugin-in (canal, calico, flannel or none)\nnetwork:\n  plugin: canal\n  # Specify MTU\n  mtu: 1400\n  options:\n    # Configure interface to use for Canal\n    canal_iface: eth1\n    canal_flannel_backend_type: vxlan\n    # Available as of v1.2.6\n    canal_autoscaler_priority_class_name: system-cluster-critical\n    canal_priority_class_name: system-cluster-critical\n  # Available as of v1.2.4\n  tolerations:\n  - key: "node.kubernetes.io/unreachable"\n    operator: "Exists"\n    effect: "NoExecute"\n    tolerationseconds: 300\n  - key: "node.kubernetes.io/not-ready"\n    operator: "Exists"\n    effect: "NoExecute"\n    tolerationseconds: 300\n  # Available as of v1.1.0\n  update_strategy:\n    strategy: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 6\n\n# Specify DNS provider (coredns or kube-dns)\ndns:\n  provider: coredns\n  # Available as of v1.1.0\n  update_strategy:\n    strategy: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 20%\n      maxSurge: 15%\n  linear_autoscaler_params:\n    cores_per_replica: 0.34\n    nodes_per_replica: 4\n    prevent_single_point_failure: true\n    min: 2\n    max: 3\n\n# Specify monitoring provider (metrics-server)\nmonitoring:\n  provider: metrics-server\n  # Available as of v1.1.0\n  update_strategy:\n    strategy: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 8\n\n# Currently only nginx ingress provider is supported.\n# To disable ingress controller, set `provider: none`\n# `node_selector` controls ingress placement and is optional\ningress:\n  provider: nginx\n  node_selector:\n    app: ingress\n  # Available as of v1.1.0\n  update_strategy:\n    strategy: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 5\n\n# All add-on manifests MUST specify a namespace\naddons: |-\n    ---\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: my-nginx\n      namespace: default\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\naddons_include:\n    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-operator.yaml\n    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-cluster.yaml\n    - /path/to/manifest\n'})})]})}function d(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>o});var r=t(7294);const s={},a=r.createContext(s);function o(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);