{"searchDocs":[{"title":"Authorization","type":0,"sectionRef":"#","url":"/config-options/authorization","content":"Authorization Kubernetes supports multiple Authorization Modules. Currently, RKE only supports the RBAC module. By default, RBAC is already enabled. If you wanted to turn off RBAC support, which isn't recommended, you set the authorization mode to none in your cluster.yml. authorization: # Use `mode: none` to disable authorization mode: rbac ","keywords":"","version":"Next"},{"title":"Cloud Providers","type":0,"sectionRef":"#","url":"/config-options/cloud-providers","content":"Cloud Providers RKE supports the ability to set your specific cloud provider for your Kubernetes cluster. There are specific cloud configurations for these cloud providers. To enable a cloud provider its name as well as any required configuration options must be provided under the cloud_provider directive in the cluster YML. AWSAzureOpenStackvSphere Outside of this list, RKE also supports the ability to handle any custom cloud provider.","keywords":"","version":"Next"},{"title":"Certificate Management","type":0,"sectionRef":"#","url":"/cert-mgmt","content":"","keywords":"","version":"Next"},{"title":"Generating Certificate Signing Requests (CSRs) and Keys​","type":1,"pageTitle":"Certificate Management","url":"/cert-mgmt#generating-certificate-signing-requests-csrs-and-keys","content":" If you want to create and sign the certificates by a real Certificate Authority (CA), you can use RKE to generate a set of Certificate Signing Requests (CSRs) and keys.  You can use the CSRs and keys to sign the certificates by a real CA. After the certificates are signed, these custom certificates can be used by RKE to as custom certificates for the Kubernetes cluster.  ","version":"Next","tagName":"h2"},{"title":"Certificate Rotation​","type":1,"pageTitle":"Certificate Management","url":"/cert-mgmt#certificate-rotation","content":" By default, Kubernetes clusters require certificates and RKE will automatically generate certificates for the clusters. Rotating these certificates are important before the certificates expire as well as if a certificate is compromised.  After the certificates are rotated, the Kubernetes components are automatically restarted. Certificates can be rotated for the following services:  etcdkubelet (node certificate)kubelet (serving certificate, if enabled)kube-apiserverkube-proxykube-schedulerkube-controller-manager  RKE has the ability to rotate the auto-generated certificates with some simple commands:  Rotating all service certificates while using the same CARotating a certificate on an individual service while using the same CARotating the CA and all service certificates  Whenever you're trying to rotate certificates, the cluster.yml that was used to deploy the Kubernetes cluster is required. You can reference a different location for this file by using the --config option when running rke cert rotate.  ","version":"Next","tagName":"h2"},{"title":"Rotating all Service Certificates while using the same CA​","type":1,"pageTitle":"Certificate Management","url":"/cert-mgmt#rotating-all-service-certificates-while-using-the-same-ca","content":" To rotate the service certificates for all the Kubernetes services, run the following command, i.e. rke cert rotate. After all the service certificates are rotated, these services will automatically be restarted to start using the new certificate.  $ rke cert rotate INFO[0000] Initiating Kubernetes cluster INFO[0000] Rotating Kubernetes cluster certificates INFO[0000] [certificates] Generating Kubernetes API server certificates INFO[0000] [certificates] Generating Kube Controller certificates INFO[0000] [certificates] Generating Kube Scheduler certificates INFO[0001] [certificates] Generating Kube Proxy certificates INFO[0001] [certificates] Generating Node certificate INFO[0001] [certificates] Generating admin certificates and kubeconfig INFO[0001] [certificates] Generating Kubernetes API server proxy client certificates INFO[0001] [certificates] Generating etcd-xxxxx certificate and key INFO[0001] [certificates] Generating etcd-yyyyy certificate and key INFO[0002] [certificates] Generating etcd-zzzzz certificate and key INFO[0002] Successfully Deployed state file at [./cluster.rkestate] INFO[0002] Rebuilding Kubernetes cluster with rotated certificates ..... INFO[0050] [worker] Successfully restarted Worker Plane..   ","version":"Next","tagName":"h3"},{"title":"Rotating a Certificate on an Individual Service while using the same CA​","type":1,"pageTitle":"Certificate Management","url":"/cert-mgmt#rotating-a-certificate-on-an-individual-service-while-using-the-same-ca","content":" To rotate the certificate for an individual Kubernetes service, use the --service option when rotating certificates to specify the service. After the specified Kubernetes service has had its certificate rotated, it is automatically restarted to start using the new certificate.  Example of rotating the certificate for only the kubelet:  $ rke cert rotate --service kubelet INFO[0000] Initiating Kubernetes cluster INFO[0000] Rotating Kubernetes cluster certificates INFO[0000] [certificates] Generating Node certificate INFO[0000] Successfully Deployed state file at [./cluster.rkestate] INFO[0000] Rebuilding Kubernetes cluster with rotated certificates ..... INFO[0033] [worker] Successfully restarted Worker Plane..   ","version":"Next","tagName":"h3"},{"title":"Rotating the CA and all service certificates​","type":1,"pageTitle":"Certificate Management","url":"/cert-mgmt#rotating-the-ca-and-all-service-certificates","content":" If the CA certificate needs to be rotated, you are required to rotate all the services certificates as they need to be signed with the newly rotated CA certificate. To include rotating the CA with the service certificates, add the --rotate-ca option. After the CA and all the service certificates are rotated, these services will automatically be restarted to start using the new certificate.  Rotating the CA certificate will result in restarting other system pods, that will also use the new CA certificate. This includes:  Networking pods (canal, calico and flannel)Ingress Controller podsKubeDNS pods  $ rke cert rotate --rotate-ca INFO[0000] Initiating Kubernetes cluster INFO[0000] Rotating Kubernetes cluster certificates INFO[0000] [certificates] Generating CA kubernetes certificates INFO[0000] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates INFO[0000] [certificates] Generating Kubernetes API server certificates INFO[0000] [certificates] Generating Kube Controller certificates INFO[0000] [certificates] Generating Kube Scheduler certificates INFO[0000] [certificates] Generating Kube Proxy certificates INFO[0000] [certificates] Generating Node certificate INFO[0001] [certificates] Generating admin certificates and kubeconfig INFO[0001] [certificates] Generating Kubernetes API server proxy client certificates INFO[0001] [certificates] Generating etcd-xxxxx certificate and key INFO[0001] [certificates] Generating etcd-yyyyy certificate and key INFO[0001] [certificates] Generating etcd-zzzzz certificate and key INFO[0001] Successfully Deployed state file at [./cluster.rkestate] INFO[0001] Rebuilding Kubernetes cluster with rotated certificates  ","version":"Next","tagName":"h3"},{"title":"Custom Cloud Provider","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/custom","content":"Custom Cloud Provider If you want to enable a different cloud provider, RKE allows for custom cloud provider options. A name must be provided and the custom Cloud Provider options can be passed in as a multiline string in customCloudProvider. For example, in order to use the oVirt cloud provider with Kubernetes, here's the following cloud provider information: [connection] uri = https://localhost:8443/ovirt-engine/api username = admin@internal password = admin To add this cloud config file to RKE, the cloud_provider would be need to be set. cloud_provider: name: ovirt # Note the pipe as this is what indicates a multiline string customCloudProvider: |- [connection] uri = https://localhost:8443/ovirt-engine/api username = admin@internal password = admin ","keywords":"","version":"Next"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/config-options/authentication","content":"Authentication RKE supports x509 authentication strategy. You can additionally define a list of SANs (Subject Alternative Names) to add to the Kubernetes API Server PKI certificates. As an example, this allows you to connect to your Kubernetes cluster API Server through a load balancer instead of a single node. authentication: strategy: x509 sans: - &quot;10.18.160.10&quot; - &quot;my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com&quot; RKE also supports the webhook authentication strategy. You can enable both x509 and webhook strategies by using a | separator in the configuration. Contents of the webhook config file should be provided, see Kubernetes webhook documentation for information on the file format. Additionally, a cache timeout for webhook authentication responses can be set. authentication: strategy: x509|webhook webhook: config_file: &quot;....&quot; cache_timeout: 5s ","keywords":"","version":"Next"},{"title":"Metrics Server","type":0,"sectionRef":"#","url":"/config-options/add-ons/metrics-server","content":"","keywords":"","version":"Next"},{"title":"Tolerations​","type":1,"pageTitle":"Metrics Server","url":"/config-options/add-ons/metrics-server#tolerations","content":" Available as of v1.2.4  The configured tolerations apply to the metrics-server Deployment.  monitoring: tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300   To check for applied tolerations on the metrics-server Deployment, use the following commands:  kubectl -n kube-system get deploy metrics-server -o jsonpath='{.spec.template.spec.tolerations}'   ","version":"Next","tagName":"h3"},{"title":"Metrics Server Priority Class Name​","type":1,"pageTitle":"Metrics Server","url":"/config-options/add-ons/metrics-server#metrics-server-priority-class-name","content":" Available as of RKE v1.2.6+  The pod priority is set by configuring a priority class name:  monitoring: provider: metrics-server metrics_server_priority_class_name: system-cluster-critical   ","version":"Next","tagName":"h3"},{"title":"Disabling the Metrics Server​","type":1,"pageTitle":"Metrics Server","url":"/config-options/add-ons/metrics-server#disabling-the-metrics-server","content":" Available as of v0.2.0  You can disable the default controller by specifying none to the monitoring provider directive in the cluster configuration.  monitoring: provider: none  ","version":"Next","tagName":"h3"},{"title":"Bastion/Jump Host Configuration","type":0,"sectionRef":"#","url":"/config-options/bastion-host","content":"","keywords":"","version":"Next"},{"title":"Bastion Host Options​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#bastion-host-options","content":" ","version":"Next","tagName":"h2"},{"title":"Address​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#address","content":" The address directive will be used to set the hostname or IP address of the bastion host. RKE must be able to connect to this address.  ","version":"Next","tagName":"h3"},{"title":"SSH Port​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#ssh-port","content":" You specify which port to be used when connecting to the bastion host. The default port is 22.  ","version":"Next","tagName":"h3"},{"title":"SSH Users​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#ssh-users","content":" You specify the user to be used when connecting to this node.  ","version":"Next","tagName":"h3"},{"title":"SSH Key Path​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#ssh-key-path","content":" You specify the path, i.e. ssh_key_path, for the SSH private key to be used when connecting to the bastion host.  ","version":"Next","tagName":"h3"},{"title":"SSH Key​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#ssh-key","content":" Instead of setting the path to the SSH key, you can specify the actual key, i.e. ssh_key, to be used to connect to the bastion host.  ","version":"Next","tagName":"h3"},{"title":"SSH Certificate Path​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#ssh-certificate-path","content":" You specify the path, i.e. ssh_cert_path, for the signed SSH certificate to be used when connecting to the bastion host.  ","version":"Next","tagName":"h3"},{"title":"SSH Certificate​","type":1,"pageTitle":"Bastion/Jump Host Configuration","url":"/config-options/bastion-host#ssh-certificate","content":" Instead of setting the path to the signed SSH certificate, you can specify the actual certificate, i.e. ssh_cert, to be used to connect to the bastion host. ","version":"Next","tagName":"h3"},{"title":"Kubernetes Configuration Options","type":0,"sectionRef":"#","url":"/config-options","content":"","keywords":"","version":"Next"},{"title":"Configuring Nodes​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#configuring-nodes","content":" NodesIgnoring unsupported Docker versionsPrivate RegistriesCluster Level SSH Key PathSSH AgentBastion Host  ","version":"Next","tagName":"h3"},{"title":"Configuring Kubernetes Cluster​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#configuring-kubernetes-cluster","content":" Cluster NameKubernetes VersionPrefix PathSystem ImagesServicesExtra Args and Binds and Environment VariablesExternal EtcdAuthenticationAuthorizationRate LimitingCloud ProvidersAudit LogAdd-ons Network Plug-insDNS providersIngress ControllersMetrics ServerUser-Defined Add-onsAdd-ons Job Timeout  ","version":"Next","tagName":"h3"},{"title":"Cluster Level Options​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#cluster-level-options","content":" ","version":"Next","tagName":"h2"},{"title":"Cluster Name​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#cluster-name","content":" By default, the name of your cluster will be local. If you want a different name, you would use the cluster_name directive to change the name of your cluster. The name will be set in your cluster's generated kubeconfig file.  cluster_name: mycluster   ","version":"Next","tagName":"h3"},{"title":"Supported Docker Versions​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#supported-docker-versions","content":" By default, RKE will check the installed Docker version on all hosts and fail with an error if the version is not supported by Kubernetes. The list of supported Docker versions is set specifically for each Kubernetes version in kontainer-driver-metadata depending on the RKE version used, as shown below. To override this behavior, set this option to true. Refer to the following:  For RKE v1.3.x, see this link.For RKE v1.2.x, see this link.  The default value is false.  ignore_docker_version: true   ","version":"Next","tagName":"h3"},{"title":"Kubernetes Version​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#kubernetes-version","content":" For information on upgrading Kubernetes, refer to the upgrade section.  Rolling back to previous Kubernetes versions is not supported.  ","version":"Next","tagName":"h3"},{"title":"Prefix Path​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#prefix-path","content":" For some operating systems including ROS, and CoreOS, RKE stores its resources to a different prefix path, this prefix path is by default for these operating systems is:  /opt/rke   So /etc/kubernetes will be stored in /opt/rke/etc/kubernetes and /var/lib/etcd will be stored in /opt/rke/var/lib/etcd etc.  To change the default prefix path for any cluster, you can use the following option in the cluster configuration file cluster.yml:  prefix_path: /opt/custom_path   ","version":"Next","tagName":"h3"},{"title":"Cluster Level SSH Key Path​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#cluster-level-ssh-key-path","content":" RKE connects to host(s) using ssh. Typically, each node will have an independent path for each ssh key, i.e. ssh_key_path, in the nodes section, but if you have a SSH key that is able to access all hosts in your cluster configuration file, you can set the path to that ssh key at the top level. Otherwise, you would set the ssh key path in the nodes.  If ssh key paths are defined at the cluster level and at the node level, the node-level key will take precedence.  ssh_key_path: ~/.ssh/test   ","version":"Next","tagName":"h3"},{"title":"SSH Agent​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#ssh-agent","content":" RKE supports using ssh connection configuration from a local ssh agent. The default value for this option is false. If you want to set using a local ssh agent, you would set this to true.  ssh_agent_auth: true   If you want to use an SSH private key with a passphrase, you will need to add your key to ssh-agent and have the environment variable SSH_AUTH_SOCK configured.  $ eval &quot;$(ssh-agent -s)&quot; Agent pid 3975 $ ssh-add /home/user/.ssh/id_rsa Enter passphrase for /home/user/.ssh/id_rsa: Identity added: /home/user/.ssh/id_rsa (/home/user/.ssh/id_rsa) $ echo $SSH_AUTH_SOCK /tmp/ssh-118TMqxrXsEx/agent.3974   ","version":"Next","tagName":"h3"},{"title":"Add-ons Job Timeout​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#add-ons-job-timeout","content":" You can define add-ons to be deployed after the Kubernetes cluster comes up, which uses Kubernetes jobs. RKE will stop attempting to retrieve the job status after the timeout, which is in seconds. The default timeout value is 30 seconds.  ","version":"Next","tagName":"h3"},{"title":"cri-dockerd​","type":1,"pageTitle":"Kubernetes Configuration Options","url":"/config-options#cri-dockerd","content":" Kubernetes will remove code in the kubelet that interacts with Docker (dockershim) in a future Kubernetes release. For more information, see Dockershim Deprecation FAQ: When will dockershim be removed?. The component that replaces this code is called cri-dockerd and can be enabled using the following configuration:  enable_cri_dockerd: true  ","version":"Next","tagName":"h3"},{"title":"Add-Ons","type":0,"sectionRef":"#","url":"/config-options/add-ons","content":"","keywords":"","version":"Next"},{"title":"Critical and Non-Critical Add-ons​","type":1,"pageTitle":"Add-Ons","url":"/config-options/add-ons#critical-and-non-critical-add-ons","content":" As of version v0.1.7, add-ons are split into two categories:  Critical add-ons: If these add-ons fail to deploy for any reason, RKE will error out. All system add-ons, such as the network plug-in, KubeDNS, and ingress controllers, are considered critical. Non-critical add-ons: If these add-ons fail to deploy, RKE will only log a warning and continue deploying any other add-ons. User-defined add-ons are considered non-critical.  ","version":"Next","tagName":"h2"},{"title":"Add-on Deployment Jobs​","type":1,"pageTitle":"Add-Ons","url":"/config-options/add-ons#add-on-deployment-jobs","content":" RKE uses Kubernetes jobs to deploy add-ons. In some cases, add-ons deployment takes longer than expected. As of with version v0.1.7, RKE provides an option to control the job check timeout in seconds. This timeout is set at the cluster level.  addon_job_timeout: 30   ","version":"Next","tagName":"h2"},{"title":"Add-on Placement​","type":1,"pageTitle":"Add-Ons","url":"/config-options/add-ons#add-on-placement","content":" Applies to v0.2.3 and higher  Component\tnodeAffinity nodeSelectorTerms\tnodeSelector\tTolerationsCalico\tbeta.kubernetes.io/os:NotIn:windows\tnone\t- NoSchedule:Exists - NoExecute:Exists - CriticalAddonsOnly:Exists Flannel\tbeta.kubernetes.io/os:NotIn:windows\tnone\t- operator:Exists Canal\tbeta.kubernetes.io/os:NotIn:windows\tnone\t- NoSchedule:Exists - NoExecute:Exists - CriticalAddonsOnly:Exists Weave\tbeta.kubernetes.io/os:NotIn:windows\tnone\t- NoSchedule:Exists - NoExecute:Exists CoreDNS\tnode-role.kubernetes.io/worker:Exists\tbeta.kubernetes.io/os:linux\t- NoSchedule:Exists - NoExecute:Exists - CriticalAddonsOnly:Exists kube-dns\t- beta.kubernetes.io/os:NotIn:windows - node-role.kubernetes.io/worker Exists\tnone\t- NoSchedule:Exists - NoExecute:Exists - CriticalAddonsOnly:Exists nginx-ingress\t- beta.kubernetes.io/os:NotIn:windows - node-role.kubernetes.io/worker Exists\tnone\t- NoSchedule:Exists - NoExecute:Exists metrics-server\t- beta.kubernetes.io/os:NotIn:windows - node-role.kubernetes.io/worker Exists\tnone\t- NoSchedule:Exists - NoExecute:Exists  ","version":"Next","tagName":"h2"},{"title":"Tolerations​","type":1,"pageTitle":"Add-Ons","url":"/config-options/add-ons#tolerations","content":" Available as of v1.2.4  Tolerations can be configured per add-on and apply to Deployment resources. The configured tolerations will replace the existing tolerations so make sure you configure all the tolerations you need. See the specific add-on doc pages for more information. ","version":"Next","tagName":"h2"},{"title":"Azure Cloud Provider","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/azure","content":"","keywords":"","version":"Next"},{"title":"Overriding the hostname​","type":1,"pageTitle":"Azure Cloud Provider","url":"/config-options/cloud-providers/azure#overriding-the-hostname","content":" Since the Azure node name must match the Kubernetes node name, you override the Kubernetes name on the node by setting the hostname_override for each node. If you do not set the hostname_override, the Kubernetes node name will be set as the address, which will cause the Azure cloud provider to fail.  nodes: - address: x.x.x.x hostname_override: azure-rke1 user: ubuntu role: - controlplane - etcd - worker   ","version":"Next","tagName":"h2"},{"title":"Azure Configuration Options​","type":1,"pageTitle":"Azure Cloud Provider","url":"/config-options/cloud-providers/azure#azure-configuration-options","content":" Besides the minimum set of options, there are many other options that are supported in RKE:  Azure Configuration Options\tType\tRequired\tDescriptiontenantId\tstring\t*\tThe Azure Active Directory (Azure AD) tenant ID for the subscription that the cluster is deployed in. subscriptionId\tstring\t*\tThe ID of the Azure subscription that the cluster is deployed in. aadClientId\tstring\t*\tThe client ID for an Azure AD application with RBAC access to talk to Azure Resource Manager APIs. This is used for service principal authentication. aadClientSecret\tstring\t*\tThe client secret for an Azure AD application with RBAC access to talk to Azure Resource Manager APIs. This is used for service principal authentication. cloud\tstring The cloud environment identifier. Takes values from here. resourceGroup\tstring The name of the resource group that the Vnet is deployed in. location\tstring The location of the resource group that the cluster is deployed in. vnetName\tstring The name of the virtual network that the cluster is deployed in. vnetResourceGroup\tstring The name of the resource group that the virtual network is deployed in. subnetName\tstring The name of the subnet that the cluster is deployed in. securityGroupName\tstring The name of the security group attached to the cluster's subnet. routeTableName\tstring The name of the route table attached to the subnet that the cluster is deployed in. primaryAvailabilitySetName\tstring The name of the availability set that should be used as the load balancer backend. If this is set, the Azure cloud provider will only add nodes from that availability set to the load balancer backend pool. If this is not set, and multiple agent pools (availability sets) are used, then the cloud provider will try to add all nodes to a single backend pool which is forbidden. In other words, if you use multiple agent pools (availability sets), you must set this field. vmType\tstring The type of Azure nodes. Candidate values are: vmss and standard. If not set, it will be default to standard. Set to vmss if the cluster is running on Azure virtual machine scale sets instead of standard machines. primaryScaleSetName\tstring The name of the scale set that should be used as the load balancer backend. If this is set, the Azure cloud provider will only add nodes from that scale set to the load balancer backend pool. If this is not set, and multiple agent pools (scale sets) are used, then the cloud provider will try to add all nodes to a single backend pool which is forbidden. In other words, if you use multiple agent pools (scale sets), you must set this field. aadClientCertPath\tstring The path of a client certificate for an Azure AD application with RBAC access to talk to Azure Resource Manager APIs. This is used for client certificate authentication. aadClientCertPassword\tstring The password of the client certificate for an Azure AD application with RBAC access to talk to Azure Resource Manager APIs. This is used for client certificate authentication. cloudProviderBackoff\tbool Enable exponential backoff to manage resource request retries. cloudProviderBackoffRetries\tint Backoff retry limit. cloudProviderBackoffExponent\tint Backoff exponent. cloudProviderBackoffDuration\tint Backoff duration. cloudProviderBackoffJitter\tint Backoff jitter. cloudProviderRateLimit\tbool Enable rate limiting. cloudProviderRateLimitQPS\tint Rate limit QPS. cloudProviderRateLimitBucket\tint Rate limit bucket Size. useInstanceMetadata\tbool Use instance metadata service where possible. useManagedIdentityExtension\tbool Use managed service identity for the virtual machine to access Azure Resource Manager APIs. This is used for managed identity authentication. For user-assigned managed identity, UserAssignedIdentityID needs to be set. UserAssignedIdentityID\tstring The client ID of the user assigned Managed Service Identity (MSI) which is assigned to the underlying VMs. This is used for managed identity authentication. maximumLoadBalancerRuleCount\tint The limit enforced by Azure Load balancer. The default is 0 and maximum is 148. LoadBalancerSku\tstring SKU of the load balancer and public IP. Valid values are basic or standard. Default(blank) to basic. ExcludeMasterFromStandardLB\tbool Excludes master nodes (labeled with node-role.kubernetes.io/master) from the backend pool of Azure standard loadbalancer. Defaults to nil. ","version":"Next","tagName":"h2"},{"title":"vSphere Cloud Provider","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/vsphere","content":"","keywords":"","version":"Next"},{"title":"Related Links​","type":1,"pageTitle":"vSphere Cloud Provider","url":"/config-options/cloud-providers/vsphere#related-links","content":" Configuration: For details on vSphere configuration in RKE, refer to the configuration reference.Troubleshooting: For guidance on troubleshooting a cluster with the vSphere cloud provider enabled, refer to the troubleshooting section.Storage: If you are setting up storage, see the official vSphere documentation on storage for Kubernetes, or the official Kubernetes documentation on persistent volumes. If you are using Rancher, refer to the Rancher documentation on provisioning storage in vSphere.For Rancher users: Refer to the Rancher documentation on creating vSphere Kubernetes clusters and provisioning storage.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"vSphere Cloud Provider","url":"/config-options/cloud-providers/vsphere#prerequisites","content":" Credentials: You'll need to have credentials of a vCenter/ESXi user account with privileges allowing the cloud provider to interact with the vSphere infrastructure to provision storage. Refer to this document to create and assign a role with the required permissions in vCenter.VMware Tools must be running in the Guest OS for all nodes in the cluster.Disk UUIDs: All nodes must be configured with disk UUIDs. This is required so that attached VMDKs present a consistent UUID to the VM, allowing the disk to be mounted properly. See the section on enabling disk UUIDs.  ","version":"Next","tagName":"h2"},{"title":"Enabling the vSphere Provider with the RKE CLI​","type":1,"pageTitle":"vSphere Cloud Provider","url":"/config-options/cloud-providers/vsphere#enabling-the-vsphere-provider-with-the-rke-cli","content":" To enable the vSphere Cloud Provider in the cluster, you must add the top-level cloud_provider directive to the cluster configuration file, set the name property to vsphere and add the vsphereCloudProvider directive containing the configuration matching your infrastructure. See the configuration reference for the gory details. ","version":"Next","tagName":"h2"},{"title":"OpenStack Cloud Provider","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/openstack","content":"","keywords":"","version":"Next"},{"title":"Overriding the hostname​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#overriding-the-hostname","content":" The OpenStack cloud provider uses the instance name (as determined from OpenStack metadata) as the name of the Kubernetes Node object, you must override the Kubernetes name on the node by setting the hostname_override for each node. If you do not set the hostname_override, the Kubernetes node name will be set as the address, which will cause the OpenStack cloud provider to fail.  ","version":"Next","tagName":"h2"},{"title":"OpenStack Configuration Options​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#openstack-configuration-options","content":" The OpenStack configuration options are divided into 5 groups.  GlobalLoad BalancerBlock StorageRouteMetadata  ","version":"Next","tagName":"h2"},{"title":"Global​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#global","content":" These are the options that are available under the global directive.  OpenStack's Global Configuration Options\tType\tRequiredauth_url\tstring\t* username\tstring\t* user-id\tstring\t* password\tstring\t* tenant-id\tstring\t* tenant-name\tstring trust-id\tstring domain-id\tstring domain-name\tstring region\tstring ca-file\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Load Balancer​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#load-balancer","content":" These are the options that are available under the load_balancer directive.  OpenStack's Load Balancer Configuration Options\tType\tRequiredlb-version\tstring use-octavia\tbool subnet-id\tstring floating-network-id\tstring lb-method\tstring lb-provider\tstring manage-security-groups\tbool create-monitor\tbool monitor-delay\tint\t* if create-monitor is true monitor-timeout\tint\t* if create-monitor is true monitor-max-retries\tint\t* if create-monitor is true  ","version":"Next","tagName":"h3"},{"title":"Block Storage​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#block-storage","content":" These are the options that are available under the block_storage directive.  OpenStack's Block Storage Configuration Options\tType\tRequiredbs-version\tstring trust-device-path\tbool ignore-volume-az\tbool\t  ","version":"Next","tagName":"h3"},{"title":"Route​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#route","content":" This is the option that is available under the route directive.  OpenStack's Route Configuration Option\tType\tRequiredrouter-id\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Metadata​","type":1,"pageTitle":"OpenStack Cloud Provider","url":"/config-options/cloud-providers/openstack#metadata","content":" These are the options that are available under the metadata directive.  OpenStack's Metadata Configuration Options\tType\tRequiredsearch-order\tstring request-timeout\tint\t  For more information of OpenStack configurations options please refer to the official Kubernetes documentation. ","version":"Next","tagName":"h3"},{"title":"vSphere Configuration Reference","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/vsphere/config-reference","content":"","keywords":"","version":"Next"},{"title":"vSphere Configuration Example​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#vsphere-configuration-example","content":" Given the following:  VMs in the cluster are running in the same datacenter eu-west-1 managed by the vCenter vc.example.com.The vCenter has a user provisioner with password secret with the required roles assigned, see Prerequisites.The vCenter has a datastore named ds-1 which should be used to store the VMDKs for volumes.A vm/kubernetes folder exists in vCenter.  The corresponding configuration for the provider would then be as follows:  rancher_kubernetes_engine_config: (...) cloud_provider: name: vsphere vsphereCloudProvider: virtual_center: vc.example.com: user: provisioner password: secret port: 443 datacenters: /eu-west-1 workspace: server: vc.example.com folder: myvmfolder default-datastore: ds-1 datacenter: /eu-west-1 resourcepool-path: /eu-west-1/host/hn1/resources/myresourcepool   ","version":"Next","tagName":"h2"},{"title":"Configuration Options​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#configuration-options","content":" The vSphere configuration options are divided into 5 groups:  globalvirtual_centerworkspacedisknetwork  ","version":"Next","tagName":"h2"},{"title":"global​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#global","content":" The main purpose of global options is to be able to define a common set of configuration parameters that will be inherited by all vCenters defined under the virtual_center directive unless explicitly defined there.  Accordingly, the global directive accepts the same configuration options that are available under the virtual_center directive. Additionally it accepts a single parameter that can only be specified here:  global Options\tType\tRequired\tDescriptioninsecure-flag\tboolean Set to true if the vCenter/ESXi uses a self-signed certificate.  Example:  (...) global: insecure-flag: true   ","version":"Next","tagName":"h3"},{"title":"virtual_center​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#virtual_center","content":" This configuration directive specifies the vCenters that are managing the nodes in the cluster. You must define at least one vCenter/ESXi server. If the nodes span multiple vCenters then all must be defined.  Each vCenter is defined by adding a new entry under the virtual_center directive with the vCenter IP or FQDN as the name. All required parameters must be provided for each vCenter unless they are already defined under the global directive.  virtual_center Options\tType\tRequired\tDescriptionuser\tstring\t*\tvCenter/ESXi user used to authenticate with this server. password\tstring\t*\tUser's password. port\tstring Port to use to connect to this server. Defaults to 443. datacenters\tstring\t*\tComma-separated list of all datacenters in which cluster nodes are running in. soap-roundtrip-count\tuint Round tripper count for API requests to the vCenter (num retries = value - 1).  note The following additional options (introduced in Kubernetes v1.11) are not yet supported in RKE.  virtual_center Options\tType\tRequired\tDescriptionsecret-name\tstring Name of secret resource containing credential key/value pairs. Can be specified in lieu of user/password parameters. secret-namespace\tstring Namespace in which the secret resource was created in. ca-file\tstring Path to CA cert file used to verify the vCenter certificate.  Example:  (...) virtual_center: 172.158.111.1: {} # This vCenter inherits all it's properties from global options 172.158.110.2: # All required options are set explicitly user: vc-user password: othersecret datacenters: eu-west-2   ","version":"Next","tagName":"h3"},{"title":"workspace​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#workspace","content":" This configuration group specifies how storage for volumes is created in vSphere. The following configuration options are available:  workspace Options\tType\tRequired\tDescriptionserver\tstring\t*\tIP or FQDN of the vCenter/ESXi that should be used for creating the volumes. Must match one of the vCenters defined under the virtual_center directive. datacenter\tstring\t*\tName of the datacenter that should be used for creating volumes. For ESXi enter ha-datacenter. folder\tstring\t*\tPath of folder in which to create dummy VMs used for volume provisioning (relative from the root folder in vCenter), e.g. &quot;vm/kubernetes&quot;. default-datastore\tstring Name of default datastore to place VMDKs if neither datastore or storage policy are specified in the volume options of a PVC. If datastore is located in a storage folder or is a member of a datastore cluster, specify the full path. resourcepool-path\tstring Absolute or relative path to the resource pool where the dummy VMs for Storage policy based provisioning should be created. If a relative path is specified, it is resolved with respect to the datacenter's host folder. Examples: /&lt;dataCenter&gt;/host/&lt;hostOrClusterName&gt;/Resources/&lt;poolName&gt;, Resources/&lt;poolName&gt;. For standalone ESXi specify Resources.  Example:  (...) workspace: server: 172.158.111.1 # matches IP of vCenter defined in the virtual_center block datacenter: eu-west-1 folder: vm/kubernetes default-datastore: ds-1   ","version":"Next","tagName":"h3"},{"title":"disk​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#disk","content":" The following configuration options are available under the disk directive:  disk Options\tType\tRequired\tDescriptionscsicontrollertype\tstring SCSI controller type to use when attaching block storage to VMs. Must be one of: lsilogic-sas or pvscsi. Default: pvscsi.  ","version":"Next","tagName":"h3"},{"title":"network​","type":1,"pageTitle":"vSphere Configuration Reference","url":"/config-options/cloud-providers/vsphere/config-reference#network","content":" The following configuration options are available under the network directive:  network Options\tType\tRequired\tDescriptionpublic-network\tstring Name of public VM Network to which the VMs in the cluster are connected. Used to determine public IP addresses of VMs. ","version":"Next","tagName":"h3"},{"title":"Custom Network Plug-in Example","type":0,"sectionRef":"#","url":"/config-options/add-ons/network-plugins/custom-network-plugin-example","content":"Custom Network Plug-in Example The below example shows how to configure a custom network plug-in with an in-line add-on to the cluster.yml. First, to edit the network plug-ins, change the network section of the YAML from: network: options: flannel_backend_type: &quot;vxlan&quot; plugin: &quot;canal&quot; to: network: plugin: none Then, in the addons section of the cluster.yml, you can add the add-on manifest of a cluster that has the network plugin-that you want. In the below example, we are replacing the Canal plugin with a Flannel plugin by adding the add-on manifest for the cluster through the addons field: addons: |- --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: - &quot;&quot; resources: - pods verbs: - get - apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: &quot;kube-system&quot; labels: tier: node app: flannel data: cni-conf.json: | { &quot;name&quot;:&quot;cbr0&quot;, &quot;cniVersion&quot;:&quot;0.3.1&quot;, &quot;plugins&quot;:[ { &quot;type&quot;:&quot;flannel&quot;, &quot;delegate&quot;:{ &quot;forceAddress&quot;:true, &quot;isDefaultGateway&quot;:true } }, { &quot;type&quot;:&quot;portmap&quot;, &quot;capabilities&quot;:{ &quot;portMappings&quot;:true } } ] } net-conf.json: | { &quot;Network&quot;: &quot;10.42.0.0/16&quot;, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } } --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel namespace: &quot;kube-system&quot; labels: tier: node k8s-app: flannel spec: template: metadata: labels: tier: node k8s-app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: NotIn values: - windows serviceAccountName: flannel containers: - name: kube-flannel image: rancher/coreos-flannel:v0.10.0-rancher1 imagePullPolicy: IfNotPresent resources: limits: cpu: 300m memory: 500M requests: cpu: 150m memory: 64M command: [&quot;/opt/bin/flanneld&quot;,&quot;--ip-masq&quot;,&quot;--kube-subnet-mgr&quot;] securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: install-cni image: rancher/flannel-cni:v0.3.0-rancher1 command: [&quot;/install-cni.sh&quot;] env: # The CNI network config to install on each node. - name: CNI_NETWORK_CONFIG valueFrom: configMapKeyRef: name: kube-flannel-cfg key: cni-conf.json - name: CNI_CONF_NAME value: &quot;10-flannel.conflist&quot; volumeMounts: - name: cni mountPath: /host/etc/cni/net.d - name: host-cni-bin mountPath: /host/opt/cni/bin/ hostNetwork: true tolerations: - operator: Exists effect: NoSchedule - operator: Exists effect: NoExecute - key: node.kubernetes.io/not-ready effect: NoSchedule operator: Exists volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg - name: host-cni-bin hostPath: path: /opt/cni/bin updateStrategy: rollingUpdate: maxUnavailable: 20% type: RollingUpdate --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system Result: The cluster is up with the custom network plug-in.","keywords":"","version":"Next"},{"title":"Troubleshooting vSphere Clusters","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/vsphere/troubleshooting","content":"Troubleshooting vSphere Clusters If you are experiencing issues while provisioning a cluster with enabled vSphere Cloud Provider or while creating vSphere volumes for your workloads, you should inspect the logs of the following K8s services: controller-manager (Manages volumes in vCenter)kubelet: (Mounts vSphere volumes to pods) If your cluster is not configured with external Cluster Logging, you will need to SSH into nodes to get the logs of the kube-controller-manager (running on one of the control plane nodes) and the kubelet (pertaining to the node where the stateful pod has been scheduled). The easiest way to create a SSH session with a node is the Rancher CLI tool. Configure the Rancher CLI for your cluster.Run the following command to get a shell to the corresponding nodes: $ rancher ssh &lt;nodeName&gt; Inspect the logs of the controller-manager and kubelet containers looking for errors related to the vSphere cloud provider: $ docker logs --since 15m kube-controller-manager $ docker logs --since 15m kubelet ","keywords":"","version":"Next"},{"title":"User-Defined Add-Ons","type":0,"sectionRef":"#","url":"/config-options/add-ons/user-defined-add-ons","content":"","keywords":"","version":"Next"},{"title":"In-line Add-ons​","type":1,"pageTitle":"User-Defined Add-Ons","url":"/config-options/add-ons/user-defined-add-ons#in-line-add-ons","content":" To define an add-on directly in the YAML file, make sure to use the YAML's block indicator |- as the addons directive is a multi-line string option. It's possible to specify multiple YAML resource definitions by separating them using the --- directive.  addons: |- --- apiVersion: v1 kind: Pod metadata: name: my-nginx namespace: default spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80   ","version":"Next","tagName":"h2"},{"title":"Referencing YAML files for Add-ons​","type":1,"pageTitle":"User-Defined Add-Ons","url":"/config-options/add-ons/user-defined-add-ons#referencing-yaml-files-for-add-ons","content":" Use the addons_include directive to reference a local file or a URL for any user-defined add-ons.  addons_include: - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml - /opt/manifests/example.yaml - ./nginx.yaml  ","version":"Next","tagName":"h2"},{"title":"K8s Ingress Controllers","type":0,"sectionRef":"#","url":"/config-options/add-ons/ingress-controllers","content":"","keywords":"","version":"Next"},{"title":"Default Ingress​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#default-ingress","content":" By default, RKE deploys the NGINX ingress controller on all schedulable nodes.  note As of v0.1.8, only workers are considered schedulable nodes, but before v0.1.8, worker and controlplane nodes were considered schedulable nodes.  RKE will deploy the ingress controller as a DaemonSet with hostNetwork: true, so ports 80, and 443 will be opened on each node where the controller is deployed.  note As of v1.1.11, the network options of the ingress controller are configurable. See Configuring network options.  The images used for ingress controller is under the system_images directive. For each Kubernetes version, there are default images associated with the ingress controller, but these can be overridden by changing the image tag in system_images.  ","version":"Next","tagName":"h3"},{"title":"Scheduling Ingress Controllers​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#scheduling-ingress-controllers","content":" If you only wanted ingress controllers to be deployed on specific nodes, you can set a node_selector for the ingress. The label in the node_selector would need to match the label on the nodes for the ingress controller to be deployed.  nodes: - address: 1.1.1.1 role: [controlplane,worker,etcd] user: root labels: app: ingress ingress: provider: nginx node_selector: app: ingress   ","version":"Next","tagName":"h3"},{"title":"Ingress Priority Class Name​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#ingress-priority-class-name","content":" Available as of RKE v1.2.6+  The pod priority is set by configuring a priority class name:  ingress: provider: nginx ingress_priority_class_name: system-cluster-critical   ","version":"Next","tagName":"h3"},{"title":"Tolerations​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#tolerations","content":" Available as of v1.2.4  The configured tolerations apply to the default-http-backend Deployment.  ingress: tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300   To check for applied tolerations default-http-backend Deployment, use the following commands:  kubectl -n ingress-nginx get deploy default-http-backend -o jsonpath='{.spec.template.spec.tolerations}'   ","version":"Next","tagName":"h3"},{"title":"Disabling the Default Ingress Controller​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#disabling-the-default-ingress-controller","content":" You can disable the default controller by specifying none to the ingress provider directive in the cluster configuration.  ingress: provider: none   ","version":"Next","tagName":"h3"},{"title":"Configuring NGINX Ingress Controller​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#configuring-nginx-ingress-controller","content":" For the configuration of NGINX, there are configuration options available in Kubernetes. There are a list of options for the NGINX config map , command line extra_args and annotations.  ingress: provider: nginx options: map-hash-bucket-size: &quot;128&quot; ssl-protocols: SSLv2 extra_args: enable-ssl-passthrough: &quot;&quot;   ","version":"Next","tagName":"h3"},{"title":"Disabling NGINX Ingress Default Backend​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#disabling-nginx-ingress-default-backend","content":" As of v0.20.0, you can disable the default backend service for the ingress controller. This is possible because ingress-nginx will fall back to a local 404 page, and does not require a backend service. The service can be enabled/disabled with a boolean value.  ingress: default_backend: false   What happens if the field is omitted? The value of default_backend will default to true. This maintains behavior with older versions of rke. However, a future version of rke will change the default value to false.  ","version":"Next","tagName":"h3"},{"title":"Configuring network options​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#configuring-network-options","content":" v1.3.xv1.1.11 and up &amp; v1.2.x For Kubernetes v1.21 and up, the NGINX ingress controller no longer runs in hostNetwork: true but uses hostPorts for port 80 and port 443. This was done so the admission webhook can be configured to be accessed using ClusterIP so it can only be reached inside the cluster. If you want to change the mode and/or the ports, see the options below.  Here's an example of how to configure the nginx ingress controller using hostPort and override the default ports:  ingress: provider: nginx network_mode: hostPort http_port: 9090 https_port: 9443 extra_args: http-port: 3080 https-port: 3443   Here's an example of how to configure the nginx ingress controller using hostNetwork:  ingress: provider: nginx network_mode: hostNetwork   Here's an example of how to configure the nginx ingress controller with no network mode which will make it run on the overlay network (for example, if you want to expose the nginx ingress controller using a LoadBalancer) and override the default ports:  ingress: provider: nginx network_mode: none extra_args: http-port: 8080 https-port: 8443   ","version":"Next","tagName":"h3"},{"title":"Configuring an NGINX Default Certificate​","type":1,"pageTitle":"K8s Ingress Controllers","url":"/config-options/add-ons/ingress-controllers#configuring-an-nginx-default-certificate","content":" When configuring an ingress object with TLS termination, you must provide it with a certificate used for encryption/decryption. Instead of explicitly defining a certificate each time you configure an ingress, you can set up a custom certificate that's used by default.  Setting up a default certificate is especially helpful in environments where a wildcard certificate is used, as the certificate can be applied in multiple subdomains.  Prerequisites Access to the cluster.yml used to create the cluster.The PEM encoded certificate you will use as the default certificate.  Obtain or generate your certificate key pair in a PEM encoded form. Generate a Kubernetes secret from your PEM encoded certificate with the following command, substituting your certificate for mycert.cert and mycert.key. kubectl -n ingress-nginx create secret tls ingress-default-cert --cert=mycert.cert --key=mycert.key -o yaml --dry-run=true &gt; ingress-default-cert.yaml Include the contents of ingress-default-cert.yml inline with your RKE cluster.yml file. For example: addons: |- --- apiVersion: v1 data: tls.crt: [ENCODED CERT] tls.key: [ENCODED KEY] kind: Secret metadata: creationTimestamp: null name: ingress-default-cert namespace: ingress-nginx type: kubernetes.io/tls Define your ingress resource with the following default-ssl-certificate argument, which references the secret we created earlier under extra_args in your cluster.yml: ingress: provider: &quot;nginx&quot; extra_args: default-ssl-certificate: &quot;ingress-nginx/ingress-default-cert&quot; Optional: If you want to apply the default certificate to ingresses in a cluster that already exists, you must delete the NGINX ingress controller pods to have Kubernetes schedule new pods with the newly configured extra_args. kubectl delete pod -l app=ingress-nginx -n ingress-nginx  ","version":"Next","tagName":"h3"},{"title":"Audit Log","type":0,"sectionRef":"#","url":"/config-options/audit-log","content":"","keywords":"","version":"Next"},{"title":"Enabled by default​","type":1,"pageTitle":"Audit Log","url":"/config-options/audit-log#enabled-by-default","content":" In RKE v1.1.0 and higher and when using specific Kubernetes versions, audit log is enabled by default. See the table below to check when audit log is enabled by default.  RKE version\tKubernetes version\taudit log Enabledv1.1.0 and higher\tv1.17.4 and higher (v1.17.x)\tYes v1.1.0 and higher\tv1.16.8 and higher (v1.16.x)\tYes v1.1.0 and higher\tv1.15.11 and higher (v1.15.x)\tYes  ","version":"Next","tagName":"h3"},{"title":"Example Configurations​","type":1,"pageTitle":"Audit Log","url":"/config-options/audit-log#example-configurations","content":" The audit log can be enabled by default using the following configuration in cluster.yml:  services: kube-api: audit_log: enabled: true   When the audit log is enabled, you should be able to see the default values at /etc/kubernetes/audit-policy.yaml (This is located at /etc/kubernetes/audit.yaml before RKE v1.1.0):  # Minimum Configuration: Capture event metadata. ... rules: - level: Metadata ...   When the audit log is enabled, default values are also set for the audit log path, maximum age, maximum number of backups, maximum size in megabytes, and format. To see the default values, run:  ps -ef | grep kube-apiserver   The default values for audit log were changed in RKE v1.1.0 to the following:  --audit-log-maxage=30 # The maximum number of days to retain old audit log files --audit-log-maxbackup=10 # The maximum number of audit log files to retain --audit-log-path=/var/log/kube-audit/audit-log.json # The log file path that log backend uses to write audit events --audit-log-maxsize=100 # The maximum size in megabytes of the audit log file before it gets rotated --audit-policy-file=/etc/kubernetes/audit-policy.yaml # The file containing your audit log rules --audit-log-format=json # The log file format   The default values for the audit log before RKE v1.1.0 are:  --audit-log-maxage=5 # The maximum number of days to retain old audit log files --audit-log-maxbackup=5 # The maximum number of audit log files to retain --audit-log-path=/var/log/kube-audit/audit-log.json # The log file path that log backend uses to write audit events --audit-log-maxsize=100 # The maximum size in megabytes of the audit log file before it gets rotated --audit-policy-file=/etc/kubernetes/audit.yaml # The file containing your audit log rules --audit-log-format=json # The log file format   To customize the audit log, the configuration directive is used.  A rules policy is passed to kube-apiserver using the --audit-policy-file or the policy directive in the cluster.yml. Below is an example cluster.yml with custom values and an audit log policy nested under the configuration directive. This example audit log policy is taken from the official Kubernetes documentation:  services: kube-api: audit_log: enabled: true configuration: max_age: 6 max_backup: 6 max_size: 110 path: /var/log/kube-audit/audit-log.json format: json policy: apiVersion: audit.k8s.io/v1 # This is required. kind: Policy omitStages: - &quot;RequestReceived&quot; rules: # Log pod changes at RequestResponse level - level: RequestResponse resources: - group: &quot;&quot; # Resource &quot;pods&quot; doesn't match requests to any subresource of pods, # which is consistent with the RBAC policy. resources: [&quot;pods&quot;] # Log &quot;pods/log&quot;, &quot;pods/status&quot; at Metadata level - level: Metadata resources: - group: &quot;&quot; resources: [&quot;pods/log&quot;, &quot;pods/status&quot;] # Don't log requests to a configmap called &quot;controller-leader&quot; - level: None resources: - group: &quot;&quot; resources: [&quot;configmaps&quot;] resourceNames: [&quot;controller-leader&quot;] # Don't log watch requests by the &quot;system:kube-proxy&quot; on endpoints or services - level: None users: [&quot;system:kube-proxy&quot;] verbs: [&quot;watch&quot;] resources: - group: &quot;&quot; # core API group resources: [&quot;endpoints&quot;, &quot;services&quot;] # Don't log authenticated requests to certain non-resource URL paths. - level: None userGroups: [&quot;system:authenticated&quot;] nonResourceURLs: - &quot;/api*&quot; # Wildcard matching. - &quot;/version&quot; # Log the request body of configmap changes in kube-system. - level: Request resources: - group: &quot;&quot; # core API group resources: [&quot;configmaps&quot;] # This rule only applies to resources in the &quot;kube-system&quot; namespace. # The empty string &quot;&quot; can be used to select non-namespaced resources. namespaces: [&quot;kube-system&quot;] # Log configmap and secret changes in all other namespaces at the Metadata level. - level: Metadata resources: - group: &quot;&quot; # core API group resources: [&quot;secrets&quot;, &quot;configmaps&quot;] # Log all other resources in core and extensions at the Request level. - level: Request resources: - group: &quot;&quot; # core API group - group: &quot;extensions&quot; # Version of group should NOT be included. # A catch-all rule to log all other requests at the Metadata level. - level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: - &quot;RequestReceived&quot;  ","version":"Next","tagName":"h3"},{"title":"Enabling Disk UUIDs for vSphere VMs","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/vsphere/enabling-uuid","content":"","keywords":"","version":"Next"},{"title":"Using the vSphere Console​","type":1,"pageTitle":"Enabling Disk UUIDs for vSphere VMs","url":"/config-options/cloud-providers/vsphere/enabling-uuid#using-the-vsphere-console","content":" The required property can be set while creating or modifying VMs in the vSphere Console:  For each VM navigate to the tab VM Options and click on Edit Configuration.Add the parameter disk.EnableUUID with a value of TRUE.    ","version":"Next","tagName":"h3"},{"title":"Using the GOVC CLI tool​","type":1,"pageTitle":"Enabling Disk UUIDs for vSphere VMs","url":"/config-options/cloud-providers/vsphere/enabling-uuid#using-the-govc-cli-tool","content":" You can also modify properties of VMs with the govc command-line tool to enable disk UUIDs:  $ govc vm.change -vm &lt;vm-path&gt; -e disk.enableUUID=TRUE   ","version":"Next","tagName":"h3"},{"title":"Using a Rancher Node Template​","type":1,"pageTitle":"Enabling Disk UUIDs for vSphere VMs","url":"/config-options/cloud-providers/vsphere/enabling-uuid#using-a-rancher-node-template","content":" In Rancher v2.0.4+, disk UUIDs are enabled in vSphere node templates by default.  If you are using Rancher before v2.0.4, refer to the vSphere node template documentation. for details on how to enable a UUID with a Rancher node template. ","version":"Next","tagName":"h3"},{"title":"External etcd","type":0,"sectionRef":"#","url":"/config-options/services/external-etcd","content":"","keywords":"","version":"Next"},{"title":"External etcd Options​","type":1,"pageTitle":"External etcd","url":"/config-options/services/external-etcd#external-etcd-options","content":" ","version":"Next","tagName":"h2"},{"title":"Path​","type":1,"pageTitle":"External etcd","url":"/config-options/services/external-etcd#path","content":" The path defines the location of where the etcd cluster is on the endpoints.  ","version":"Next","tagName":"h3"},{"title":"External URLs​","type":1,"pageTitle":"External etcd","url":"/config-options/services/external-etcd#external-urls","content":" The external_urls are the endpoints of where the etcd cluster is hosted. There can be multiple endpoints for the etcd cluster.  ","version":"Next","tagName":"h3"},{"title":"CA Cert/Cert/KEY​","type":1,"pageTitle":"External etcd","url":"/config-options/services/external-etcd#ca-certcertkey","content":" The certificates and private keys used to authenticate and access the etcd service. ","version":"Next","tagName":"h3"},{"title":"Dual-stack","type":0,"sectionRef":"#","url":"/config-options/dual-stack","content":"","keywords":"","version":"Next"},{"title":"Requirements​","type":1,"pageTitle":"Dual-stack","url":"/config-options/dual-stack#requirements","content":" In order to use the dual-stack feature, RKE and the infrastructure it's deploy to must be configured as follows:  Kubernetes 1.21 or newer is used.RKE is configured to use Calico as the Container Network Interface (CNI) provider. Other providers are not supported.RKE is deployed on Amazon EC2 instances with the following prerequisites:Enable IPv6 support: set the network range at VPC and its subnetworks.Add a IPv6 default gateway to VPC routes.Add inbound/outbound rules for IPv6 traffic to your cluster's security group(s).Ensure instances have Auto-assign IPv6 IP enabled. See the AWS documentation for instructions.Disable source/destination checks on all instances in the cluster. See the AWS documentation for instructions.  For more information on configuring your AWS enivronment for IPv6, refer to the AWS Getting started with IPv6 documentation.  ","version":"Next","tagName":"h3"},{"title":"Example RKE Configuration​","type":1,"pageTitle":"Dual-stack","url":"/config-options/dual-stack#example-rke-configuration","content":" The following is an example RKE configuration that can be used to deploy RKE with dual-stack support configured:  kubernetes_version: &quot;v1.21.1-rancher2-1&quot; services: kube-api: service_cluster_ip_range: 10.43.0.0/16,fd98::/108 kube-controller: service_cluster_ip_range: 10.43.0.0/16,fd98::/108 cluster_cidr: 10.42.0.0/16,fd01::/64 network: plugin: calico  ","version":"Next","tagName":"h3"},{"title":"Rate Limiting","type":0,"sectionRef":"#","url":"/config-options/rate-limiting","content":"","keywords":"","version":"Next"},{"title":"Example Configurations​","type":1,"pageTitle":"Rate Limiting","url":"/config-options/rate-limiting#example-configurations","content":" The following configuration in the cluster.yml can be used to enable the event rate limit by default:  services: kube-api: event_rate_limit: enabled: true   When the event rate limit is enabled, you should be able to see the default values at /etc/kubernetes/admission.yaml:  ... plugins: - configuration: apiVersion: eventratelimit.admission.k8s.io/v1alpha1 kind: Configuration limits: - burst: 20000 qps: 5000 type: Server ...   To customize the event rate limit, the entire Kubernetes resource for the configuration must be provided in the configuration directive:  services: kube-api: event_rate_limit: enabled: true configuration: apiVersion: eventratelimit.admission.k8s.io/v1alpha1 kind: Configuration limits: - type: Server qps: 6000 burst: 30000  ","version":"Next","tagName":"h3"},{"title":"Nodes","type":0,"sectionRef":"#","url":"/config-options/nodes","content":"","keywords":"","version":"Next"},{"title":"Node Configuration Example​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#node-configuration-example","content":" The following example shows node configuration in an example cluster.yml:  nodes: - address: 1.1.1.1 user: ubuntu role: - controlplane - etcd ssh_key_path: /home/user/.ssh/id_rsa port: 2222 - address: 2.2.2.2 user: ubuntu role: - worker ssh_key: |- -----BEGIN RSA PRIVATE KEY----- -----END RSA PRIVATE KEY----- - address: 3.3.3.3 user: ubuntu role: - worker ssh_key_path: /home/user/.ssh/id_rsa ssh_cert_path: /home/user/.ssh/id_rsa-cert.pub - address: 4.4.4.4 user: ubuntu role: - worker ssh_key_path: /home/user/.ssh/id_rsa ssh_cert: |- ssh-rsa-cert-v01@openssh.com AAAAHHNza... taints: # Available as of v0.3.0 - key: test-key value: test-value effect: NoSchedule - address: example.com user: ubuntu role: - worker hostname_override: node3 internal_address: 192.168.1.6 labels: app: ingress   ","version":"Next","tagName":"h2"},{"title":"Kubernetes Roles​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#kubernetes-roles","content":" You can specify the list of roles that you want the node to be as part of the Kubernetes cluster. Three roles are supported: controlplane, etcd and worker. Node roles are not mutually exclusive. It's possible to assign any combination of roles to any node. It's also possible to change a node's role using the upgrade process.  note Before v0.1.8, workloads/pods might have run on any nodes with worker or controlplane roles, but as of v0.1.8, they will only be deployed to any worker nodes.  ","version":"Next","tagName":"h2"},{"title":"etcd​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#etcd","content":" With this role, the etcd container will be run on these nodes. Etcd keeps the state of your cluster and is the most important component in your cluster, single source of truth of your cluster. Although you can run etcd on just one node, it typically takes 3, 5 or more nodes to create an HA configuration. Etcd is a distributed reliable key-value store which stores all Kubernetes state. Taint set on nodes with the etcd role is shown below:  Taint Key\tTaint Value\tTaint Effectnode-role.kubernetes.io/etcd\ttrue\tNoExecute  ","version":"Next","tagName":"h3"},{"title":"Controlplane​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#controlplane","content":" With this role, the stateless components that are used to deploy Kubernetes will run on these nodes. These components are used to run the API server, scheduler, and controllers. Taint set on nodes with the controlplane role is shown below:  Taint Key\tTaint Value\tTaint Effectnode-role.kubernetes.io/controlplane\ttrue\tNoSchedule  ","version":"Next","tagName":"h3"},{"title":"Worker​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#worker","content":" With this role, any workloads or pods that are deployed will land on these nodes.  ","version":"Next","tagName":"h3"},{"title":"Node Options​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#node-options","content":" Within each node, there are multiple directives that can be used.  ","version":"Next","tagName":"h2"},{"title":"Address​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#address","content":" The address directive will be used to set the hostname or IP address of the node. RKE must be able to connect to this address.  ","version":"Next","tagName":"h3"},{"title":"Internal Address​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#internal-address","content":" The internal_address provides the ability to have nodes with multiple addresses set a specific address to use for inter-host communication on a private network. If the internal_address is not set, the address is used for inter-host communication. The internal_address directive will set the address used for inter-host communication of the Kubernetes components, e.g. kube-apiserver and etcd. To change the interface used for the vxlan traffic of the Canal or Flannel network plug-ins please refer to the Network Plug-ins Documentation.  ","version":"Next","tagName":"h3"},{"title":"Overriding the Hostname​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#overriding-the-hostname","content":" The hostname_override is used to be able to provide a friendly name for RKE to use when registering the node in Kubernetes. This hostname doesn't need to be a routable address, but it must be a valid Kubernetes resource name. If the hostname_override isn't set, then the address directive is used when registering the node in Kubernetes.  note When cloud providers are configured, you may need to override the hostname in order to use the cloud provider correctly. There is an exception for the AWS cloud provider, where the hostname_override field will be explicitly ignored.  ","version":"Next","tagName":"h3"},{"title":"SSH Port​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#ssh-port","content":" In each node, you specify which port to be used when connecting to this node. The default port is 22.  ","version":"Next","tagName":"h3"},{"title":"SSH Users​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#ssh-users","content":" For each node, you specify the user to be used when connecting to this node. This user must be a member of the Docker group or allowed to write to the node's Docker socket.  ","version":"Next","tagName":"h3"},{"title":"SSH Key Path​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#ssh-key-path","content":" For each node, you specify the path, i.e. ssh_key_path, for the SSH private key to be used when connecting to this node. The default key path for each node is ~/.ssh/id_rsa.  note If you have a private key that can be used across all nodes, you can set the SSH key path at the cluster level. The SSH key path set in each node will always take precedence.  ","version":"Next","tagName":"h3"},{"title":"SSH Key​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#ssh-key","content":" Instead of setting the path to the SSH key, you can alternatively specify the actual key, i.e. ssh_key, to be used to connect to the node.  ","version":"Next","tagName":"h3"},{"title":"SSH Certificate Path​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#ssh-certificate-path","content":" For each node, you can specify the path, i.e. ssh_cert_path, for the signed SSH certificate to be used when connecting to this node.  ","version":"Next","tagName":"h3"},{"title":"SSH Certificate​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#ssh-certificate","content":" Instead of setting the path to the signed SSH certificate, you can alternatively specify the actual certificate, i.e. ssh_cert, to be used to connect to the node.  ","version":"Next","tagName":"h3"},{"title":"Docker Socket​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#docker-socket","content":" If the Docker socket is different than the default, you can set the docker_socket. The default is /var/run/docker.sock  ","version":"Next","tagName":"h3"},{"title":"Labels​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#labels","content":" You have the ability to add an arbitrary map of labels for each node. It can be used when using the ingress controller's node_selector option.  ","version":"Next","tagName":"h3"},{"title":"Taints​","type":1,"pageTitle":"Nodes","url":"/config-options/nodes#taints","content":" Available as of v0.3.0  You have the ability to add taints for each node. ","version":"Next","tagName":"h3"},{"title":"DNS providers","type":0,"sectionRef":"#","url":"/config-options/add-ons/dns","content":"","keywords":"","version":"Next"},{"title":"Available DNS Providers​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#available-dns-providers","content":" RKE provides the following DNS providers that can be deployed as add-ons:  CoreDNSkube-dns  RKE version\tKubernetes version\tDefault DNS providerv0.2.5 and higher\tv1.14.0 and higher\tCoreDNS v0.2.5 and higher\tv1.13.x and lower\tkube-dns v0.2.4 and lower\tany\tkube-dns  CoreDNS was made the default in RKE v0.2.5 when using Kubernetes 1.14 and higher. If you are using an RKE version lower than v0.2.5, kube-dns will be deployed by default.  note If you switch from one DNS provider to another, the existing DNS provider will be removed before the new one is deployed.  ","version":"Next","tagName":"h2"},{"title":"Disabling Deployment of a DNS Provider​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#disabling-deployment-of-a-dns-provider","content":" Available as of v0.2.0  You can disable the default DNS provider by specifying none to the dns provider directive in the cluster configuration. Be aware that this will prevent your pods from doing name resolution in your cluster.  dns: provider: none   ","version":"Next","tagName":"h2"},{"title":"CoreDNS​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#coredns","content":" Available as of v0.2.5  CoreDNS can only be used on Kubernetes v1.12.0 and higher.  RKE will deploy CoreDNS as a Deployment with the default replica count of 1. The pod consists of 1 container: coredns. RKE will also deploy coredns-autoscaler as a Deployment, which will scale the coredns Deployment by using the number of cores and nodes. Please see Linear Mode for more information about this logic.  The images used for CoreDNS are under the system_images directive. For each Kubernetes version, there are default images associated with CoreDNS, but these can be overridden by changing the image tag in system_images.  ","version":"Next","tagName":"h2"},{"title":"Scheduling CoreDNS​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#scheduling-coredns","content":" If you only want the CoreDNS pod to be deployed on specific nodes, you can set a node_selector in the dns section. The label in the node_selector would need to match the label on the nodes for the CoreDNS pod to be deployed.  nodes: - address: 1.1.1.1 role: [controlplane,worker,etcd] user: root labels: app: dns dns: provider: coredns node_selector: app: dns   ","version":"Next","tagName":"h3"},{"title":"CoreDNS Upstream nameservers​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#coredns-upstream-nameservers","content":" By default, CoreDNS will use the host configured nameservers (usually residing at /etc/resolv.conf) to resolve external queries. If you want to configure specific upstream nameservers to be used by CoreDNS, you can use the upstreamnameservers directive.  When you set upstreamnameservers, the provider also needs to be set.  dns: provider: coredns upstreamnameservers: - 1.1.1.1 - 8.8.4.4   ","version":"Next","tagName":"h3"},{"title":"CoreDNS Priority Class Name​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#coredns-priority-class-name","content":" Available as of RKE v1.2.6+  The pod priority is set by configuring a priority class name under options:  dns: options: coredns_autoscaler_priority_class_name: system-cluster-critical coredns_priority_class_name: system-cluster-critical provider: coredns   ","version":"Next","tagName":"h3"},{"title":"CoreDNS Tolerations​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#coredns-tolerations","content":" Available as of v1.2.4  The configured tolerations apply to the coredns and the coredns-autoscaler Deployment.  dns: provider: coredns tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300   To check for applied tolerations on the coredns and coredns-autoscaler Deployment, use the following commands:  kubectl -n kube-system get deploy coredns -o jsonpath='{.spec.template.spec.tolerations}' kubectl -n kube-system get deploy coredns-autoscaler -o jsonpath='{.spec.template.spec.tolerations}'   ","version":"Next","tagName":"h3"},{"title":"kube-dns​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#kube-dns","content":" RKE will deploy kube-dns as a Deployment with the default replica count of 1. The pod consists of 3 containers: kubedns, dnsmasq and sidecar. RKE will also deploy kube-dns-autoscaler as a Deployment, which will scale the kube-dns Deployment by using the number of cores and nodes. Please see Linear Mode for more information about this logic.  The images used for kube-dns are under the system_images directive. For each Kubernetes version, there are default images associated with kube-dns, but these can be overridden by changing the image tag in system_images.  ","version":"Next","tagName":"h2"},{"title":"Scheduling kube-dns​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#scheduling-kube-dns","content":" Available as of v0.2.0  If you only want the kube-dns pod to be deployed on specific nodes, you can set a node_selector in the dns section. The label in the node_selector would need to match the label on the nodes for the kube-dns pod to be deployed.  nodes: - address: 1.1.1.1 role: [controlplane,worker,etcd] user: root labels: app: dns dns: provider: kube-dns node_selector: app: dns   ","version":"Next","tagName":"h3"},{"title":"kube-dns Upstream nameservers​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#kube-dns-upstream-nameservers","content":" Available as of v0.2.0  By default, kube-dns will use the host configured nameservers (usually residing at /etc/resolv.conf) to resolve external queries. If you want to configure specific upstream nameservers to be used by kube-dns, you can use the upstreamnameservers directive.  When you set upstreamnameservers, the provider also needs to be set.  dns: provider: kube-dns upstreamnameservers: - 1.1.1.1 - 8.8.4.4   ","version":"Next","tagName":"h3"},{"title":"kube-dns Priority Class Name​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#kube-dns-priority-class-name","content":" Available as of RKE v1.2.6+  The pod priority is set by configuring a priority class name under options:  dns: options: kube_dns_autoscaler_priority_class_name: system-cluster-critical kube_dns_priority_class_name: system-cluster-critical provider: kube-dns   ","version":"Next","tagName":"h3"},{"title":"kube-dns Tolerations​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#kube-dns-tolerations","content":" Available as of v1.2.4  The configured tolerations apply to the kube-dns and the kube-dns-autoscaler Deployment.  dns: provider: kube-dns tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300   To check for applied tolerations on the coredns and coredns-autoscaler Deployment, use the following commands:  kubectl get deploy kube-dns -n kube-system -o jsonpath='{.spec.template.spec.tolerations}' kubectl get deploy kube-dns-autoscaler -n kube-system -o jsonpath='{.spec.template.spec.tolerations}'   ","version":"Next","tagName":"h3"},{"title":"NodeLocal DNS​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#nodelocal-dns","content":" Available as of v1.1.0  The option to enable NodeLocal DNS is available for: Kubernetes v1.15.11 and upKubernetes v1.16.8 and upKubernetes v1.17.4 and up  NodeLocal DNS is an additional component that can be deployed on each node to improve DNS performance. It is not a replacement for the provider parameter, you will still need to have one of the available DNS providers configured. See Using NodeLocal DNSCache in Kubernetes clusters for more information on how NodeLocal DNS works.  Enable NodeLocal DNS by configuring an IP address.  ","version":"Next","tagName":"h2"},{"title":"Configuring NodeLocal DNS​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#configuring-nodelocal-dns","content":" The ip_address parameter is used to configure what link-local IP address will be configured one each host to listen on, make sure this IP address is not already configured on the host.  dns: provider: coredns nodelocal: ip_address: &quot;169.254.20.10&quot;   note When enabling NodeLocal DNS on an existing cluster, pods that are currently running will not be modified, the updated /etc/resolv.conf configuration will take effect only for pods started after enabling NodeLocal DNS.  ","version":"Next","tagName":"h3"},{"title":"NodeLocal Priority Class Name​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#nodelocal-priority-class-name","content":" Available as of RKE v1.2.6+  The pod priority is set by configuring a priority class name under options:  dns: options: nodelocal_autoscaler_priority_class_name: system-cluster-critical nodelocal_priority_class_name: system-cluster-critical provider: coredns # a DNS provider must be configured   ","version":"Next","tagName":"h3"},{"title":"Removing NodeLocal DNS​","type":1,"pageTitle":"DNS providers","url":"/config-options/add-ons/dns#removing-nodelocal-dns","content":" By removing the ip_address value, NodeLocal DNS will be removed from the cluster.  warning When removing NodeLocal DNS, a disruption to DNS can be expected. The updated /etc/resolv.conf configuration will take effect only for pods that are started after removing NodeLocal DNS. In general pods using the default dnsPolicy: ClusterFirst will need to be re-deployed. ","version":"Next","tagName":"h3"},{"title":"Extra Args, Extra Binds, and Extra Environment Variables","type":0,"sectionRef":"#","url":"/config-options/services/services-extras","content":"","keywords":"","version":"Next"},{"title":"Extra Args​","type":1,"pageTitle":"Extra Args, Extra Binds, and Extra Environment Variables","url":"/config-options/services/services-extras#extra-args","content":" For any of the Kubernetes services, you can update the extra_args to change the existing defaults.  As of v0.1.3, using extra_args will add new arguments and override any existing defaults. For example, if you need to modify the default admission plugins list, you need to include the default list and edit it with your changes so all changes are included.  Before v0.1.3, using extra_args would only add new arguments to the list and there was no ability to change the default list.  All service defaults and parameters are defined per kubernetes_version:  For RKE v0.3.0+, the service defaults and parameters are defined per kubernetes_version. The service defaults are located here. The default list of admissions plugins is the same for all Kubernetes versions and is located here. For RKE before v0.3.0, the service defaults and admission plugins are defined per kubernetes_version and located here.  services: kube-controller: extra_args: cluster-name: &quot;mycluster&quot;   ","version":"Next","tagName":"h3"},{"title":"Extra Binds​","type":1,"pageTitle":"Extra Args, Extra Binds, and Extra Environment Variables","url":"/config-options/services/services-extras#extra-binds","content":" Additional volume binds can be added to services using the extra_binds arguments.  services: kubelet: extra_binds: - &quot;/dev:/host/dev&quot; - &quot;/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins:z&quot;   ","version":"Next","tagName":"h3"},{"title":"Extra Environment Variables​","type":1,"pageTitle":"Extra Args, Extra Binds, and Extra Environment Variables","url":"/config-options/services/services-extras#extra-environment-variables","content":" Additional environment variables can be added to services by using the extra_env arguments.  services: kubelet: extra_env: - &quot;HTTP_PROXY=http://your_proxy&quot;  ","version":"Next","tagName":"h3"},{"title":"Default Kubernetes Services","type":0,"sectionRef":"#","url":"/config-options/services","content":"","keywords":"","version":"Next"},{"title":"etcd​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#etcd","content":" Kubernetes uses etcd as a store for cluster state and data. Etcd is a reliable, consistent and distributed key-value store.  RKE supports running etcd in a single node mode or in HA cluster mode. It also supports adding and removing etcd nodes to the cluster.  You can enable etcd to take recurring snapshots. These snapshots can be used to restore etcd.  By default, RKE will deploy a new etcd service, but you can also run Kubernetes with an external etcd service.  ","version":"Next","tagName":"h2"},{"title":"Kubernetes API Server​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubernetes-api-server","content":" Note for Rancher 2 users If you are configuring Cluster Options using a Config File when creating Rancher Launched Kubernetes, the names of services should contain underscores only: kube_api. This only applies to Rancher v2.0.5 and v2.0.6.  The Kubernetes API REST service, which handles requests and data for all Kubernetes objects and provide shared state for all the other Kubernetes components.  services: kube-api: # IP range for any services created on Kubernetes # This must match the service_cluster_ip_range in kube-controller service_cluster_ip_range: 10.43.0.0/16 # Expose a different port range for NodePort services service_node_port_range: 30000-32767 pod_security_policy: false # Valid values are either restricted or privileged pod_security_configuration: restricted # Enable AlwaysPullImages Admission controller plugin # Available as of v0.2.0 always_pull_images: false secrets_encryption_config: enabled: true   ","version":"Next","tagName":"h2"},{"title":"Kubernetes API Server Options​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubernetes-api-server-options","content":" RKE supports the following options for the kube-api service :  Service Cluster IP Range (service_cluster_ip_range) - This is the virtual IP address that will be assigned to services created on Kubernetes. By default, the service cluster IP range is 10.43.0.0/16. If you change this value, then it must also be set with the same value on the Kubernetes Controller Manager (kube-controller).Node Port Range (service_node_port_range) - The port range to be used for Kubernetes services created with the type NodePort. By default, the port range is 30000-32767.Pod Security Policy (pod_security_policy) - An option to enable the Kubernetes Pod Security Policy. By default, we do not enable pod security policies as it is set to false. This feature is removed as of Kubernetes v1.25. Note: If you set pod_security_policy value to true, RKE will configure an open policy to allow any pods to work on the cluster. You will need to configure your own policies to fully utilize PSP. Pod Security Admission (pod_security_configuration) - An option to enable the Kubernetes Pod Security Admission. This feature is available as of RKE v1.4.4 for Kubernetes v1.23 and above.Always Pull Images (always_pull_images) - Enable AlwaysPullImages Admission controller plugin. Enabling AlwaysPullImages is a security best practice. It forces Kubernetes to validate the image and pull credentials with the remote image registry. Local image layer cache will still be used, but it does add a small bit of overhead when launching containers to pull and compare image hashes. Note: Available as of v0.2.0Secrets Encryption Config (secrets_encryption_config) - Manage Kubernetes at-rest data encryption. Documented here  ","version":"Next","tagName":"h3"},{"title":"Kubernetes Controller Manager​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubernetes-controller-manager","content":" Note for Rancher 2 users If you are configuring Cluster Options using a Config File when creating Rancher Launched Kubernetes, the names of services should contain underscores only: kube_controller. This only applies to Rancher v2.0.5 and v2.0.6.  The Kubernetes Controller Manager service is the component responsible for running Kubernetes main control loops. The controller manager monitors the cluster desired state through the Kubernetes API server and makes the necessary changes to the current state to reach the desired state.  services: kube-controller: # CIDR pool used to assign IP addresses to pods in the cluster cluster_cidr: 10.42.0.0/16 # IP range for any services created on Kubernetes # This must match the service_cluster_ip_range in kube-api service_cluster_ip_range: 10.43.0.0/16   ","version":"Next","tagName":"h2"},{"title":"Kubernetes Controller Manager Options​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubernetes-controller-manager-options","content":" RKE supports the following options for the kube-controller service:  Cluster CIDR (cluster_cidr) - The CIDR pool used to assign IP addresses to pods in the cluster. By default, each node in the cluster is assigned a /24 network from this pool for pod IP assignments. The default value for this option is 10.42.0.0/16.Service Cluster IP Range (service_cluster_ip_range) - This is the virtual IP address that will be assigned to services created on Kubernetes. By default, the service cluster IP range is 10.43.0.0/16. If you change this value, then it must also be set with the same value on the Kubernetes API server (kube-api).  ","version":"Next","tagName":"h3"},{"title":"Kubelet​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubelet","content":" The kubelet services acts as a &quot;node agent&quot; for Kubernetes. It runs on all nodes deployed by RKE, and gives Kubernetes the ability to manage the container runtime on the node.  services: kubelet: # Base domain for the cluster cluster_domain: cluster.local # IP address for the DNS service endpoint cluster_dns_server: 10.43.0.10 # Fail if swap is on fail_swap_on: false # Generate per node serving certificate generate_serving_certificate: false   ","version":"Next","tagName":"h2"},{"title":"Kubelet Options​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubelet-options","content":" RKE supports the following options for the kubelet service:  Cluster Domain (cluster_domain) - The base domain for the cluster. All services and DNS records created on the cluster. By default, the domain is set to cluster.local.Cluster DNS Server (cluster_dns_server) - The IP address assigned to the DNS service endpoint within the cluster. DNS queries will be sent to this IP address which is used by KubeDNS. The default value for this option is 10.43.0.10Fail if Swap is On (fail_swap_on) - In Kubernetes, the default behavior for the kubelet is to fail if swap is enabled on the node. RKE does not follow this default and allows deployments on nodes with swap enabled. By default, the value is false. If you'd like to revert to the default kubelet behavior, set this option to true.Generate Serving Certificate (generate_serving_certificate) - Generate a certificate signed by the kube-ca Certificate Authority for the kubelet to use as a server certificate. The default value for this option is false. Before enabling this option, please read the requirements  ","version":"Next","tagName":"h3"},{"title":"Kubelet Serving Certificate Requirements​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubelet-serving-certificate-requirements","content":" If hostname_override is configured for one or more nodes in cluster.yml, please make sure the correct IP address is configured in address (and the internal address in internal_address) to make sure the generated certificate contains the correct IP address(es).  An example of an error situation is an EC2 instance where the the public IP address is configured in address, and hostname_override is used, the connection between kube-apiserver and kubelet will fail because the kubelet will be contacted on the private IP address and the generated certificate will not be valid (the error x509: certificate is valid for value_in_address, not private_ip will be seen). The resolution is to provide the internal IP address in internal_address.  For more information on host overrides, refer to the node configuration page.  ","version":"Next","tagName":"h3"},{"title":"Kubernetes Scheduler​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubernetes-scheduler","content":" The Kubernetes Scheduler service is responsible for scheduling cluster workloads based on various configurations, metrics, resource requirements and workload-specific requirements.  Currently, RKE doesn't support any specific options for the scheduler service.  ","version":"Next","tagName":"h2"},{"title":"Kubernetes Network Proxy​","type":1,"pageTitle":"Default Kubernetes Services","url":"/config-options/services#kubernetes-network-proxy","content":" The Kubernetes network proxy service runs on all nodes and manages endpoints created by Kubernetes for TCP/UDP ports.  Currently, RKE doesn't support any specific options for the kubeproxy service. ","version":"Next","tagName":"h2"},{"title":"AWS Cloud Provider","type":0,"sectionRef":"#","url":"/config-options/cloud-providers/aws","content":"","keywords":"","version":"Next"},{"title":"IAM Requirements​","type":1,"pageTitle":"AWS Cloud Provider","url":"/config-options/cloud-providers/aws#iam-requirements","content":" In a cluster with the AWS cloud provider enabled, nodes must have at least the ec2:Describe* action.  In order to use Elastic Load Balancers (ELBs) and EBS volumes with Kubernetes, the node(s) will need to have the an IAM role with appropriate permissions.  IAM policy for nodes with the controlplane role:  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;autoscaling:DescribeAutoScalingGroups&quot;, &quot;autoscaling:DescribeLaunchConfigurations&quot;, &quot;autoscaling:DescribeTags&quot;, &quot;ec2:DescribeInstances&quot;, &quot;ec2:DescribeRegions&quot;, &quot;ec2:DescribeRouteTables&quot;, &quot;ec2:DescribeSecurityGroups&quot;, &quot;ec2:DescribeSubnets&quot;, &quot;ec2:DescribeVolumes&quot;, &quot;ec2:CreateSecurityGroup&quot;, &quot;ec2:CreateTags&quot;, &quot;ec2:CreateVolume&quot;, &quot;ec2:ModifyInstanceAttribute&quot;, &quot;ec2:ModifyVolume&quot;, &quot;ec2:AttachVolume&quot;, &quot;ec2:AuthorizeSecurityGroupIngress&quot;, &quot;ec2:CreateRoute&quot;, &quot;ec2:DeleteRoute&quot;, &quot;ec2:DeleteSecurityGroup&quot;, &quot;ec2:DeleteVolume&quot;, &quot;ec2:DetachVolume&quot;, &quot;ec2:RevokeSecurityGroupIngress&quot;, &quot;ec2:DescribeVpcs&quot;, &quot;elasticloadbalancing:AddTags&quot;, &quot;elasticloadbalancing:AttachLoadBalancerToSubnets&quot;, &quot;elasticloadbalancing:ApplySecurityGroupsToLoadBalancer&quot;, &quot;elasticloadbalancing:CreateLoadBalancer&quot;, &quot;elasticloadbalancing:CreateLoadBalancerPolicy&quot;, &quot;elasticloadbalancing:CreateLoadBalancerListeners&quot;, &quot;elasticloadbalancing:ConfigureHealthCheck&quot;, &quot;elasticloadbalancing:DeleteLoadBalancer&quot;, &quot;elasticloadbalancing:DeleteLoadBalancerListeners&quot;, &quot;elasticloadbalancing:DescribeLoadBalancers&quot;, &quot;elasticloadbalancing:DescribeLoadBalancerAttributes&quot;, &quot;elasticloadbalancing:DetachLoadBalancerFromSubnets&quot;, &quot;elasticloadbalancing:DeregisterInstancesFromLoadBalancer&quot;, &quot;elasticloadbalancing:ModifyLoadBalancerAttributes&quot;, &quot;elasticloadbalancing:RegisterInstancesWithLoadBalancer&quot;, &quot;elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer&quot;, &quot;elasticloadbalancing:AddTags&quot;, &quot;elasticloadbalancing:CreateListener&quot;, &quot;elasticloadbalancing:CreateTargetGroup&quot;, &quot;elasticloadbalancing:DeleteListener&quot;, &quot;elasticloadbalancing:DeleteTargetGroup&quot;, &quot;elasticloadbalancing:DescribeListeners&quot;, &quot;elasticloadbalancing:DescribeLoadBalancerPolicies&quot;, &quot;elasticloadbalancing:DescribeTargetGroups&quot;, &quot;elasticloadbalancing:DescribeTargetHealth&quot;, &quot;elasticloadbalancing:ModifyListener&quot;, &quot;elasticloadbalancing:ModifyTargetGroup&quot;, &quot;elasticloadbalancing:RegisterTargets&quot;, &quot;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&quot;, &quot;iam:CreateServiceLinkedRole&quot;, &quot;kms:DescribeKey&quot; ], &quot;Resource&quot;: [ &quot;*&quot; ] } ] }   IAM policy for nodes with the etcd or worker role:  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:DescribeInstances&quot;, &quot;ec2:DescribeRegions&quot;, &quot;ecr:GetAuthorizationToken&quot;, &quot;ecr:BatchCheckLayerAvailability&quot;, &quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:GetRepositoryPolicy&quot;, &quot;ecr:DescribeRepositories&quot;, &quot;ecr:ListImages&quot;, &quot;ecr:BatchGetImage&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] }   ","version":"Next","tagName":"h2"},{"title":"Tagging AWS Resources​","type":1,"pageTitle":"AWS Cloud Provider","url":"/config-options/cloud-providers/aws#tagging-aws-resources","content":" The AWS cloud provider uses tagging to discover and manage resources, the following resources are not automatically tagged by Kubernetes or RKE:  VPC: The VPC used by the cluster Subnet: The subnets used by the cluster EC2 instances: All nodes launched for the cluster Security Groups: The security group(s) used by nodes in the cluster Note: If creating a LoadBalancer service and there is more than one security group attached to nodes, you must tag only one of the security groups as owned so that Kubernetes knows which group to add and remove rules. A single untagged security group is allowed, however, sharing this between clusters is not recommended.  AWS Documentation: Tagging Your Amazon EC2 Resources  You must tag with one of the following:  Key\tValuekubernetes.io/cluster/&lt;CLUSTERID&gt;\tshared  &lt;CLUSTERID&gt; can be any string you choose. However, the same string must be used on every resource you tag. Setting the tag value to owned informs the cluster that all resources tagged with the &lt;CLUSTERID&gt; are owned and managed by this cluster only.  If you do not share resources between clusters, you can change the tag to:  Key\tValuekubernetes.io/cluster/&lt;CLUSTERID&gt;\towned  ","version":"Next","tagName":"h2"},{"title":"Tagging for Load Balancers​","type":1,"pageTitle":"AWS Cloud Provider","url":"/config-options/cloud-providers/aws#tagging-for-load-balancers","content":" When provisioning a LoadBalancer service Kubernetes will attempt to discover the correct subnets, this is also achieved by tags and requires adding additional subnet tags to ensure internet-facing and internal ELBs are created in the correct subnets.  AWS Documentation: Subnet tagging for load balancers  ","version":"Next","tagName":"h2"},{"title":"Using the Out-of-Tree AWS Cloud Provider for RKE​","type":1,"pageTitle":"AWS Cloud Provider","url":"/config-options/cloud-providers/aws#using-the-out-of-tree-aws-cloud-provider-for-rke","content":" Node name conventions and other prerequisites must be followed so that the cloud provider can find the instance. RKE provisioned clusters don't support configuring providerID.  note If you use IP-based naming, the nodes must be named after the instance followed by the regional domain name (ip-xxx-xxx-xxx-xxx.ec2.&lt;region&gt;.internal). If you have a custom domain name set in the DHCP options, you must set --hostname-override on kube-proxy and kubelet to match this naming convention.  Select the cloud provider.  Selecting external-aws sets --cloud-provider=external and allows setting useInstanceMetadataHostname. Enabling useInstanceMetadataHostname will query the EC2 metadata service and set http://169.254.169.254/latest/meta-data/hostname as hostname-override for kubelet and kube-proxy.  Enabling useInstanceMetadataHostname is required if hostname-override is empty or if hostname-override doesn't meet the node naming conventions mentioned above in step 1.  rancher_kubernetes_engine_config: cloud_provider: name: external-aws useInstanceMetadataHostname: true/false   Existing clusters that use external cloud provider will set --cloud-provider=external for Kubernetes components but won't set the hostname-override by querying the EC2 metadata service.  Install the AWS cloud controller manager after the cluster finishes provisioning. Note that the cluster isn't successfully provisioned and nodes are still in an uninitialized state until you deploy the cloud controller manager.  ","version":"Next","tagName":"h3"},{"title":"Helm Chart Installation from CLI​","type":1,"pageTitle":"AWS Cloud Provider","url":"/config-options/cloud-providers/aws#helm-chart-installation-from-cli","content":" Official upstream docs for Helm chart installation can be found on GitHub.  Add the Helm repository:  helm repo add aws-cloud-controller-manager https://kubernetes.github.io/cloud-provider-aws helm repo update   Create a values.yaml file with the following contents, to override the default values.yaml:  # values.yaml hostNetworking: true tolerations: - effect: NoSchedule key: node.cloudprovider.kubernetes.io/uninitialized value: 'true' - effect: NoSchedule value: 'true' key: node-role.kubernetes.io/controlplane nodeSelector: node-role.kubernetes.io/controlplane: 'true' args: - --configure-cloud-routes=false - --use-service-account-credentials=true - --v=2 - --cloud-provider=aws clusterRoleRules: - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - update - apiGroups: - &quot;&quot; resources: - nodes verbs: - '*' - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch - apiGroups: - &quot;&quot; resources: - services verbs: - list - patch - update - watch - apiGroups: - &quot;&quot; resources: - services/status verbs: - list - patch - update - watch - apiGroups: - '' resources: - serviceaccounts verbs: - create - get - apiGroups: - &quot;&quot; resources: - persistentvolumes verbs: - get - list - update - watch - apiGroups: - &quot;&quot; resources: - endpoints verbs: - create - get - list - watch - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - get - list - watch - update - apiGroups: - &quot;&quot; resources: - serviceaccounts/token verbs: - create   Install the Helm chart:  helm upgrade --install aws-cloud-controller-manager -n kube-system aws-cloud-controller-manager/aws-cloud-controller-manager --values values.yaml   Verify that the Helm chart installed successfully:  helm status -n kube-system aws-cloud-controller-manager   If present, edit the DaemonSet to remove the default node selector node-role.kubernetes.io/control-plane: &quot;&quot;:  kubectl edit daemonset aws-cloud-controller-manager -n kube-system   (Optional) Verify that the cloud controller manager update succeeded:  kubectl rollout status daemonset -n kube-system aws-cloud-controller-manager   ","version":"Next","tagName":"h3"},{"title":"Migrating to the Out-of-Tree AWS Cloud Provider for RKE​","type":1,"pageTitle":"AWS Cloud Provider","url":"/config-options/cloud-providers/aws#migrating-to-the-out-of-tree-aws-cloud-provider-for-rke","content":" To migrate from an in-tree cloud provider to the out-of-tree AWS cloud provider, you must stop the existing cluster's kube controller manager and install the AWS cloud controller manager. There are many ways to do this. Refer to the official AWS documentation on the external cloud controller manager for details.  If it's acceptable to have some downtime, you can switch to an external cloud provider, which removes in-tree components and then deploy charts to install the AWS cloud controller manager.  If your setup can't tolerate any control plane downtime, you must enable leader migration. This facilitates a smooth transition from the controllers in the kube controller manager to their counterparts in the cloud controller manager. Refer to the official AWS documentation on Using Leader Migration for more details.  Important The Kubernetes cloud controller migration documentation mentions that it is possible to migrate with the same Kubernetes version, but assumes that migration is part of a Kubernetes upgrade. Refer to the Kubernetes documentation on migrating to use the cloud controller manager to see if you need to customize your setup before migrating. Confirm your migration configuration values. If your cloud provider provides an implementation of the Node IPAM controller, you also need to migrate the IPAM controller.  Update the cluster config to enable leader migration in cluster.yml:  services: kube-controller: extra_args: enable-leader-migration: &quot;true&quot;   Note that the cloud provider is still aws at this step:  cloud_provider: name: aws   Cordon the control plane nodes, so that the AWS cloud controller pods run on nodes only after upgrading to the external cloud provider.  kubectl cordon -l &quot;node-role.kubernetes.io/controlplane=true&quot;   To install the AWS cloud controller manager, you must enable leader migration in values.yaml and follow the same steps as when installing chart on a new cluster. To enable leader migration, add the following to the container arguments in values.yaml:  - '--enable-leader-migration=true'   Confirm that the chart is installed but the new pods aren't running yet due to cordoned controlplane nodes. After updating the cluster in the next step, RKE will uncordon each node after upgrading and aws-controller-manager pods will be scheduled. Update cluster.yml to change the cloud provider and remove the leader migration arguments from the kube-controller.  Selecting external-aws sets --cloud-provider=external and allows setting useInstanceMetadataHostname. Enabling useInstanceMetadataHostname will query the EC2 metadata service and set http://169.254.169.254/latest/meta-data/hostname as hostname-override for kubelet and kube-proxy.  Enabling useInstanceMetadataHostname is required if hostname-override is empty or if hostname-override doesn't meet the node naming conventions.  rancher_kubernetes_engine_config: cloud_provider: name: external-aws useInstanceMetadataHostname: true/false   Remove enable-leader-migration from:  services: kube-controller: extra_args: enable-leader-migration: &quot;true&quot;   If you're upgrading the cluster's Kubernetes version, set the Kubernetes version as well. Update the cluster. The aws-cloud-controller-manager pods should now be running. (Optional) After the upgrade, leader migration is no longer required due to only one cloud-controller-manager and can be removed. Upgrade the chart and remove the following section from the container arguments:  - --enable-leader-migration=true   Verify the cloud controller manager update was successfully rolled out with the following command:  kubectl rollout status daemonset -n kube-system aws-cloud-controller-manager  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/etcd-snapshots/troubleshooting","content":"Troubleshooting As of v0.1.9, the rke-bundle-cert container is removed on both success and failure of a restore. To debug any issues, you will need to look at the logs generated from rke. As of v0.1.8 and below, the rke-bundle-cert container is left over from a failed etcd restore. If you are having an issue with restoring an etcd snapshot then you can do the following on each etcd nodes before attempting to do another restore: docker container rm --force rke-bundle-cert The rke-bundle-cert container is usually removed when a backup or restore of etcd succeeds. Whenever something goes wrong, the rke-bundle-cert container will be left over. You can look at the logs or inspect the container to see what the issue is. docker container logs --follow rke-bundle-cert docker container inspect rke-bundle-cert The important thing to note is the mounts of the container and location of the pki.bundle.tar.gz.","keywords":"","version":"Next"},{"title":"Encrypting Secret Data at Rest","type":0,"sectionRef":"#","url":"/config-options/secrets-encryption","content":"","keywords":"","version":"Next"},{"title":"Managed At-Rest Data Encryption​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#managed-at-rest-data-encryption","content":" Enabling and disabling at-rest data encryption in Kubernetes is a relatively complex process that requires several steps to be performed by the Kubernetes cluster administrator. The managed configuration aims to reduce this overhead and provides a simple abstraction layer to manage the process.  ","version":"Next","tagName":"h2"},{"title":"Enable Encryption​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#enable-encryption","content":" Managed at-rest data encryption is disabled by default and can be enabled by using the following configuration:  services: kube-api: secrets_encryption_config: enabled: true   Once enabled, RKE will perform the following actions to enable at-rest data encryption:  Generate a new random 32-byte encryption keyGenerate an encryption provider configuration file using the new key The default provider used is aescbcDeploy the provider configuration file to all nodes with controlplane roleUpdate the kube-apiserver container arguments to point to the provider configuration file.Restart the kube-apiserver container.  After the kube-api server is restarted, data encryption is enabled. However, all existing secrets are still stored in plain text. RKE will rewrite all secrets to ensure encryption is fully in effect.  ","version":"Next","tagName":"h3"},{"title":"Disable Encryption​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#disable-encryption","content":" To disable encryption, you can either set the enabled flag to false, or simply remove the secrets_encryption_config block entirely from cluster.yml.  services: kube-api: secrets_encryption_config: enabled: false   Once encryption is disabled in cluster.yml, RKE will perform the following actions to disable encryption in your cluster:  Generate a new provider configuration file with the no-encryption identity{} provider as the first provider, and the previous aescbc set in the second place. This will allow Kubernetes to use the first entry to write the secrets, and the second one to decrypt them.Deploy the new provider configuration and restart kube-apiserver.Rewrite all secrets. This is required because, at this point, new data will be written to disk in plain text, but the existing data is still encrypted using the old provider. By rewriting all secrets, RKE ensures that all stored data is decrypted.Update kube-apiserver arguments to remove the encryption provider configuration and restart the kube-apiserver.Remove the provider configuration file.  ","version":"Next","tagName":"h3"},{"title":"Key Rotation​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#key-rotation","content":" Sometimes there is a need to rotate encryption config in your cluster. For example, the key is compromised. There are two ways to rotate the keys: with an RKE CLI command, or by disabling and re-enabling encryption in cluster.yml.  ","version":"Next","tagName":"h2"},{"title":"Rotating Keys with the RKE CLI​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#rotating-keys-with-the-rke-cli","content":" With managed configuration, RKE CLI has the ability to perform the key rotation process documented here with one command. To perform this operation, the following subcommand is used:  $ ./rke encrypt rotate-key --help NAME: rke encrypt rotate-key - Rotate cluster encryption provider key USAGE: rke encrypt rotate-key [command options] [arguments...] OPTIONS: --config value Specify an alternate cluster YAML file (default: &quot;cluster.yml&quot;) [$RKE_CONFIG] --ssh-agent-auth Use SSH Agent Auth defined by SSH_AUTH_SOCK --ignore-docker-version Disable Docker version check   This command will perform the following actions:  Generate a new random 32-byte encryption keyGenerate a new provider configuration with the new key as the first provider and the old key as the second provider. When the secrets are rewritten, the first key will be used to encrypt the data on the write operation, while the second key (the old key) will be used to decrypt the stored data during the the read operationDeploy the new provider configuration to all controlplane nodes and restart the kube-apiserverRewrite all secrets. This process will re-encrypt all the secrets with the new key.Update the configuration to remove the old key and restart the kube-apiserver  ","version":"Next","tagName":"h3"},{"title":"Rotating Keys by Disabling and Re-enabling Encryption in cluster.yml​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#rotating-keys-by-disabling-and-re-enabling-encryption-in-clusteryml","content":" For a cluster with encryption enabled, you can rotate the encryption keys by updating cluster.yml. If you disable and re-enable the data encryption in the cluster.yml, RKE will not reuse old keys. Instead, it will generate new keys every time, yielding the same result as a key rotation with the RKE CLI.  ","version":"Next","tagName":"h3"},{"title":"Custom At-Rest Data Encryption Configuration​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#custom-at-rest-data-encryption-configuration","content":" With managed configuration, RKE provides the user with a very simple way to enable and disable encryption with minimal interaction and configuration. However, it doesn't allow for any customization to the configuration.  With custom encryption configuration, RKE allows the user to provide their own configuration. Although RKE will help the user to deploy the configuration and rewrite the secrets if needed, it doesn't provide a configuration validation on user's behalf. It's the user responsibility to make sure their configuration is valid.  warning Using invalid Encryption Provider Configuration could cause several issues with your cluster, ranging from crashing the Kubernetes API service, kube-api, to completely losing access to encrypted data.  ","version":"Next","tagName":"h2"},{"title":"Example: Using Custom Encryption Configuration with User Provided 32-byte Random Key​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#example-using-custom-encryption-configuration-with-user-provided-32-byte-random-key","content":" The following describes the steps required to configure custom encryption with a user provided 32-byte random key.  Step 1: Generate a 32-byte random key and base64 encode it. If you're on Linux or macOS, run the following command:  head -c 32 /dev/urandom | base64   Place that value in the secret field.  kube-api: secrets_encryption_config: enabled: true custom_config: apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - Providers: - AESCBC: Keys: - Name: key1 Secret: &lt;BASE 64 ENCODED SECRET&gt; resources: - secrets - identity: {}   ","version":"Next","tagName":"h3"},{"title":"Example: Using Custom Encryption Configuration with Amazon KMS​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#example-using-custom-encryption-configuration-with-amazon-kms","content":" An example for custom configuration would be enabling an external key management system like Amazon KMS. The following is an example of the configuration for AWS KMS:   services: kube-api: extra_binds: - &quot;/var/run/kmsplugin/:/var/run/kmsplugin/&quot; secrets_encryption_config: enabled: true custom_config: apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: aws-encryption-provider endpoint: unix:///var/run/kmsplugin/socket.sock cachesize: 1000 timeout: 3s - identity: {}   Documentation for AWS KMS can be found here. When Custom Configuration is set to to enable the AWS KMS provider, you should consider the following points:  Since RKE runs the kube-api service in a container, it's required that you use the extra_binds feature to bind-mount the KMS provider socket location inside the kube-api container.The AWS KMS provider runs as a pod in the cluster. Therefor, the proper way to enable it is to: Deploy your cluster with at-rest encryption disabled.Deploy the KMS pod and make sure it's working correctly.Update your cluster with the custom encryption configuration to utilize the KMS provider. Kube API connects to the KMS provider using a Unix socket. You should configure your KMS deployment to run pods on all controlplane nodes in the cluster.Your controlplane node should be configured with an AMI profile that has access to the KMS key you used in your configuration.  ","version":"Next","tagName":"h3"},{"title":"How to Prevent Restore Failures after Rotating Keys​","type":1,"pageTitle":"Encrypting Secret Data at Rest","url":"/config-options/secrets-encryption#how-to-prevent-restore-failures-after-rotating-keys","content":" It's important to understand that enabling encryption for you cluster means that you can no longer access encrypted data in your etcd database and/or etcd database backups without using your encryption keys.  The encryption configuration is stored in the cluster state file cluster.rkestate, which is decoupled from the etcd backups. For example, in any of the following backup cases, the restore process will fail:  The snapshot is taken while encryption is enabled and restored when it's disabled. In this case, the encryption keys are no longer stored in the cluster state.The snapshot is taken before the keys are rotated and restore is attempted after. In this case, the old keys used for encryption at the time of the snapshot no longer exist in the cluster state file.  Therefore, we recommend that when you enable or disable encryption, or when you rotate keys, you should create a snapshot so that your backup requires the same keys that you have access to.  This also means you should not rotate the keys during the restore process, because you would lose the encryption keys in cluster.rkestate.  The same applies to the custom configuration use case, however in this case it will depend on the user-provided encryption configuration. ","version":"Next","tagName":"h3"},{"title":"Private Registries","type":0,"sectionRef":"#","url":"/config-options/private-registries","content":"","keywords":"","version":"Next"},{"title":"Default Registry​","type":1,"pageTitle":"Private Registries","url":"/config-options/private-registries#default-registry","content":" As of v0.1.10, RKE supports specifying a default registry from the list of private registries to be used with all system images. In this example, RKE will use registry.com as the default registry for all system images, e.g. rancher/rke-tools:v0.1.14 will become registry.com/rancher/rke-tools:v0.1.14.  private_registries: - url: registry.com user: Username password: password is_default: true # All system images will be pulled using this registry.   ","version":"Next","tagName":"h3"},{"title":"Air-gapped Setups​","type":1,"pageTitle":"Private Registries","url":"/config-options/private-registries#air-gapped-setups","content":" By default, all system images are being pulled from DockerHub. If you are on a system that does not have access to DockerHub, you will need to create a private registry that is populated with all the required system images.  As of v0.1.10, you have to configure your private registry credentials, but you can specify this registry as a default registry so that all system images are pulled from the designated private registry. You can use the command rke config --system-images to get the list of default system images to populate your private registry.  Before v0.1.10, you had to configure your private registry credentials and update the names of all the system images in the cluster.yml so that the image names would have the private registry URL appended before each image name.  ","version":"Next","tagName":"h3"},{"title":"Amazon Elastic Container Registry (ECR) Private Registry Setup​","type":1,"pageTitle":"Private Registries","url":"/config-options/private-registries#amazon-elastic-container-registry-ecr-private-registry-setup","content":" Amazon ECR is an AWS managed container image registry service that is secure, scalable, and reliable. There are two ways in which to provide ECR credentials to set up your ECR private registry: using an instance profile or adding a configuration snippet, which are hard-coded credentials in environment variables for the kubelet and credentials under the ecrCredentialPlugin.  Instance Profile: An instance profile is the preferred and more secure approach to provide ECR credentials (when running in EC2, etc.). The instance profile will be autodetected and used by default. For more information on configuring an instance profile with ECR permissions, go here. Configuration Snippet: You will use the configuration snippet below rather than an instance profile only if the following conditions exist in your node: Node is not an EC2 instanceNode is an EC2 instance but does not have an instance profile configuredNode is an EC2 instance and has an instance profile configured but has no permissions for ECR  note The ECR credentials are only used in the kubelet and ecrCredentialPlugin areas. This is important to remember if you have issues while creating a new cluster or when pulling images during reconcile/upgrades. Kubelet: For add-ons, custom workloads, etc., the instance profile or credentials are used by the downstream cluster nodesPulling system images (directly via Docker): For bootstrap, upgrades, reconcile, etc., the instance profile or credentials are used by nodes running RKE or running the Rancher pods.   # Configuration snippet to be used when the instance profile is unavailable. services: kubelet: extra_env: - &quot;AWS_ACCESS_KEY_ID=ACCESSKEY&quot; - &quot;AWS_SECRET_ACCESS_KEY=SECRETKEY&quot; private_registries: - url: ACCOUNTID.dkr.ecr.REGION.amazonaws.com is_default: true ecrCredentialPlugin: aws_access_key_id: &quot;ACCESSKEY&quot; aws_secret_access_key: &quot;SECRETKEY&quot;  ","version":"Next","tagName":"h3"},{"title":"Backups and Disaster Recovery","type":0,"sectionRef":"#","url":"/etcd-snapshots","content":"","keywords":"","version":"Next"},{"title":"Backing Up a Cluster​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#backing-up-a-cluster","content":" You can create one-time snapshots to back up your cluster, and you can also configure recurring snapshots.  ","version":"Next","tagName":"h2"},{"title":"Restoring a Cluster from Backup​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#restoring-a-cluster-from-backup","content":" You can use RKE to restore your cluster from backup.  ","version":"Next","tagName":"h2"},{"title":"Example Scenarios​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#example-scenarios","content":" These example scenarios for backup and restore are different based on your version of RKE.  ","version":"Next","tagName":"h2"},{"title":"How Snapshots Work​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#how-snapshots-work","content":" For each etcd node in the cluster, the etcd cluster health is checked. If the node reports that the etcd cluster is healthy, a snapshot is created from it and optionally uploaded to S3.  The snapshot is stored in /opt/rke/etcd-snapshots. If the directory is configured on the nodes as a shared mount, it will be overwritten. On S3, the snapshot will always be from the last node that uploads it, as all etcd nodes upload it and the last will remain.  In the case when multiple etcd nodes exist, any created snapshot is created after the cluster has been health checked, so it can be considered a valid snapshot of the data in the etcd cluster.  Available as of v1.1.4  Each snapshot will include the cluster state file in addition to the etcd snapshot file.  ","version":"Next","tagName":"h2"},{"title":"Snapshot Naming​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#snapshot-naming","content":" The name of the snapshot is auto-generated. The --name option can be used to override the name of the snapshot when creating one-time snapshots with the RKE CLI.  An example one-time snapshot name is rke_etcd_snapshot_2020-10-15T16:47:24+02:00. An example recurring snapshot name is 2020-10-15T14:53:26Z_etcd.  ","version":"Next","tagName":"h3"},{"title":"How Restoring from a Snapshot Works​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#how-restoring-from-a-snapshot-works","content":" On restore, the following process is used:  The snapshot is retrieved from S3, if S3 is configured.The snapshot is unzipped (if zipped).It is checked if the cluster state file is included in the snapshot, if it is included, it will be used instead of the local cluster state file (Available as of v1.1.4)One of the etcd nodes in the cluster serves that snapshot file to the other nodes.The other etcd nodes download the snapshot and validate the checksum so that they all use the same snapshot for the restore.The cluster is restored and post-restore actions will be done in the cluster.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Backups and Disaster Recovery","url":"/etcd-snapshots#troubleshooting","content":" If you have trouble restoring your cluster, you can refer to the troubleshooting page. ","version":"Next","tagName":"h2"},{"title":"Custom Certificates","type":0,"sectionRef":"#","url":"/installation/certs","content":"","keywords":"","version":"Next"},{"title":"Using Custom Certificates​","type":1,"pageTitle":"Custom Certificates","url":"/installation/certs#using-custom-certificates","content":" # Use certificates located in the default directory `/cluster_certs` $ rke up --custom-certs # Use certificates located in your own directory $ rke up --custom-certs --cert-dir ~/my/own/certs   ","version":"Next","tagName":"h2"},{"title":"Certificates​","type":1,"pageTitle":"Custom Certificates","url":"/installation/certs#certificates","content":" The following certificates must exist in the certificate directory.  Name\tCertificate\tKeyMaster CA\tkube-ca.pem\t- Kube API\tkube-apiserver.pem\tkube-apiserver-key.pem Kube Controller Manager\tkube-controller-manager.pem\tkube-controller-manager-key.pem Kube Scheduler\tkube-scheduler.pem\tkube-scheduler-key.pem Kube Proxy\tkube-proxy.pem\tkube-proxy-key.pem Kube Admin\tkube-admin.pem\tkube-admin-key.pem Kube Node\tkube-node.pem\tkube-node-key.pem Apiserver Proxy Client\tkube-apiserver-proxy-client.pem\tkube-apiserver-proxy-client-key.pem Etcd Nodes\tkube-etcd-x-x-x-x.pem\tkube-etcd-x-x-x-x-key.pem Kube Api Request Header CA\tkube-apiserver-requestheader-ca.pem*\tkube-apiserver-requestheader-ca-key.pem Service Account Token\t-\tkube-service-account-token-key.pem  * Is the same as kube-ca.pem  ","version":"Next","tagName":"h2"},{"title":"Generating Certificate Signing Requests (CSRs) and Keys​","type":1,"pageTitle":"Custom Certificates","url":"/installation/certs#generating-certificate-signing-requests-csrs-and-keys","content":" If you want to create and sign the certificates by a real Certificate Authority (CA), you can use RKE to generate a set of Certificate Signing Requests (CSRs) and keys. Using the rke cert generate-csr command, you can generate the CSRs and keys.  Set up your cluster.yml with the node information. Run rke cert generate-csr to generate certificates for the node(s) in the cluster.yml. By default, the CSRs and keys will be saved in ./cluster_certs. To have them saved in a different directory, use --cert-dir to define what directory to have them saved in. $ rke cert generate-csr INFO[0000] Generating Kubernetes cluster CSR certificates INFO[0000] [certificates] Generating Kubernetes API server csr INFO[0000] [certificates] Generating Kube Controller csr INFO[0000] [certificates] Generating Kube Scheduler csr INFO[0000] [certificates] Generating Kube Proxy csr INFO[0001] [certificates] Generating Node csr and key INFO[0001] [certificates] Generating admin csr and kubeconfig INFO[0001] [certificates] Generating Kubernetes API server proxy client csr INFO[0001] [certificates] Generating etcd-x.x.x.x csr and key INFO[0001] Successfully Deployed certificates at [./cluster_certs] In addition to the CSRs, you also need to generate the kube-service-account-token-key.pem key. To do this, run the following: $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ./cluster_certs/kube-service-account-token-key.pem -out ./cluster_certs/kube-service-account-token.pem   Result: The CSRs and keys will be deployed in ./cluster_certs directory, assuming you didn't specify a --cert-dir. The CSR files will contain the right Alternative DNS and IP Names for the certificates. You can use them to sign the certificates by a real CA. After the certificates are signed, those certificates can be used by RKE as custom certificates.  $ tree cluster_certs cluster_certs ├── kube-admin-csr.pem ├── kube-admin-key.pem ├── kube-apiserver-csr.pem ├── kube-apiserver-key.pem ├── kube-apiserver-proxy-client-csr.pem ├── kube-apiserver-proxy-client-key.pem ├── kube-controller-manager-csr.pem ├── kube-controller-manager-key.pem ├── kube-etcd-x-x-x-x-csr.pem ├── kube-etcd-x-x-x-x-key.pem ├── kube-node-csr.pem ├── kube-node-key.pem ├── kube-proxy-csr.pem ├── kube-proxy-key.pem ├── kube-scheduler-csr.pem ├── kube-service-account-token-key.pem ├── kube-service-account-token.pem └── kube-scheduler-key.pem 0 directories, 18 files  ","version":"Next","tagName":"h2"},{"title":"Network Plug-ins","type":0,"sectionRef":"#","url":"/config-options/add-ons/network-plugins","content":"","keywords":"","version":"Next"},{"title":"Changing the Default Network Plug-in​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#changing-the-default-network-plug-in","content":" By default, the network plug-in is canal. If you want to use another network plug-in, you need to specify which network plug-in to enable at the cluster level in the cluster.yml.  ## Setting the flannel network plug-in network: plugin: flannel   The images used for network plug-ins are under the system_images directive. For each Kubernetes version, there are default images associated with each network plug-in, but these can be overridden by changing the image tag in system_images.  ","version":"Next","tagName":"h2"},{"title":"Disabling Deployment of a Network Plug-in​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#disabling-deployment-of-a-network-plug-in","content":" You can disable deploying a network plug-in by specifying none to the network plugin directive in the cluster configuration.  network: plugin: none   ","version":"Next","tagName":"h2"},{"title":"Network Plug-in Options​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#network-plug-in-options","content":" Besides the different images that could be used to deploy network plug-ins, certain network plug-ins support additional options that can be used to customize the network plug-in.  CanalFlannelCalicoWeave  ","version":"Next","tagName":"h2"},{"title":"Canal​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#canal","content":" ","version":"Next","tagName":"h2"},{"title":"Canal Network Plug-in Options​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#canal-network-plug-in-options","content":" network: plugin: canal options: canal_iface: eth1 canal_flannel_backend_type: vxlan canal_autoscaler_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+ canal_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+   ","version":"Next","tagName":"h3"},{"title":"Canal Interface​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#canal-interface","content":" By setting the canal_iface, you can configure the interface to use for inter-host communication.  The canal_flannel_backend_type option allows you to specify the type of flannel backend to use. By default the vxlan backend is used.  ","version":"Next","tagName":"h3"},{"title":"Canal Network Plug-in Tolerations​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#canal-network-plug-in-tolerations","content":" Available as of v1.2.4  The configured tolerations apply to the calico-kube-controllers Deployment.  network: plugin: canal tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300   To check for applied tolerations on the calico-kube-controllers Deployment, use the following command:  kubectl -n kube-system get deploy calico-kube-controllers -o jsonpath='{.spec.template.spec.tolerations}'   ","version":"Next","tagName":"h3"},{"title":"Flannel​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#flannel","content":" ","version":"Next","tagName":"h2"},{"title":"Flannel Network Plug-in Options​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#flannel-network-plug-in-options","content":" network: plugin: flannel options: flannel_iface: eth1 flannel_backend_type: vxlan flannel_autoscaler_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+ flannel_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+   ","version":"Next","tagName":"h3"},{"title":"Flannel Interface​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#flannel-interface","content":" By setting the flannel_iface, you can configure the interface to use for inter-host communication. The flannel_backend_type option allows you to specify the type of flannel backend to use. By default the vxlan backend is used.  ","version":"Next","tagName":"h3"},{"title":"Calico​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#calico","content":" ","version":"Next","tagName":"h2"},{"title":"Calico Network Plug-in Options​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#calico-network-plug-in-options","content":" network: plugin: calico options: calico_cloud_provider: aws calico_autoscaler_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+ calico_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+   ","version":"Next","tagName":"h3"},{"title":"Calico Cloud Provider​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#calico-cloud-provider","content":" Calico currently only supports 2 cloud providers, AWS or GCE, which can be set using calico_cloud_provider.  Valid Options  awsgce  ","version":"Next","tagName":"h3"},{"title":"Calico Network Plug-in Tolerations​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#calico-network-plug-in-tolerations","content":" Available as of v1.2.4  The configured tolerations apply to the calico-kube-controllers Deployment.  network: plugin: calico tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300   To check for applied tolerations on the calico-kube-controllers Deployment, use the following command:  kubectl -n kube-system get deploy calico-kube-controllers -o jsonpath='{.spec.template.spec.tolerations}'   ","version":"Next","tagName":"h3"},{"title":"Weave​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#weave","content":" warning Weave is deprecated in v1.27 and will be removed in v1.30  ","version":"Next","tagName":"h2"},{"title":"Weave Network Plug-in Options​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#weave-network-plug-in-options","content":" network: plugin: weave options: weave_autoscaler_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+ weave_priority_class_name: system-cluster-critical # Available as of RKE v1.2.6+ weave_network_provider: password: &quot;Q]SZOQ5wp@n$oijz&quot;   ","version":"Next","tagName":"h3"},{"title":"Weave Encryption​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#weave-encryption","content":" Weave encryption can be enabled by passing a string password to the network provider config.  ","version":"Next","tagName":"h3"},{"title":"Custom Network Plug-ins​","type":1,"pageTitle":"Network Plug-ins","url":"/config-options/add-ons/network-plugins#custom-network-plug-ins","content":" It is possible to add a custom network plug-in by using the user-defined add-on functionality of RKE. In the addons field, you can add the add-on manifest of a cluster that has the network plugin-that you want, as shown in this example. ","version":"Next","tagName":"h2"},{"title":"Configuring Pod Security Admission (PSA)","type":0,"sectionRef":"#","url":"/config-options/services/pod-security-admission","content":"","keywords":"","version":"Next"},{"title":"Method 1: Using pod_security_configuration option​","type":1,"pageTitle":"Configuring Pod Security Admission (PSA)","url":"/config-options/services/pod-security-admission#method-1-using-pod_security_configuration-option","content":" To use the built-in PSA configuration, you can set the services.kube-api.pod_security_configuration field in the cluster.yml file. Valid values for services.kube-api.pod_security_configuration are either restricted or privileged.  services: kube-api: pod_security_configuration: &lt;VALUE&gt; # restricted or privileged   If set to restricted, the PodSecurityConfiguration section from the admission configuration file below is applied:  apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - configuration: apiVersion: pod-security.admission.config.k8s.io/v1 defaults: audit: restricted audit-version: latest enforce: restricted enforce-version: latest warn: restricted warn-version: latest exemptions: namespaces: - ingress-nginx - kube-system kind: PodSecurityConfiguration name: PodSecurity path: &quot;&quot;   If set to privileged, the PodSecurityConfiguration section from the admission configuration file below is applied:  apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - configuration: apiVersion: pod-security.admission.config.k8s.io/v1 defaults: enforce: privileged enforce-version: latest exemptions: {} kind: PodSecurityConfiguration name: PodSecurity path: &quot;&quot;   ","version":"Next","tagName":"h2"},{"title":"Method 2: Using admission_configuration to pass a customized admission configuration file​","type":1,"pageTitle":"Configuring Pod Security Admission (PSA)","url":"/config-options/services/pod-security-admission#method-2-using-admission_configuration-to-pass-a-customized-admission-configuration-file","content":" You can directly pass your customized admission configuration file in the cluster.yml file by setting the services.kube-api.admission_configuration field.  note pod-security.admission.config.k8s.io/v1 configuration requires Kubernetes v1.25 and above. For Kubernetes v1.23 and v1.24, use v1beta1 instead.  services: kube-api: admission_configuration: apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: PodSecurity configuration: apiVersion: pod-security.admission.config.k8s.io/v1 kind: PodSecurityConfiguration defaults: enforce: &quot;restricted&quot; enforce-version: &quot;latest&quot; exemptions: namespaces: [&quot;cattle-system&quot;, &quot;cattle-monitoring&quot;, &quot;kube-system&quot;] runtimeClasses: [] usernames: []   ","version":"Next","tagName":"h2"},{"title":"Method 3: Using extra_args to pass customized admission configuration file​","type":1,"pageTitle":"Configuring Pod Security Admission (PSA)","url":"/config-options/services/pod-security-admission#method-3--using-extra_args-to-pass-customized-admission-configuration-file","content":" If you want to use an external admission configuration file in a control plane node, you can use the services.kube-api.extra_args option to set the path to the file and the services.kube-api.extra_binds option to mount the file in the kube-apiserver.  note pod-security.admission.config.k8s.io/v1 configuration requires Kubernetes v1.25 and above. For Kubernetes v1.23 and v1.24, use v1beta1 instead.  services: kube-api: extra_args: admission-control-config-file: &lt;PATH&gt; # path to the file extra_binds: - &quot;&lt;PATH-IN-NODES&gt;:&lt;PATH-IN-CONTAINER&gt;&quot;  ","version":"Next","tagName":"h2"},{"title":"System Images","type":0,"sectionRef":"#","url":"/config-options/system-images","content":"","keywords":"","version":"Next"},{"title":"Air-gapped Setups​","type":1,"pageTitle":"System Images","url":"/config-options/system-images#air-gapped-setups","content":" If you have an air-gapped setup and cannot access docker.io, you will need to set up your private registry in your cluster configuration file. After you set up private registry, you will need to update these images to pull from your private registry. ","version":"Next","tagName":"h3"},{"title":"One-time Snapshots","type":0,"sectionRef":"#","url":"/etcd-snapshots/one-time-snapshots","content":"One-time Snapshots One-time snapshots are handled differently depending on your version of RKE. RKE v0.2.0+RKE before v0.2.0 To save a snapshot of etcd from each etcd node in the cluster config file, run the rke etcd snapshot-save command. The snapshot is saved in /opt/rke/etcd-snapshots. When running the command, an additional container is created to take the snapshot. When the snapshot is completed, the container is automatically removed. The one-time snapshot can be uploaded to a S3 compatible backend by using the additional options to specify the S3 backend. To create a local one-time snapshot, run: $ rke etcd snapshot-save --config cluster.yml --name snapshot-name Result: The snapshot is saved in /opt/rke/etcd-snapshots. To save a one-time snapshot to S3, run: $ rke etcd snapshot-save \\ --config cluster.yml \\ --name snapshot-name \\ --s3 \\ --access-key S3_ACCESS_KEY \\ --secret-key S3_SECRET_KEY \\ --bucket-name s3-bucket-name \\ --folder s3-folder-name \\ # Optional - Available as of v0.3.0 --s3-endpoint s3.amazonaws.com Result: The snapshot is saved in /opt/rke/etcd-snapshots as well as uploaded to the S3 backend. Options for rke etcd snapshot-save​ Option\tDescription\tS3 Specific--name value\tSpecify snapshot name --config value\tSpecify an alternate cluster YAML file (default: cluster.yml) [$RKE_CONFIG] --s3\tEnabled backup to s3\t* --s3-endpoint value\tSpecify s3 endpoint url (default: &quot;s3.amazonaws.com&quot;)\t* --s3-endpoint-ca value\tSpecify a path to a CA cert file to connect to a custom s3 endpoint (optional) Available as of v0.2.5\t* --access-key value\tSpecify s3 accessKey\t* --secret-key value\tSpecify s3 secretKey\t* --bucket-name value\tSpecify s3 bucket name\t* --folder value\tSpecify folder inside bucket where backup will be stored. This is optional. Available as of v0.3.0\t* --region value\tSpecify the s3 bucket location (optional)\t* --ssh-agent-auth\tUse SSH Agent Auth defined by SSH_AUTH_SOCK --ignore-docker-version\tDisable Docker version check The --access-key and --secret-key options are not required if the etcd nodes are AWS EC2 instances that have been configured with a suitable IAM instance profile. Using a custom CA certificate for S3​ Available as of v0.2.0 The backup snapshot can be stored on a custom S3 backup like minio. If the S3 backend uses a self-signed or custom certificate, provide a custom certificate using the --s3-endpoint-ca to connect to the S3 backend. IAM Support for Storing Snapshots in S3​ In addition to API access keys, RKE supports using IAM roles for S3 authentication. The cluster etcd nodes must be assigned an IAM role that has read/write access to the designated backup bucket on S3. Also, the nodes must have network access to the S3 endpoint specified. Below is an example IAM policy that would allow nodes to store and retrieve backups from S3: { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;ListObjectsInBucket&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [&quot;s3:ListBucket&quot;], &quot;Resource&quot;: [&quot;arn:aws:s3:::bucket-name&quot;] }, { &quot;Sid&quot;: &quot;AllObjectActions&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;s3:*Object&quot;, &quot;Resource&quot;: [&quot;arn:aws:s3:::bucket-name/*&quot;] } ] } For details on giving an application access to S3, refer to the AWS documentation on Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances.","keywords":"","version":"Next"},{"title":"Recurring Snapshots","type":0,"sectionRef":"#","url":"/etcd-snapshots/recurring-snapshots","content":"Recurring Snapshots Recurring snapshots are handled differently based on your version of RKE. RKE v0.2.0+RKE before v0.2.0 To schedule automatic recurring etcd snapshots, you can enable the etcd-snapshot service with extra configuration options. etcd-snapshot runs in a service container alongside the etcd container. By default, the etcd-snapshot service takes a snapshot for every node that has the etcd role and stores them to local disk in /opt/rke/etcd-snapshots. If you set up the options for S3, the snapshot will also be uploaded to the S3 backend. Snapshot Service Logging​ When a cluster is launched with the etcd-snapshot service enabled, you can view the etcd-rolling-snapshots logs to confirm backups are being created automatically. $ docker logs etcd-rolling-snapshots time=&quot;2018-05-04T18:39:16Z&quot; level=info msg=&quot;Initializing Rolling Backups&quot; creation=1m0s retention=24h0m0s time=&quot;2018-05-04T18:40:16Z&quot; level=info msg=&quot;Created backup&quot; name=&quot;2018-05-04T18:40:16Z_etcd&quot; runtime=108.332814ms time=&quot;2018-05-04T18:41:16Z&quot; level=info msg=&quot;Created backup&quot; name=&quot;2018-05-04T18:41:16Z_etcd&quot; runtime=92.880112ms time=&quot;2018-05-04T18:42:16Z&quot; level=info msg=&quot;Created backup&quot; name=&quot;2018-05-04T18:42:16Z_etcd&quot; runtime=83.67642ms time=&quot;2018-05-04T18:43:16Z&quot; level=info msg=&quot;Created backup&quot; name=&quot;2018-05-04T18:43:16Z_etcd&quot; runtime=86.298499ms Options for the Etcd-Snapshot Service​ Option\tDescription\tS3 Specificinterval_hours\tThe duration in hours between recurring backups. This supercedes the creation option (which was used in RKE before v0.2.0) and will override it if both are specified. (Default: 12) retention\tThe number of snapshots to retain before rotation. If the retention is configured in both etcd.retention (time period to keep snapshots in hours), which was required in RKE before v0.2.0, and at etcd.backup_config.retention (number of snapshots), the latter will be used. (Default: 6) bucket_name\tS3 bucket name where backups will be stored\t* folder\tFolder inside S3 bucket where backups will be stored. This is optional. Available as of v0.3.0\t* access_key\tS3 access key with permission to access the backup bucket.\t* secret_key\tS3 secret key with permission to access the backup bucket.\t* region\tS3 region for the backup bucket. This is optional.\t* endpoint\tS3 regions endpoint for the backup bucket.\t* custom_ca\tCustom certificate authority to use when connecting to the endpoint. Only required for private S3 compatible storage solutions. Available for RKE v0.2.5+.\t* The --access-key and --secret-key options are not required if the etcd nodes are AWS EC2 instances that have been configured with a suitable IAM instance profile. Using a custom CA certificate for S3​ The backup snapshot can be stored on a custom S3 backup like minio. If the S3 backend uses a self-signed or custom certificate, provide a custom certificate using the option custom_ca to connect to the S3 backend. IAM Support for Storing Snapshots in S3​ In addition to API access keys, RKE supports using IAM roles for S3 authentication. The cluster etcd nodes must be assigned an IAM role that has read/write access to the designated backup bucket on S3. Also, the nodes must have network access to the S3 endpoint specified. Below is an example IAM policy that would allow nodes to store and retrieve backups from S3: { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;ListObjectsInBucket&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [&quot;s3:ListBucket&quot;], &quot;Resource&quot;: [&quot;arn:aws:s3:::bucket-name&quot;] }, { &quot;Sid&quot;: &quot;AllObjectActions&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;s3:*Object&quot;, &quot;Resource&quot;: [&quot;arn:aws:s3:::bucket-name/*&quot;] } ] } For details on giving an application access to S3, refer to the AWS documentation on Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances. Configuring the Snapshot Service in YAML​ services: etcd: backup_config: interval_hours: 12 retention: 6 s3backupconfig: access_key: S3_ACCESS_KEY secret_key: S3_SECRET_KEY bucket_name: s3-bucket-name region: &quot;&quot; folder: &quot;&quot; # Optional - Available as of v0.3.0 endpoint: s3.amazonaws.com custom_ca: |- -----BEGIN CERTIFICATE----- $CERTIFICATE -----END CERTIFICATE----- ","keywords":"","version":"Next"},{"title":"Restoring from Backup","type":0,"sectionRef":"#","url":"/etcd-snapshots/restoring-from-backup","content":"Restoring from Backup The details of restoring your cluster from backup are different depending on your version of RKE. RKE v0.2.0+RKE before v0.2.0 If there is a disaster with your Kubernetes cluster, you can use rke etcd snapshot-restore to recover your etcd. This command reverts etcd to a specific snapshot and should be run on an etcd node of the the specific cluster that has suffered the disaster. The following actions will be performed when you run the command: Syncs the snapshot or downloads the snapshot from S3, if necessary.Checks snapshot checksum across etcd nodes to make sure they are identical.Deletes your current cluster and cleans old data by running rke remove. This removes the entire Kubernetes cluster, not just the etcd cluster.Rebuilds the etcd cluster from the chosen snapshot.Creates a new cluster by running rke up.Restarts cluster system pods. danger You should back up any important data in your cluster before running rke etcd snapshot-restore because the command deletes your current Kubernetes cluster and replaces it with a new one. The snapshot used to restore your etcd cluster can either be stored locally in /opt/rke/etcd-snapshots or from a S3 compatible backend. Available as of v1.1.4 If the snapshot contains the cluster state file, it will automatically be extracted and used for the restore. If you want to force the use of the local state file, you can add --use-local-state to the command. If the snapshot was created using an RKE version before v1.1.4, or if the snapshot does not contain a state file, make sure the cluster state file (by default available as cluster.rkestate) is present before executing the command. Example of Restoring from a Local Snapshot​ To restore etcd from a local snapshot, run: $ rke etcd snapshot-restore --config cluster.yml --name mysnapshot The snapshot is assumed to be located in /opt/rke/etcd-snapshots. Note: The pki.bundle.tar.gz file is not needed because RKE v0.2.0 changed how the Kubernetes cluster state is stored. Example of Restoring from a Snapshot in S3​ When restoring etcd from a snapshot located in S3, the command needs the S3 information in order to connect to the S3 backend and retrieve the snapshot. $ rke etcd snapshot-restore \\ --config cluster.yml \\ --name snapshot-name \\ --s3 \\ --access-key S3_ACCESS_KEY \\ --secret-key S3_SECRET_KEY \\ --bucket-name s3-bucket-name \\ --folder s3-folder-name \\ # Optional - Available as of v0.3.0 --s3-endpoint s3.amazonaws.com Note: if you were restoring a cluster that had Rancher installed, the Rancher UI should start up after a few minutes; you don't need to re-run Helm. Options for rke etcd snapshot-restore​ Option\tDescription\tS3 Specific--name value\tSpecify snapshot name --config value\tSpecify an alternate cluster YAML file (default: cluster.yml) [$RKE_CONFIG] --use-local-state\tForce the use of the local state file instead of looking for a state file in the snapshot Available as of v1.1.4 --s3\tEnabled backup to s3\t* --s3-endpoint value\tSpecify s3 endpoint url (default: &quot;s3.amazonaws.com&quot;)\t* --access-key value\tSpecify s3 accessKey\t* --secret-key value\tSpecify s3 secretKey\t* --bucket-name value\tSpecify s3 bucket name\t* --folder value\tSpecify folder inside bucket where backup will be stored. This is optional. This is optional. Available as of v0.3.0\t* --region value\tSpecify the s3 bucket location (optional)\t* --ssh-agent-auth\tUse SSH Agent Auth defined by SSH_AUTH_SOCK --ignore-docker-version\tDisable Docker version check\t","keywords":"","version":"Next"},{"title":"Kubeconfig File","type":0,"sectionRef":"#","url":"/kubeconfig","content":"Kubeconfig File In order to start interacting with your Kubernetes cluster, you will use a different binary called kubectl. You will need to install kubectl on your local machine. A kubeconfig file is a file used to configure access to Kubernetes when used in conjunction with the kubectl commandline tool (or other clients). For more details on how kubeconfig and kubectl work together, see the Kubernetes documentation. When you deployed Kubernetes, a kubeconfig is automatically generated for your RKE cluster. This file is created and saved as kube_config_cluster.yml. note By default, kubectl checks ~/.kube/config for a kubeconfig file, but you can use any directory you want using the --kubeconfig flag. For example: kubectl --kubeconfig /custom/path/kube.config get pods Confirm that kubectl is working by checking the version of your Kubernetes cluster kubectl --kubeconfig kube_config_cluster.yml version Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;10&quot;, GitVersion:&quot;v1.10.0&quot;, GitCommit:&quot;fc32d2f3698e36b93322a3465f63a14e9f0eaead&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-27T00:13:02Z&quot;, GoVersion:&quot;go1.9.4&quot;, Compiler:&quot;gc&quot;, Platform:&quot;darwin/amd64&quot;} Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;8+&quot;, GitVersion:&quot;v1.8.9-rancher1&quot;, GitCommit:&quot;68595e18f25e24125244e9966b1e5468a98c1cd4&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-13T04:37:53Z&quot;, GoVersion:&quot;go1.8.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} The client and server version are reported, indicating that you have a local kubectl client and are able to request the server version from the newly built cluster. Now, you can issue any kubectl command to your cluster, like requesting the nodes that are in the cluster. kubectl --kubeconfig kube_config_cluster.yml get nodes NAME STATUS ROLES AGE VERSION 10.0.0.1 Ready controlplane,etcd,worker 35m v1.10.3-rancher1 ","keywords":"","version":"Next"},{"title":"Adding and Removing Nodes","type":0,"sectionRef":"#","url":"/managing-clusters","content":"","keywords":"","version":"Next"},{"title":"Adding/Removing Nodes​","type":1,"pageTitle":"Adding and Removing Nodes","url":"/managing-clusters#addingremoving-nodes","content":" RKE supports adding/removing nodes for worker and controlplane hosts.  In order to add additional nodes, you update the original cluster.yml file with any additional nodes and specify their role in the Kubernetes cluster.  In order to remove nodes, remove the node information from the nodes list in the original cluster.yml.  After you've made changes to add/remove nodes, run rke up with the updated cluster.yml.  ","version":"Next","tagName":"h3"},{"title":"Adding/Removing Worker Nodes​","type":1,"pageTitle":"Adding and Removing Nodes","url":"/managing-clusters#addingremoving-worker-nodes","content":" You can add/remove only worker nodes, by running rke up --update-only. This will ignore everything else in the cluster.yml except for any worker nodes.  note When using --update-only, other actions that do not specifically relate to nodes may be deployed or updated, for example addons.  ","version":"Next","tagName":"h3"},{"title":"Removing Kubernetes Components from Nodes​","type":1,"pageTitle":"Adding and Removing Nodes","url":"/managing-clusters#removing-kubernetes-components-from-nodes","content":" In order to remove the Kubernetes components from nodes, you use the rke remove command.  danger This command is irreversible and will destroy the Kubernetes cluster, including etcd snapshots on S3. If there is a disaster and your cluster is inaccessible, refer to the process for restoring your cluster from a snapshot.  The rke remove command does the following to each node in the cluster.yml:  Remove the Kubernetes component deployed on it etcdkube-apiserverkube-controller-managerkubeletkube-proxynginx-proxy  The cluster's etcd snapshots are removed, including both local snapshots and snapshots that are stored on S3.  note Pods are not removed from the nodes. If the node is re-used, the pods will automatically be removed when the new Kubernetes cluster is created.  Clean each host from the directories left by the services: /etc/kubernetes/ssl/var/lib/etcd/etc/cni/opt/cni/var/run/calico ","version":"Next","tagName":"h3"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/troubleshooting","content":"Troubleshooting SSH Connectivity ErrorsProvisioning Errors","keywords":"","version":"Next"},{"title":"Provisioning Errors","type":0,"sectionRef":"#","url":"/troubleshooting/provisioning-errors","content":"","keywords":"","version":"Next"},{"title":"Failed to get job complete status​","type":1,"pageTitle":"Provisioning Errors","url":"/troubleshooting/provisioning-errors#failed-to-get-job-complete-status","content":" Most common reason for this error is that a node is having issues that block the deploy job from completing successfully. See Get node conditions how to check node conditions.  You can also retrieve the log from the job to see if it has an indication of the error, make sure you replace rke-network-plugin-deploy-job with the job name from the error:  Example command to get logs for error Failed to get job complete status for job rke-network-plugin-deploy-job:  kubectl -n kube-system get pods -l job-name=rke-network-plugin-deploy-job --no-headers -o custom-columns=NAME:.metadata.name | xargs -L1 kubectl -n kube-system logs   ","version":"Next","tagName":"h3"},{"title":"Failed to apply the ServiceAccount needed for job execution​","type":1,"pageTitle":"Provisioning Errors","url":"/troubleshooting/provisioning-errors#failed-to-apply-the-serviceaccount-needed-for-job-execution","content":" Because this action requires connectivity from the host running rke up to the controlplane nodes, this is usually caused by incorrect proxy configuration on the host running rke up. The message printed after this error usually is the response from the proxy that is blocking the request. Please verify the HTTP_PROXY, HTTPS_PROXY and NO_PROXY environment variables are correctly configured, especially NO_PROXY if the host cannot reach the controlplane nodes via the configured proxy. (this IP range then needs to be added to NO_PROXY to make it work) ","version":"Next","tagName":"h3"},{"title":"RKE Kubernetes Installation","type":0,"sectionRef":"#","url":"/installation","content":"","keywords":"","version":"Next"},{"title":"Download the RKE binary​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#download-the-rke-binary","content":" From your workstation, open a web browser and look up the latest available RKE release. You can click on the release notes link to go straight to that release or manually navigate to our RKE Releases page and download the latest available RKE installer applicable to your operating system and architecture: Note:Be aware that the release that is marked as Latest release on GitHub release page might not be the actual latest available release of RKE. macOS: rke_darwin-amd64Linux (Intel/AMD): rke_linux-amd64Linux (ARM 32-bit): rke_linux-armLinux (ARM 64-bit): rke_linux-arm64Windows (32-bit): rke_windows-386.exeWindows (64-bit): rke_windows-amd64.exe Copy the RKE binary to a folder in your $PATH and rename it rke (or rke.exe for Windows) # macOS $ mv rke_darwin-amd64 rke # Linux $ mv rke_linux-amd64 rke # Windows PowerShell &gt; mv rke_windows-amd64.exe rke.exe Make the RKE binary that you just downloaded executable. Open Terminal, change directory to the location of the RKE binary, and then run one of the commands below. Using Windows?The file is already an executable. Skip to Prepare the Nodes for the Kubernetes Cluster. $ chmod +x rke Confirm that RKE is now executable by running the following command: $ rke --version   ","version":"Next","tagName":"h2"},{"title":"Alternative RKE macOS Install - Homebrew​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#alternative-rke-macos-install---homebrew","content":" RKE can also be installed and updated using Homebrew, a package manager for macOS.  Install Homebrew. See https://brew.sh/ for instructions. Using brew, install RKE by running the following command in a Terminal window: $ brew install rke   If you have already installed RKE using brew, you can upgrade RKE by running:  $ brew upgrade rke   ","version":"Next","tagName":"h3"},{"title":"Alternative RKE macOS Install - MacPorts​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#alternative-rke-macos-install---macports","content":" RKE can also be installed and updated using MacPorts, a package manager for macOS.  Install MacPorts. See https://www.macports.org/ for instructions. Using port, install RKE by running the following command in a Terminal window: $ port install rke   If you have already installed RKE using port, you can upgrade RKE by running:  $ port upgrade rke   ","version":"Next","tagName":"h3"},{"title":"Prepare the Nodes for the Kubernetes cluster​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#prepare-the-nodes-for-the-kubernetes-cluster","content":" The Kubernetes cluster components are launched using Docker on a Linux distro. You can use any Linux you want, as long as you can install Docker on it.  tip For information on which Docker versions were tested with your version of RKE, refer to the support matrix for installing Rancher on RKE.  Review the OS requirements and configure each node appropriately.  ","version":"Next","tagName":"h2"},{"title":"Creating the Cluster Configuration File​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#creating-the-cluster-configuration-file","content":" RKE uses a cluster configuration file, referred to as cluster.yml to determine what nodes will be in the cluster and how to deploy Kubernetes. There are many configuration options that can be set in the cluster.yml. In our example, we will be assuming the minimum of one node for your Kubernetes cluster.  There are two easy ways to create a cluster.yml:  Using our minimal cluster.yml and updating it based on the node that you will be using.Using rke config to query for all the information needed.  ","version":"Next","tagName":"h2"},{"title":"Using rke config​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#using-rke-config","content":" Run rke config to create a new cluster.yml in the current directory. This command will prompt you for all the information needed to build a cluster. See cluster configuration options for details on the various options.  rke config --name cluster.yml   Other RKE Configuration Options​  You can create an empty template cluster.yml file by specifying the --empty flag.  rke config --empty --name cluster.yml   Instead of creating a file, you can print the generated configuration to stdout using the --print flag.  rke config --print   ","version":"Next","tagName":"h3"},{"title":"High Availability​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#high-availability","content":" RKE is HA ready, you can specify more than one controlplane node in the cluster.yml file. RKE will deploy master components on all of these nodes and the kubelets are configured to connect to 127.0.0.1:6443 by default which is the address of nginx-proxy service that proxy requests to all master nodes.  To create an HA cluster, specify more than one host with role controlplane.  ","version":"Next","tagName":"h3"},{"title":"Certificates​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#certificates","content":" Available as of v0.2.0  By default, Kubernetes clusters require certificates and RKE auto-generates the certificates for all cluster components. You can also use custom certificates. After the Kubernetes cluster is deployed, you can manage these auto-generated certificates.  ","version":"Next","tagName":"h3"},{"title":"Deploying Kubernetes with RKE​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#deploying-kubernetes-with-rke","content":" After you've created your cluster.yml, you can deploy your cluster with a simple command. This command assumes the cluster.yml file is in the same directory as where you are running the command.  rke up INFO[0000] Building Kubernetes cluster INFO[0000] [dialer] Setup tunnel for host [10.0.0.1] INFO[0000] [network] Deploying port listener containers INFO[0000] [network] Pulling image [alpine:latest] on host [10.0.0.1] ... INFO[0101] Finished building Kubernetes cluster successfully   The last line should read Finished building Kubernetes cluster successfully to indicate that your cluster is ready to use. As part of the Kubernetes creation process, a kubeconfig file has been created and written at kube_config_cluster.yml, which can be used to start interacting with your Kubernetes cluster.  note If you have used a different file name from cluster.yml, then the kube config file will be named kube_config_&lt;FILE_NAME&gt;.yml.  ","version":"Next","tagName":"h2"},{"title":"Save Your Files​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#save-your-files","content":" Important The files mentioned below are needed to maintain, troubleshoot and upgrade your cluster.  Save a copy of the following files in a secure location:  cluster.yml: The RKE cluster configuration file.kube_config_cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster.cluster.rkestate: The Kubernetes Cluster State file, this file contains credentials for full access to the cluster. The Kubernetes Cluster State file is only created when using RKE v0.2.0 or higher.  note The &quot;rancher-cluster&quot; parts of the two latter file names are dependent on how you name the RKE cluster configuration file.  ","version":"Next","tagName":"h2"},{"title":"Kubernetes Cluster State​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#kubernetes-cluster-state","content":" The Kubernetes cluster state, which consists of the cluster configuration file cluster.yml and components certificates in Kubernetes cluster, is saved by RKE, but depending on your RKE version, the cluster state is saved differently.  As of v0.2.0, RKE creates a .rkestate file in the same directory that has the cluster configuration file cluster.yml. The .rkestate file contains the current state of the cluster including the RKE configuration and the certificates. It is required to keep this file in order to update the cluster or perform any operation on it through RKE.  Before v0.2.0, RKE saved the Kubernetes cluster state as a secret. When updating the state, RKE pulls the secret, updates/changes the state and saves a new secret.  ","version":"Next","tagName":"h3"},{"title":"Interacting with your Kubernetes cluster​","type":1,"pageTitle":"RKE Kubernetes Installation","url":"/installation#interacting-with-your-kubernetes-cluster","content":" After your cluster is up and running, you can start using the generated kubeconfig file to start interacting with your Kubernetes cluster using kubectl.  After installation, there are several maintenance items that might arise:  Certificate ManagementAdding and Removing Nodes in the cluster ","version":"Next","tagName":"h2"},{"title":"SSH Connectivity Errors","type":0,"sectionRef":"#","url":"/troubleshooting/ssh-connectivity-errors","content":"","keywords":"","version":"Next"},{"title":"Failed to set up SSH tunneling for host [xxx.xxx.xxx.xxx]: Can't retrieve Docker Info​","type":1,"pageTitle":"SSH Connectivity Errors","url":"/troubleshooting/ssh-connectivity-errors#failed-to-set-up-ssh-tunneling-for-host-xxxxxxxxxxxx-cant-retrieve-docker-info","content":" Failed to dial to /var/run/docker.sock: ssh: rejected: administratively prohibited (open failed)​  User specified to connect with does not have permission to access the Docker socket. This can be checked by logging into the host and running the command docker ps:  $ ssh -i ssh_privatekey_file user@server user@server$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES   See Manage Docker as a non-root user how to set this up properly.  When using RedHat/CentOS as operating system, you cannot use the user root to connect to the nodes because of Bugzilla #1527565. You will need to add a separate user and configure it to access the Docker socket. See RKE OS Requirements for more on how to set this up. SSH server version is not version 6.7 or higher. This is needed for socket forwarding to work, which is used to connect to the Docker socket over SSH. This can be checked using sshd -V on the host you are connecting to, or using netcat:  $ nc xxx.xxx.xxx.xxx 22 SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.10   Failed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: no key found​  The key file specified as ssh_key_path cannot be accessed. Make sure that you specified the private key file (not the public key, .pub), and that the user that is running the rke command can access the private key file.The key file specified as ssh_key_path is malformed. Check if the key is valid by running ssh-keygen -y -e -f private_key_file. This will print the public key of the private key, which will fail if the private key file is not valid.  Failed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain​  The key file specified as ssh_key_path is not correct for accessing the node. Double-check if you specified the correct ssh_key_path for the node and if you specified the correct user to connect with.  Failed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: cannot decode encrypted private keys​  If you want to use encrypted private keys, you should use ssh-agent to load your keys with your passphrase. You can configure RKE to use that agent by specifying --ssh-agent-auth on the command-line, it will use the SSH_AUTH_SOCK environment variable in the environment where the rke command is run.  Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?​  The node is not reachable on the configured address and port. ","version":"Next","tagName":"h3"},{"title":"Maintaining Availability for Applications During Upgrades","type":0,"sectionRef":"#","url":"/upgrades/maintaining-availability","content":"","keywords":"","version":"Next"},{"title":"1. Kubernetes Version Requirement​","type":1,"pageTitle":"Maintaining Availability for Applications During Upgrades","url":"/upgrades/maintaining-availability#1-kubernetes-version-requirement","content":" When upgrading to a newer Kubernetes version, the upgrade must be from a minor release to the next minor version, or to within the same patch release series.  ","version":"Next","tagName":"h3"},{"title":"2. Cluster Requirements​","type":1,"pageTitle":"Maintaining Availability for Applications During Upgrades","url":"/upgrades/maintaining-availability#2-cluster-requirements","content":" The following must be true of the cluster that will be upgraded:  The cluster has three or more etcd nodes.The cluster has two or more controlplane nodes.The cluster has two or more worker nodes.The Ingress, DNS, and other addons are schedulable to a number of nodes that exceeds the maximum number of unavailable worker nodes, also called the batch size. By default, the minimum number of unavailable worker nodes is 10 percent of worker nodes, rounded down to the nearest node, with a minimum batch size of one node.  ","version":"Next","tagName":"h3"},{"title":"3. Workload Requirements​","type":1,"pageTitle":"Maintaining Availability for Applications During Upgrades","url":"/upgrades/maintaining-availability#3-workload-requirements","content":" The following must be true of the cluster's applications:  The application and Ingress are deployed across a number of nodes exceeding the maximum number of unavailable worker nodes, also called the batch size. By default, the minimum number of unavailable worker nodes is 10 percent of worker nodes, rounded down to the nearest node, with a minimum batch size of one node.The applications must make use of liveness and readiness probes.  For information on how to use node selectors to assign pods to nodes, refer to the official Kubernetes documentation.  For information on configuring the number of replicas for each addon, refer to this section. ","version":"Next","tagName":"h3"},{"title":"Overview of RKE","type":0,"sectionRef":"#","url":"/","content":"","keywords":"","version":"Next"},{"title":"TEST SECTION​","type":1,"pageTitle":"Overview of RKE","url":"/#test-section","content":" SOME INSTRUCTIONS  DO THIS THING RUN THIS CODE: helm repo add rancher-stable https://releases.rancher.com/server-charts/stable FINISH TASK ","version":"Next","tagName":"h3"},{"title":"Upgrades","type":0,"sectionRef":"#","url":"/upgrades","content":"","keywords":"","version":"Next"},{"title":"How Upgrades Work​","type":1,"pageTitle":"Upgrades","url":"/upgrades#how-upgrades-work","content":" In this section, you'll learn what happens when you edit or upgrade your RKE Kubernetes cluster.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Upgrades","url":"/upgrades#prerequisites","content":" Ensure that any system_images configuration is absent from the cluster.yml. The Kubernetes version should only be listed under the system_images directive if an unsupported version is being used. Refer to Kubernetes version precedence for more information.Ensure that the correct files to manage Kubernetes cluster state are present in the working directory. Refer to the tabs below for the required files, which differ based on the RKE version.  RKE v0.2.0+RKE before v0.2.0 The cluster.rkestate file contains the current state of the cluster including the RKE configuration and the certificates. This file is created in the same directory that has the cluster configuration file cluster.yml. It is required to keep the cluster.rkestate file to perform any operation on the cluster through RKE, or when upgrading a cluster last managed via RKE v0.2.0 or later.  ","version":"Next","tagName":"h3"},{"title":"Upgrading Kubernetes​","type":1,"pageTitle":"Upgrades","url":"/upgrades#upgrading-kubernetes","content":" To upgrade the Kubernetes version of an RKE-provisioned cluster, set the kubernetes_version string in the cluster.yml to the desired version from the list of supported Kubernetes versions for the specific version of RKE:  kubernetes_version: &quot;v1.15.5-rancher1-1&quot;   Then invoke rke up:  $ rke up --config cluster.yml   ","version":"Next","tagName":"h3"},{"title":"Configuring the Upgrade Strategy​","type":1,"pageTitle":"Upgrades","url":"/upgrades#configuring-the-upgrade-strategy","content":" As of v0.1.8, upgrades to add-ons are supported. Add-ons can also be upgraded by changing any of the add-ons and running rke up again with the updated configuration file.  As of v1.1.0, additional upgrade options became available to give you more granular control over the upgrade process. These options can be used to maintain availability of your applications during a cluster upgrade.  For details on upgrade configuration options, refer to Configuring the Upgrade Strategy.  ","version":"Next","tagName":"h3"},{"title":"Maintaining Availability for Applications During Upgrades​","type":1,"pageTitle":"Upgrades","url":"/upgrades#maintaining-availability-for-applications-during-upgrades","content":" In this section, you'll learn the requirements to prevent downtime for your applications when you upgrade the cluster using rke up.  ","version":"Next","tagName":"h3"},{"title":"Listing Supported Kubernetes Versions​","type":1,"pageTitle":"Upgrades","url":"/upgrades#listing-supported-kubernetes-versions","content":" Please refer to the release notes of the RKE version that you are running, to find the list of supported Kubernetes versions as well as the default Kubernetes version. Note: RKE v1.x should be used.  You can also list the supported versions and system images of specific version of RKE release with a quick command.  $ rke config --list-version --all v1.15.3-rancher2-1 v1.13.10-rancher1-2 v1.14.6-rancher2-1 v1.16.0-beta.1-rancher1-1   ","version":"Next","tagName":"h3"},{"title":"Kubernetes Version Precedence​","type":1,"pageTitle":"Upgrades","url":"/upgrades#kubernetes-version-precedence","content":" In case both kubernetes_version and system_images are defined, the system_images configuration will take precedence over kubernetes_version.  In addition, if neither kubernetes_version nor system_images are configured in the cluster.yml, RKE will apply the default Kubernetes version for the specific version of RKE used to invoke rke up.  ","version":"Next","tagName":"h3"},{"title":"Using an Unsupported Kubernetes Version​","type":1,"pageTitle":"Upgrades","url":"/upgrades#using-an-unsupported-kubernetes-version","content":" As of v0.2.0, if a version is defined in kubernetes_version and is not found in the specific list of supported Kubernetes versions, then RKE will error out.  Before v0.2.0, if a version is defined in kubernetes_version and is not found in the specific list of supported Kubernetes versions, the default version from the supported list is used.  If you want to use a different version from the supported list, please use the system images option.  ","version":"Next","tagName":"h3"},{"title":"Mapping the Kubernetes Version to Services​","type":1,"pageTitle":"Upgrades","url":"/upgrades#mapping-the-kubernetes-version-to-services","content":" In RKE, kubernetes_version is used to map the version of Kubernetes to the default services, parameters, and options.  For RKE v0.3.0+, the service defaults are located here.  For RKE before v0.3.0, the service defaults are located here. Note: The version in the path of the service defaults file corresponds to a Rancher version. Therefore, for Rancher v2.1.x, this file should be used.  ","version":"Next","tagName":"h3"},{"title":"Service Upgrades​","type":1,"pageTitle":"Upgrades","url":"/upgrades#service-upgrades","content":" Services can be upgraded by changing any of the services arguments or extra_args and running rke up again with the updated configuration file.  note The following arguments, service_cluster_ip_range or cluster_cidr, cannot be changed as any changes to these arguments will result in a broken cluster. Currently, network pods are not automatically upgraded.  ","version":"Next","tagName":"h3"},{"title":"Upgrading Nodes Manually​","type":1,"pageTitle":"Upgrades","url":"/upgrades#upgrading-nodes-manually","content":" Available as of v1.1.0  You can manually update each type of node separately. As a best practice, upgrade the etcd nodes first, followed by controlplane and then worker nodes.  ","version":"Next","tagName":"h3"},{"title":"Rolling Back the Kubernetes Version​","type":1,"pageTitle":"Upgrades","url":"/upgrades#rolling-back-the-kubernetes-version","content":" Available as of v1.1.0  A cluster can be restored back to a snapshot that uses a previous Kubernetes version.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Upgrades","url":"/upgrades#troubleshooting","content":" Applies to v1.1.0+  If a node doesn't come up after an upgrade, the rke up command errors out.  No upgrade will proceed if the number of unavailable nodes exceeds the configured maximum.  If an upgrade stops, you may need to fix an unavailable node or remove it from the cluster before the upgrade can continue.  A failed node could be in many different states:  Powered offUnavailableUser drains a node while upgrade is in process, so there are no kubelets on the nodeThe upgrade itself failed  Some expected failure scenarios include the following:  If the maximum unavailable number of nodes is reached during an upgrade, the RKE CLI will error out and exit the CLI with a failure code.If some nodes fail to upgrade, but the number of failed nodes doesn't reach the maximum unavailable number of nodes, the RKE CLI logs the nodes that were unable to upgrade and continues to upgrade the add-ons. After the add-ons are upgraded, RKE will error out and exit the CLI with a failure code regardless of add-on upgrade status. ","version":"Next","tagName":"h3"},{"title":"How Upgrades Work","type":0,"sectionRef":"#","url":"/upgrades/how-upgrades-work","content":"How Upgrades Work In this section, you'll learn what happens when you edit or upgrade your RKE Kubernetes cluster. The below sections describe how each type of node is upgraded by default when a cluster is upgraded using rke up. RKE v1.1.0+RKE before v1.1.0 The following features are new in RKE v1.1.0: The ability to upgrade or edit a cluster without downtime for your applications.The ability to manually upgrade nodes of a certain role without upgrading others.The ability to restore a Kubernetes cluster to an older Kubernetes version by restoring it to a snapshot that includes the older Kubernetes version. This capability allows you to safely upgrade one type of node at a time, because if an upgrade cannot be completed by all nodes in the cluster, you can downgrade the Kubernetes version of the nodes that were already upgraded. When a cluster is upgraded with rke up, using the default options, the following process is used: The etcd plane gets get updated, one node at a time.Controlplane nodes get updated, one node at a time. This includes the controlplane components and worker plane components of the controlplane nodes.Worker plane components of etcd nodes get updated, one node at a time.Worker nodes get updated in batches of a configurable size. The default configuration for the maximum number of unavailable nodes is ten percent, rounded down to the nearest node, with a minimum batch size of one node.Addons get upgraded one by one. The following sections break down in more detail what happens when etcd nodes, controlplane nodes, worker nodes, and addons are upgraded. This information is intended to be used to help you understand the update strategy for the cluster, and may be useful when troubleshooting problems with upgrading the cluster. Upgrades of etcd Nodes​ A cluster upgrade begins by upgrading the etcd nodes one at a time. If an etcd node fails at any time, the upgrade will fail and no more nodes will be upgraded. The cluster will be stuck in an updating state and not move forward to upgrading controlplane or worker nodes. Upgrades of Controlplane Nodes​ Controlplane nodes are upgraded one at a time by default. The maximum number of unavailable controlplane nodes can also be configured, so that they can be upgraded in batches. As long as the maximum unavailable number or percentage of controlplane nodes has not been reached, Rancher will continue to upgrade other controlplane nodes, then the worker nodes. If any controlplane nodes were unable to be upgraded, the upgrade will not proceed to the worker nodes. Upgrades of Worker Nodes​ By default, worker nodes are upgraded in batches. The size of the batch is determined by the maximum number of unavailable worker nodes, configured as the max_unavailable_worker directive in the cluster.yml. By default, the max_unavailable_worker nodes is defined as 10 percent of all worker nodes. This number can be configured as a percentage or as an integer. When defined as a percentage, the batch size is rounded down to the nearest node, with a minimum of one node. For example, if you have 11 worker nodes and max_unavailable_worker is 25%, two nodes will be upgraded at once because 25% of 11 is 2.75. If you have two worker nodes and max_unavailable_worker is 1%, the worker nodes will be upgraded one at a time because the minimum batch size is one. When each node in a batch returns to a Ready state, the next batch of nodes begins to upgrade. If kubelet and kube-proxy have started, the node is Ready. As long as the max_unavailable_worker number of nodes have not failed, Rancher will continue to upgrade other worker nodes. RKE scans the cluster before starting the upgrade to find the powered down or unreachable hosts. The upgrade will stop if that number matches or exceeds the maximum number of unavailable nodes. RKE will cordon each node before upgrading it, and uncordon the node afterward. RKE can also be configured to drain nodes before upgrading them. RKE will handle all worker node upgrades before upgrading any add-ons. As long as the maximum number of unavailable worker nodes is not reached, RKE will attempt to upgrade the addons. For example, if a cluster has two worker nodes and one worker node fails, but the maximum unavailable worker nodes is greater than one, the addons will still be upgraded. Upgrades of Addons​ The availability of your applications partly depends on the availability of RKE addons. Addons are used to deploy several cluster components, including network plug-ins, the Ingress controller, DNS provider, and metrics server. Because RKE addons are necessary for allowing traffic into the cluster, they will need to be updated in batches to maintain availability. You will need to configure the maximum number of unavailable replicas for each addon in the cluster.yml to ensure that your cluster will retain enough available replicas during an upgrade. For more information on configuring the number of replicas for each addon, refer to this section. For an example showing how to configure the addons, refer to the example cluster.yml.","keywords":"","version":"Next"},{"title":"Example Scenarios","type":0,"sectionRef":"#","url":"/etcd-snapshots/example-scenarios","content":"Example Scenarios These example scenarios for backup and restore are different based on your version of RKE. RKE v0.2.0+RKE before v0.2.0 This walkthrough will demonstrate how to restore an etcd cluster from a local snapshot with the following steps: Back up the clusterSimulate a node failureAdd a new etcd node to the clusterRestore etcd on the new node from the backupConfirm that cluster operations are restored In this example, the Kubernetes cluster was deployed on two AWS nodes. Name\tIP\tRolenode1\t10.0.0.1\t[controlplane, worker] node2\t10.0.0.2\t[etcd] 1. Back Up the Cluster​ Take a local snapshot of the Kubernetes cluster. You can upload this snapshot directly to an S3 backend with the S3 options. $ rke etcd snapshot-save --name snapshot.db --config cluster.yml 2. Simulate a Node Failure​ To simulate the failure, let's power down node2. root@node2:~# poweroff Name\tIP\tRolenode1\t10.0.0.1\t[controlplane, worker] node2\t10.0.0.2\t[etcd] 3. Add a New etcd Node to the Kubernetes Cluster​ Before updating and restoring etcd, you will need to add the new node into the Kubernetes cluster with the etcd role. In the cluster.yml, comment out the old node and add in the new node. nodes: - address: 10.0.0.1 hostname_override: node1 user: ubuntu role: - controlplane - worker # - address: 10.0.0.2 # hostname_override: node2 # user: ubuntu # role: # - etcd - address: 10.0.0.3 hostname_override: node3 user: ubuntu role: - etcd 4. Restore etcd on the New Node from the Backup​ Prerequisite If the snapshot was created using RKE v1.1.4 or higher, the cluster state file should be included in the snapshot. The cluster state file will be automatically extracted and used for the restore. If the snapshot was created using RKE v1.1.3 or lower, please ensure your cluster.rkestate is present before starting the restore, because this contains your certificate data for the cluster. After the new node is added to the cluster.yml, run the rke etcd snapshot-restore to launch etcd from the backup: $ rke etcd snapshot-restore --name snapshot.db --config cluster.yml The snapshot is expected to be saved at /opt/rke/etcd-snapshots. If you want to directly retrieve the snapshot from S3, add in the S3 options. note As of v0.2.0, the file pki.bundle.tar.gz is no longer required for the restore process because the certificates required to restore are preserved within the cluster.rkestate. 5. Confirm that Cluster Operations are Restored​ The rke etcd snapshot-restore command triggers rke up using the new cluster.yml. Confirm that your Kubernetes cluster is functional by checking the pods on your cluster. &gt; kubectl get pods NAME READY STATUS RESTARTS AGE nginx-65899c769f-kcdpr 1/1 Running 0 17s nginx-65899c769f-pc45c 1/1 Running 0 17s nginx-65899c769f-qkhml 1/1 Running 0 17s ","keywords":"","version":"Next"},{"title":"Example Cluster.ymls","type":0,"sectionRef":"#","url":"/example-yamls","content":"","keywords":"","version":"Next"},{"title":"Minimal cluster.yml example​","type":1,"pageTitle":"Example Cluster.ymls","url":"/example-yamls#minimal-clusteryml-example","content":" nodes: - address: 1.2.3.4 user: ubuntu role: - controlplane - etcd - worker   ","version":"Next","tagName":"h2"},{"title":"Full cluster.yml example​","type":1,"pageTitle":"Example Cluster.ymls","url":"/example-yamls#full-clusteryml-example","content":" nodes: - address: 1.1.1.1 user: ubuntu role: - controlplane - etcd port: 2222 docker_socket: /var/run/docker.sock - address: 2.2.2.2 user: ubuntu role: - worker ssh_key_path: /home/user/.ssh/id_rsa ssh_key: |- -----BEGIN RSA PRIVATE KEY----- -----END RSA PRIVATE KEY----- ssh_cert_path: /home/user/.ssh/test-key-cert.pub ssh_cert: |- ssh-rsa-cert-v01@openssh.com AAAAHHNzaC1yc2EtY2VydC12MDFAb3Bl.... - address: example.com user: ubuntu role: - worker hostname_override: node3 internal_address: 192.168.1.6 labels: app: ingress taints: - key: test-key value: test-value effect: NoSchedule # If set to true, RKE will not fail when unsupported Docker version # are found ignore_docker_version: false # Enable running cri-dockerd # Up to Kubernetes 1.23, kubelet contained code called dockershim # to support Docker runtime. The replacement is called cri-dockerd # and should be enabled if you want to keep using Docker as your # container runtime # Only available to enable in Kubernetes 1.21 and higher enable_cri_dockerd: true # Cluster level SSH private key # Used if no ssh information is set for the node ssh_key_path: ~/.ssh/test # Enable use of SSH agent to use SSH private keys with passphrase # This requires the environment `SSH_AUTH_SOCK` configured pointing #to your SSH agent which has the private key added ssh_agent_auth: true # List of registry credentials # If you are using a Docker Hub registry, you can omit the `url` # or set it to `docker.io` # is_default set to `true` will override the system default # registry set in the global settings private_registries: - url: registry.com user: Username password: password is_default: true # Bastion/Jump host configuration bastion_host: address: x.x.x.x user: ubuntu port: 22 ssh_key_path: /home/user/.ssh/bastion_rsa # or # ssh_key: |- # -----BEGIN RSA PRIVATE KEY----- # # -----END RSA PRIVATE KEY----- # Set the name of the Kubernetes cluster cluster_name: mycluster # The Kubernetes version used. The default versions of Kubernetes # are tied to specific versions of the system images. # # For RKE v0.2.x and below, the map of Kubernetes versions and their system images is # located here: # https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go # # For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is # located here: # https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go # # In case the kubernetes_version and kubernetes image in # system_images are defined, the system_images configuration # will take precedence over kubernetes_version. kubernetes_version: v1.10.3-rancher2 # System Images are defaulted to a tag that is mapped to a specific # Kubernetes Version and not required in a cluster.yml. # Each individual system image can be specified if you want to use a different tag. # # For RKE v0.2.x and below, the map of Kubernetes versions and their system images is # located here: # https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go # # For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is # located here: # https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go # system_images: kubernetes: rancher/hyperkube:v1.10.3-rancher2 etcd: rancher/coreos-etcd:v3.1.12 alpine: rancher/rke-tools:v0.1.9 nginx_proxy: rancher/rke-tools:v0.1.9 cert_downloader: rancher/rke-tools:v0.1.9 kubernetes_services_sidecar: rancher/rke-tools:v0.1.9 kubedns: rancher/k8s-dns-kube-dns-amd64:1.14.8 dnsmasq: rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8 kubedns_sidecar: rancher/k8s-dns-sidecar-amd64:1.14.8 kubedns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0 pod_infra_container: rancher/pause-amd64:3.1 services: etcd: # Custom uid/guid for etcd directory and files uid: 52034 gid: 52034 # if external etcd is used # path: /etcdcluster # external_urls: # - https://etcd-example.com:2379 # ca_cert: |- # -----BEGIN CERTIFICATE----- # xxxxxxxxxx # -----END CERTIFICATE----- # cert: |- # -----BEGIN CERTIFICATE----- # xxxxxxxxxx # -----END CERTIFICATE----- # key: |- # -----BEGIN PRIVATE KEY----- # xxxxxxxxxx # -----END PRIVATE KEY----- # Note for Rancher v2.0.5 and v2.0.6 users: If you are configuring # Cluster Options using a Config File when creating Rancher Launched # Kubernetes, the names of services should contain underscores # only: `kube_api`. kube-api: # IP range for any services created on Kubernetes # This must match the service_cluster_ip_range in kube-controller service_cluster_ip_range: 10.43.0.0/16 # Expose a different port range for NodePort services service_node_port_range: 30000-32767 pod_security_policy: false # Encrypt secret data at Rest # Available as of v0.3.1 secrets_encryption_config: enabled: true custom_config: apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: k-fw5hn secret: RTczRjFDODMwQzAyMDVBREU4NDJBMUZFNDhCNzM5N0I= - identity: {} # Enable audit logging # Available as of v1.0.0 audit_log: enabled: true configuration: max_age: 6 max_backup: 6 max_size: 110 path: /var/log/kube-audit/audit-log.json format: json policy: apiVersion: audit.k8s.io/v1 # This is required. kind: Policy omitStages: - &quot;RequestReceived&quot; rules: # Log pod changes at RequestResponse level - level: RequestResponse resources: - group: &quot;&quot; # Resource &quot;pods&quot; doesn't match requests to any subresource of pods, # which is consistent with the RBAC policy. resources: [&quot;pods&quot;] # Using the EventRateLimit admission control enforces a limit on the number of events # that the API Server will accept in a given time period # Available as of v1.0.0 event_rate_limit: enabled: true configuration: apiVersion: eventratelimit.admission.k8s.io/v1alpha1 kind: Configuration limits: - type: Server qps: 6000 burst: 30000 # Enable AlwaysPullImages Admission controller plugin # Available as of v0.2.0 always_pull_images: false # Add additional arguments to the kubernetes API server # This WILL OVERRIDE any existing defaults extra_args: # Enable audit log to stdout audit-log-path: &quot;-&quot; # Increase number of delete workers delete-collection-workers: 3 # Set the level of log output to debug-level v: 4 # Note for Rancher 2 users: If you are configuring Cluster Options # using a Config File when creating Rancher Launched Kubernetes, # the names of services should contain underscores only: # `kube_controller`. This only applies to Rancher v2.0.5 and v2.0.6. kube-controller: # CIDR pool used to assign IP addresses to pods in the cluster cluster_cidr: 10.42.0.0/16 # IP range for any services created on Kubernetes # This must match the service_cluster_ip_range in kube-api service_cluster_ip_range: 10.43.0.0/16 # Add additional arguments to the kubernetes API server # This WILL OVERRIDE any existing defaults extra_args: # Set the level of log output to debug-level v: 4 # Enable RotateKubeletServerCertificate feature gate feature-gates: RotateKubeletServerCertificate=true # Enable TLS Certificates management # https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/ cluster-signing-cert-file: &quot;/etc/kubernetes/ssl/kube-ca.pem&quot; cluster-signing-key-file: &quot;/etc/kubernetes/ssl/kube-ca-key.pem&quot; kubelet: # Base domain for the cluster cluster_domain: cluster.local # IP address for the DNS service endpoint cluster_dns_server: 10.43.0.10 # Fail if swap is on fail_swap_on: false # Configure pod-infra-container-image argument pod-infra-container-image: &quot;k8s.gcr.io/pause:3.2&quot; # Generate a certificate signed by the kube-ca Certificate Authority # for the kubelet to use as a server certificate # Available as of v1.0.0 generate_serving_certificate: true extra_args: # Set max pods to 250 instead of default 110 max-pods: 250 # Enable RotateKubeletServerCertificate feature gate feature-gates: RotateKubeletServerCertificate=true # Optionally define additional volume binds to a service extra_binds: - &quot;/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins&quot; scheduler: extra_args: # Set the level of log output to debug-level v: 4 kubeproxy: extra_args: # Set the level of log output to debug-level v: 4 # Currently, only authentication strategy supported is x509. # You can optionally create additional SANs (hostnames or IPs) to # add to the API server PKI certificate. # This is useful if you want to use a load balancer for the # control plane servers. authentication: strategy: x509 sans: - &quot;10.18.160.10&quot; - &quot;my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com&quot; # Kubernetes Authorization mode # Use `mode: rbac` to enable RBAC # Use `mode: none` to disable authorization authorization: mode: rbac # If you want to set a Kubernetes cloud provider, you specify # the name and configuration cloud_provider: name: aws # Add-ons are deployed using kubernetes jobs. RKE will give # up on trying to get the job status after this timeout in seconds.. addon_job_timeout: 30 # Specify network plugin-in (canal, calico, flannel or none) network: plugin: canal # Specify MTU mtu: 1400 options: # Configure interface to use for Canal canal_iface: eth1 canal_flannel_backend_type: vxlan # Available as of v1.2.6 canal_autoscaler_priority_class_name: system-cluster-critical canal_priority_class_name: system-cluster-critical # Available as of v1.2.4 tolerations: - key: &quot;node.kubernetes.io/unreachable&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 - key: &quot;node.kubernetes.io/not-ready&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationseconds: 300 # Available as of v1.1.0 update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 6 # Specify DNS provider (coredns or kube-dns) dns: provider: coredns # Available as of v1.1.0 update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 20% maxSurge: 15% linear_autoscaler_params: cores_per_replica: 0.34 nodes_per_replica: 4 prevent_single_point_failure: true min: 2 max: 3 # Specify monitoring provider (metrics-server) monitoring: provider: metrics-server # Available as of v1.1.0 update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 8 # Currently only nginx ingress provider is supported. # To disable ingress controller, set `provider: none` # `node_selector` controls ingress placement and is optional ingress: provider: nginx node_selector: app: ingress # Available as of v1.1.0 update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 5 # All add-on manifests MUST specify a namespace addons: |- --- apiVersion: v1 kind: Pod metadata: name: my-nginx namespace: default spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 addons_include: - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-operator.yaml - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-cluster.yaml - /path/to/manifest  ","version":"Next","tagName":"h2"},{"title":"Requirements","type":0,"sectionRef":"#","url":"/os","content":"","keywords":"","version":"Next"},{"title":"Operating System​","type":1,"pageTitle":"Requirements","url":"/os#operating-system","content":" ","version":"Next","tagName":"h2"},{"title":"General Linux Requirements​","type":1,"pageTitle":"Requirements","url":"/os#general-linux-requirements","content":" RKE runs on almost any Linux OS with Docker installed. For details on which OS and Docker versions were tested with each version, refer to the support matrix.  SSH user - The SSH user used for node access must be a member of the docker group on the node: usermod -aG docker &lt;user_name&gt;   note Users added to the docker group are granted effective root permissions on the host by means of the Docker API. Only choose a user that is intended for this purpose and has its credentials and access properly secured.  See Manage Docker as a non-root user to see how you can configure access to Docker without using the root user.  Swap should be disabled on any worker nodes Please check the network plugin documentation for any additional requirements (for example, kernel modules) CalicoFlannelCanal (Combination Calico and Flannel)Weave (Deprecated)  note If you or your cloud provider are using a custom minimal kernel, some required (network) kernel modules might not be present.  Following sysctl settings must be applied  net.bridge.bridge-nf-call-iptables=1   ","version":"Next","tagName":"h3"},{"title":"SUSE Linux Enterprise Server (SLES) / openSUSE​","type":1,"pageTitle":"Requirements","url":"/os#suse-linux-enterprise-server-sles--opensuse","content":" If you are using SUSE Linux Enterprise Server or openSUSE follow the instructions below.  Using upstream Docker​  If you are using upstream Docker, the package name is docker-ce or docker-ee. You can check the installed package by executing:  rpm -q docker-ce   When using the upstream Docker packages, please follow Manage Docker as a non-root user.  Using SUSE/openSUSE packaged docker​  If you are using the Docker package supplied by SUSE/openSUSE, the package name is docker. You can check the installed package by executing:  rpm -q docker   Adding the Software repository for docker​  In SUSE Linux Enterprise Server 15 SP2 docker is found in the Containers module. This module will need to be added before installing docker.  To list available modules you can run SUSEConnect to list the extensions and the activation command  node:~ # SUSEConnect --list-extensions AVAILABLE EXTENSIONS AND MODULES Basesystem Module 15 SP2 x86_64 (Activated) Deactivate with: SUSEConnect -d -p sle-module-basesystem/15.2/x86_64 Containers Module 15 SP2 x86_64 Activate with: SUSEConnect -p sle-module-containers/15.2/x86_64   Run this SUSEConnect command to activate the Containers module.  node:~ # SUSEConnect -p sle-module-containers/15.2/x86_64 Registering system to registration proxy https://rmt.seader.us Updating system details on https://rmt.seader.us ... Activating sle-module-containers 15.2 x86_64 ... -&gt; Adding service to system ... -&gt; Installing release package ... Successfully registered system   In order to run docker cli commands with your user then you need to add this user to the docker group. It is preferred not to use the root user for this.  usermod -aG docker &lt;user_name&gt;   To verify that the user is correctly configured, log out of the node and login using SSH or your preferred method, and execute docker ps:  ssh user@node user@node:~&gt; docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES user@node:~&gt;   ","version":"Next","tagName":"h3"},{"title":"openSUSE MicroOS/Kubic (Atomic)​","type":1,"pageTitle":"Requirements","url":"/os#opensuse-microoskubic-atomic","content":" Consult the project pages for openSUSE MicroOS and Kubic for installation  openSUSE MicroOS​  Designed to host container workloads with automated administration &amp; patching. Installing openSUSE MicroOS you get a quick, small environment for deploying Containers, or any other workload that benefits from Transactional Updates. As rolling release distribution the software is always up-to-date.https://microos.opensuse.org  openSUSE Kubic​  Based on openSUSE MicroOS, designed with the same things in mind but is focused on being a Certified Kubernetes Distribution.https://kubic.opensuse.orgInstallation instructions:https://kubic.opensuse.org/blog/2021-02-08-MicroOS-Kubic-Rancher-RKE/  ","version":"Next","tagName":"h3"},{"title":"Red Hat Enterprise Linux (RHEL) / Oracle Linux (OL) / CentOS​","type":1,"pageTitle":"Requirements","url":"/os#red-hat-enterprise-linux-rhel--oracle-linux-ol--centos","content":" If using Red Hat Enterprise Linux, Oracle Linux or CentOS, you cannot use the root user as SSH user due to Bugzilla 1527565.  Do not use the RHEL/CentOS packaged Docker because, in reality, it is using Podman and that breaks RKE installation. Instead, fetch Docker from upstream, for example by following this link's instructions.  Please follow Manage Docker as a non-root user to complete the installation.  note In RHEL 8.4, two extra services are included on the NetworkManager: nm-cloud-setup.service and nm-cloud-setup.timer. These services add a routing table that interferes with the CNI plugin's configuration. If these services are enabled, you must disable them using the command below, and then reboot the node to restore connectivity: systemctl disable nm-cloud-setup.service nm-cloud-setup.timer reboot In addition, the default firewall settings of RHEL 8.4 prevent RKE1 pods from reaching out to Rancher to connect to the cluster agent. To allow Docker containers to reach out to the internet and connect to Rancher, make the following updates to the firewall settings: firewall-cmd --zone=public --add-masquerade --permanent firewall-cmd --reload   ","version":"Next","tagName":"h3"},{"title":"Red Hat Atomic​","type":1,"pageTitle":"Requirements","url":"/os#red-hat-atomic","content":" Before trying to use RKE with Red Hat Atomic nodes, there are a couple of updates to the OS that need to occur in order to get RKE working.  OpenSSH version​  By default, Atomic hosts ship with OpenSSH 6.4, which doesn't support SSH tunneling, which is a core RKE requirement. If you upgrade to the latest version of OpenSSH supported by Atomic, it will correct the SSH issue.  Creating a Docker Group​  By default, Atomic hosts do not come with a Docker group. You can update the ownership of the Docker socket by enabling the specific user in order to launch RKE.  # chown &lt;user&gt; /var/run/docker.sock   ","version":"Next","tagName":"h3"},{"title":"Flatcar Container Linux​","type":1,"pageTitle":"Requirements","url":"/os#flatcar-container-linux","content":" When using Flatcar Container Linux nodes, it is required to use the following configuration in the cluster configuration file:  CanalCalico rancher_kubernetes_engine_config: network: plugin: canal options: canal_flex_volume_plugin_dir: /opt/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds flannel_backend_type: vxlan services: kube-controller: extra_args: flex-volume-plugin-dir: /opt/kubernetes/kubelet-plugins/volume/exec/   It is also required to enable the Docker service, you can enable the Docker service using the following command:  systemctl enable docker.service   ","version":"Next","tagName":"h3"},{"title":"Software​","type":1,"pageTitle":"Requirements","url":"/os#software","content":" This section describes the requirements for Docker, Kubernetes, and SSH.  ","version":"Next","tagName":"h2"},{"title":"OpenSSH​","type":1,"pageTitle":"Requirements","url":"/os#openssh","content":" In order to SSH into each node, OpenSSH 7.0+ must be installed on each node.  ","version":"Next","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Requirements","url":"/os#kubernetes","content":" Refer to the RKE release notes for the supported versions of Kubernetes.  ","version":"Next","tagName":"h3"},{"title":"Docker​","type":1,"pageTitle":"Requirements","url":"/os#docker","content":" Each Kubernetes version supports different Docker versions. The Kubernetes release notes contain the current list of validated Docker versions.  ","version":"Next","tagName":"h3"},{"title":"Installing Docker​","type":1,"pageTitle":"Requirements","url":"/os#installing-docker","content":" Refer to Installing Docker  ","version":"Next","tagName":"h3"},{"title":"Checking the Installed Docker Version​","type":1,"pageTitle":"Requirements","url":"/os#checking-the-installed-docker-version","content":" Confirm that a Kubernetes supported version of Docker is installed on your machine, by running docker version --format '{{.Server.Version}}'.  ","version":"Next","tagName":"h3"},{"title":"Hardware​","type":1,"pageTitle":"Requirements","url":"/os#hardware","content":" This section describes the hardware requirements for the worker role, large Kubernetes clusters, and etcd clusters.  ","version":"Next","tagName":"h2"},{"title":"Worker Role​","type":1,"pageTitle":"Requirements","url":"/os#worker-role","content":" The hardware requirements for nodes with the worker role mostly depend on your workloads. The minimum to run the Kubernetes node components is 1 CPU (core) and 1GB of memory.  Regarding CPU and memory, it is recommended that the different planes of Kubernetes clusters (etcd, controlplane, and workers) should be hosted on different nodes so that they can scale separately from each other.  ","version":"Next","tagName":"h3"},{"title":"Large Kubernetes Clusters​","type":1,"pageTitle":"Requirements","url":"/os#large-kubernetes-clusters","content":" For hardware recommendations for large Kubernetes clusters, refer to the official Kubernetes documentation on building large clusters.  ","version":"Next","tagName":"h3"},{"title":"Etcd Clusters​","type":1,"pageTitle":"Requirements","url":"/os#etcd-clusters","content":" For hardware recommendations for etcd clusters in production, refer to the official etcd documentation.  ","version":"Next","tagName":"h3"},{"title":"Ports​","type":1,"pageTitle":"Requirements","url":"/os#ports","content":"     RKE node:Node that runs the rke commands  ","version":"Next","tagName":"h2"},{"title":"RKE node - Outbound rules​","type":1,"pageTitle":"Requirements","url":"/os#rke-node---outbound-rules","content":" Protocol\tPort\tSource\tDestination\tDescriptionTCP\t22\tRKE node\tAny node configured in Cluster Configuration File\tSSH provisioning of node by RKE TCP\t6443\tRKE node\tControl plane nodes\tKubernetes API server  etcd nodes: Nodes with the role etcd  ","version":"Next","tagName":"h3"},{"title":"etcd nodes - Inbound rules​","type":1,"pageTitle":"Requirements","url":"/os#etcd-nodes---inbound-rules","content":" Protocol\tPort\tSource\tDescriptionTCP\t2376\tRancher nodes\tDocker daemon TLS port used by Docker Machine (only needed when using Node Driver/Templates) TCP\t2379 etcd nodescontrolplane nodes etcd client requests TCP\t2380 etcd nodescontrolplane nodes etcd peer communication UDP\t8472 etcd nodescontrolplane nodesworker nodes Canal/Flannel VXLAN overlay networking TCP\t9099 etcd node itself (local traffic, not across nodes) Canal/Flannel livenessProbe/readinessProbe TCP\t10250 Metrics server communications with all nodes kubelet  ","version":"Next","tagName":"h3"},{"title":"etcd nodes - Outbound rules​","type":1,"pageTitle":"Requirements","url":"/os#etcd-nodes---outbound-rules","content":" Protocol\tPort\tDestination\tDescriptionTCP\t443 Rancher nodes Rancher agent TCP\t379 etcd nodes etcd client requests TCP\t2380 etcd nodes etcd peer communication TCP\t6443 controlplane nodes Kubernetes apiserver UDP\t8472 etcd nodescontrolplane nodesworker nodes Canal/Flannel VXLAN overlay networking TCP\t9099 etcd node itself (local traffic, not across nodes) Canal/Flannel livenessProbe/readinessProbe  controlplane nodes: Nodes with the role controlplane  ","version":"Next","tagName":"h3"},{"title":"controlplane nodes - Inbound rules​","type":1,"pageTitle":"Requirements","url":"/os#controlplane-nodes---inbound-rules","content":" Protocol\tPort\tSource\tDescriptionTCP\t80 Any that consumes Ingress services Ingress controller (HTTP) TCP\t443 Any that consumes Ingress services Ingress controller (HTTPS) TCP\t2376 Rancher nodes Docker daemon TLS port used by Docker Machine (only needed when using Node Driver/Templates) TCP\t6443 etcd nodescontrolplane nodesworker nodes Kubernetes apiserver UDP\t472 etcd nodescontrolplane nodesworker nodes Canal/Flannel VXLAN overlay networking TCP\t9099 controlplane node itself (local traffic, not across nodes) Canal/Flannel livenessProbe/readinessProbe TCP\t10250 Metrics server communications with all nodes kubelet TCP\t10254 controlplane node itself (local traffic, not across nodes) Ingress controller livenessProbe/readinessProbe TCP/UDP\t30000-32767 Any source that consumes NodePort services NodePort port range  ","version":"Next","tagName":"h3"},{"title":"controlplane nodes - Outbound rules​","type":1,"pageTitle":"Requirements","url":"/os#controlplane-nodes---outbound-rules","content":" Protocol\tPort\tDestination\tDescriptionTCP\t443 Rancher nodes Rancher agent TCP\t2379 etcd nodes etcd client requests TCP\t2380 etcd nodes etcd peer communication UDP\t8472 etcd nodescontrolplane nodesworker nodes Canal/Flannel VXLAN overlay networking TCP\t9099 controlplane node itself (local traffic, not across nodes) Canal/Flannel livenessProbe/readinessProbe TCP\t10250 etcd nodescontrolplane nodesworker nodes kubelet  Worker nodes: Nodes with the role worker  ","version":"Next","tagName":"h3"},{"title":"Worker nodes - Inbound rules​","type":1,"pageTitle":"Requirements","url":"/os#worker-nodes---inbound-rules","content":" Protocol\tPort\tSource\tDescriptionTCP\t22 Linux worker nodes onlyAny network that you want to be able to remotely access this node from. Remote access over SSH TCP\t3389 Windows worker nodes onlyAny network that you want to be able to remotely access this node from. Remote access over RDP TCP\t80 Any that consumes Ingress services Ingress controller (HTTP) TCP\t443 Any that consumes Ingress services Ingress controller (HTTPS) TCP\t2376 Rancher nodes Docker daemon TLS port used by Docker Machine only needed when using Node Driver/Templates) UDP\t8472 etcd nodescontrolplane nodesworker nodes Canal/Flannel VXLAN overlay networking TCP\t9099 worker node itself (local traffic, not across nodes) Canal/Flannel livenessProbe/readinessProbe TCP\t10250 Metrics server communications with all nodes kubelet TCP\t10254 worker node itself (local traffic, not across nodes) Ingress controller livenessProbe/readinessProbe TCP/UDP\t30000-32767 Any source that consumes NodePort services NodePort port range  ","version":"Next","tagName":"h3"},{"title":"Worker nodes - Outbound rules​","type":1,"pageTitle":"Requirements","url":"/os#worker-nodes---outbound-rules","content":" Protocol\tPort\tDestination\tDescriptionTCP\t443 Rancher nodes Rancher agent TCP\t6443 controlplane nodes Kubernetes apiserver UDP\t8472 etcd nodescontrolplane nodesworker nodes Canal/Flannel VXLAN overlay networking TCP\t9099 worker node itself (local traffic, not across nodes) Canal/Flannel livenessProbe/readinessProbe TCP\t10254 worker node itself (local traffic, not across nodes) Ingress controller livenessProbe/readinessProbe  ","version":"Next","tagName":"h3"},{"title":"Information on local node traffic​","type":1,"pageTitle":"Requirements","url":"/os#information-on-local-node-traffic","content":" Kubernetes health checks (livenessProbe and readinessProbe) are executed on the host itself. On most nodes, this is allowed by default. When you have applied strict host firewall (i.e., ptables) policies on the node, or when you are using nodes that have multiple interfaces (multi-homed), this traffic gets blocked. In this case, you have to explicitly allow this traffic in your host firewall, or in case of public/private cloud hosted machines (i.e. AWS or OpenStack), in your security group configuration. Keep in mind that when using a security group as Source or Destination in your security group, that this only applies to the private interface of the nodes/instances.  If you are using an external firewall, make sure you have this port opened between the machine you are using to run rke and the nodes that you are going to use in the cluster.  ","version":"Next","tagName":"h3"},{"title":"Opening port TCP/6443 using iptables​","type":1,"pageTitle":"Requirements","url":"/os#opening-port-tcp6443-using-iptables","content":" # Open TCP/6443 for all iptables -A INPUT -p tcp --dport 6443 -j ACCEPT # Open TCP/6443 for one specific IP iptables -A INPUT -p tcp -s your_ip_here --dport 6443 -j ACCEPT   ","version":"Next","tagName":"h3"},{"title":"Opening port TCP/6443 using firewalld​","type":1,"pageTitle":"Requirements","url":"/os#opening-port-tcp6443-using-firewalld","content":" # Open TCP/6443 for all firewall-cmd --zone=public --add-port=6443/tcp --permanent firewall-cmd --reload # Open TCP/6443 for one specific IP firewall-cmd --permanent --zone=public --add-rich-rule=' rule family=&quot;ipv4&quot; source address=&quot;your_ip_here/32&quot; port protocol=&quot;tcp&quot; port=&quot;6443&quot; accept' firewall-cmd --reload   ","version":"Next","tagName":"h3"},{"title":"SSH Server Configuration​","type":1,"pageTitle":"Requirements","url":"/os#ssh-server-configuration","content":" Your SSH server system-wide configuration file, located at /etc/ssh/sshd_config, must include this line that allows TCP forwarding:  AllowTcpForwarding yes  ","version":"Next","tagName":"h2"},{"title":"Configuring the Upgrade Strategy","type":0,"sectionRef":"#","url":"/upgrades/configuring-strategy","content":"","keywords":"","version":"Next"},{"title":"Maximum Unavailable Nodes​","type":1,"pageTitle":"Configuring the Upgrade Strategy","url":"/upgrades/configuring-strategy#maximum-unavailable-nodes","content":" The maximum number of unavailable controlplane and worker nodes can be configured in the cluster.yml before upgrading the cluster:  max_unavailable_controlplane: The maximum number of controlplane nodes that can fail without causing the cluster upgrade to fail. By default, max_unavailable_controlplane is defined as one node.max_unavailable_worker: The maximum number of worker nodes that can fail without causing the cluster upgrade to fail. By default, max_unavailable_worker is defined as 10 percent of all worker nodes.*  /* This number can be configured as a percentage or as an integer. When defined as a percentage, the batch size is rounded down to the nearest node, with a minimum of one node per batch.  An example configuration of the cluster upgrade strategy is shown below:  upgrade_strategy: max_unavailable_worker: 10% max_unavailable_controlplane: 1   ","version":"Next","tagName":"h3"},{"title":"Draining Nodes​","type":1,"pageTitle":"Configuring the Upgrade Strategy","url":"/upgrades/configuring-strategy#draining-nodes","content":" By default, nodes are cordoned first before upgrading. Each node should always be cordoned before starting its upgrade so that new pods will not be scheduled to it, and traffic will not reach the node. In addition to cordoning each node, RKE can also be configured to drain each node before starting its upgrade. Draining a node will evict all the pods running on the computing resource.  For information on draining and how to safely drain a node, refer to the Kubernetes documentation.  If the drain directive is set to true in the cluster.yml, worker nodes will be drained before they are upgraded. The default value is false:  upgrade_strategy: max_unavailable_worker: 10% max_unavailable_controlplane: 1 drain: false node_drain_input: force: false ignore_daemonsets: true delete_local_data: false grace_period: -1 // grace period specified for each pod spec will be used timeout: 60   ","version":"Next","tagName":"h3"},{"title":"Replicas for Ingress and Networking Addons​","type":1,"pageTitle":"Configuring the Upgrade Strategy","url":"/upgrades/configuring-strategy#replicas-for-ingress-and-networking-addons","content":" The Ingress and network addons are launched as Kubernetes daemonsets. If no value is given for the update strategy, Kubernetes sets the update strategy to rollingUpdate by default, with maxUnavailable set to 1.  An example configuration of the Ingress and network addons is shown below:  ingress: provider: nginx update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 5 network: plugin: canal update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 6   ","version":"Next","tagName":"h3"},{"title":"Replicas for DNS and Monitoring Addons​","type":1,"pageTitle":"Configuring the Upgrade Strategy","url":"/upgrades/configuring-strategy#replicas-for-dns-and-monitoring-addons","content":" The DNS and monitoring addons are launched as Kubernetes deployments. These addons include coredns, kubedns, and metrics-server, the monitoring deployment.  If no value is configured for their update strategy in the cluster.yml, Kubernetes sets the update strategy to rollingUpdate by default, with maxUnavailable set to 25% and maxSurge set to 25%.  The DNS addons use cluster-proportional-autoscaler, which is an open-source container image that watches over the number of schedulable nodes and cores of the cluster and resizes the number of replicas for the required resource. This functionality is useful for applications that need to be autoscaled with the number of nodes in the cluster. For the DNS addon, the fields needed for the cluster-proportional-autoscaler are made configurable.  The following table shows the default values for these fields:  Field Name\tDefault ValuecoresPerReplica\t128 nodesPerReplica\t4 min\t1 preventSinglePointFailure\ttrue  The cluster-proportional-autoscaler uses this formula to calculate the number of replicas:  replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) ) replicas = min(replicas, max) replicas = max(replicas, min)   An example configuration of the DNS and monitoring addons is shown below:  dns: provider: coredns update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 20% maxSurge: 15% linear_autoscaler_params: cores_per_replica: 0.34 nodes_per_replica: 4 prevent_single_point_failure: true min: 2 max: 3 monitoring: provider: metrics-server update_strategy: strategy: RollingUpdate rollingUpdate: maxUnavailable: 8   ","version":"Next","tagName":"h3"},{"title":"Example cluster.yml​","type":1,"pageTitle":"Configuring the Upgrade Strategy","url":"/upgrades/configuring-strategy#example-clusteryml","content":" # If you intened to deploy Kubernetes in an air-gapped environment, # please consult the documentation on how to configure custom RKE images. nodes: # At least three etcd nodes, two controlplane nodes, and two worker nodes, # nodes skipped for brevity upgrade_strategy: max_unavailable_worker: 10% max_unavailable_controlplane: 1 drain: false node_drain_input: force: false ignore_daemonsets: true delete_local_data: false grace_period: -1 // grace period specified for each pod spec will be used timeout: 60 ingress: provider: nginx update_strategy: # Available in v2.4 strategy: RollingUpdate rollingUpdate: maxUnavailable: 5 network: plugin: canal update_strategy: # Available in v2.4 strategy: RollingUpdate rollingUpdate: maxUnavailable: 6 dns: provider: coredns update_strategy: # Available in v2.4 strategy: RollingUpdate rollingUpdate: maxUnavailable: 20% maxSurge: 15% linear_autoscaler_params: cores_per_replica: 0.34 nodes_per_replica: 4 prevent_single_point_failure: true min: 2 max: 3 monitoring: provider: metrics-server update_strategy: # Available in v2.4 strategy: RollingUpdate rollingUpdate: maxUnavailable: 8  ","version":"Next","tagName":"h3"}],"options":{"indexBaseUrl":true,"id":"default"}}